{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "import math\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import ta\n",
    "# Alpha Vantage key required for creating listing status and methods are missing for it, not created yet, I manually grabbed the CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How we can adjust the model\n",
    "\n",
    "#1 we can actually tune the learning rate\n",
    "#2 we can tune L1 regularization\n",
    "#2 We can tune L2 Regularization\n",
    "#3 batch size we can tune\n",
    "#4 we can change epoch run time\n",
    "#5 we can validate the model via data from 2018 and on as planned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "We're gonna need to make a class for the generation of the tensors, I want specific things, I need to randomize the datasets because it promotes diversity, I also need to ensure that the dataframe that is hosting the list\n",
    "retains its values outside of the program thus I need some kind of a save object, we're gonna need to save the credentials inside a json file, the class will have to retain the object and be loadable and saveable when the model is saved\n",
    "this way when we run a new epoch we can remake the object from its saved state and then continue training, once all the data is used a column inside the dataframe within this data object will be wiped and reset, this will continue forever, until\n",
    "we choose to stop.\n",
    "\n",
    "This object will be used in the tensor creation process and it will simply output the tensors we need to use for training and also create the data we need for those tensors prior to extracting numpy frame values, standardizing and normalizing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter('training/MegaNumMuncher2')\n",
    "# Set pandas to display numbers without scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "\n",
    "look_back = 60\n",
    "look_forward = 30\n",
    "standard_scaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the beginning of your notebook or training loop\n",
    "if os.path.exists('C:\\\\Machine Learning\\\\SPY VIX 9 Input Output Model\\\\last_step.txt'):\n",
    "    with open('C:\\\\Machine Learning\\\\SPY VIX 9 Input Output Model\\\\last_step.txt', 'r') as f:\n",
    "        global_step = int(f.read())\n",
    "else:\n",
    "    global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TickerDataSystem(): #Simple system to create our list of data and return yFinance datasets\n",
    "    def __init__(self, start_date = None, end_date = None):\n",
    "        self.start_date = pd.to_datetime(start_date) #\"1994-01-01\"\n",
    "        self.end_date = pd.to_datetime(end_date) #\"2017-01-01\"  This will specifically be used to cap off data for a specific criteria\n",
    "        self.new_listing = None\n",
    "\n",
    "    def modify_dates(self, df, date_column, start_date, end_date): #Specifically to drop specific dates before and after Vix start and end date\n",
    "        # Convert date_column to datetime\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        \n",
    "        # Set dates earlier than start_date to start_date\n",
    "        df.loc[df[date_column] < start_date, date_column] = start_date\n",
    "        \n",
    "        # Remove rows with dates later than end_date\n",
    "        df = df[df[date_column] <= end_date]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def CreateListingStatus(self):\n",
    "        # Specify the path to your CSV file\n",
    "        file_path = r'C:\\Machine Learning\\listing_status.csv' #from alpha vantage listing_status add api call later using BS4\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        listing_statusDF = pd.read_csv(file_path)\n",
    "\n",
    "        # List of columns to drop\n",
    "        columns_to_drop = ['name', 'exchange', 'assetType', 'delistingDate', 'status', '6']\n",
    "\n",
    "        # Drop the columns\n",
    "        new_ListingStatusDF = listing_statusDF.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "        # Drop the last 30 rows\n",
    "        new_ListingStatusDF = new_ListingStatusDF.iloc[:-30]\n",
    "\n",
    "        # Modify dates and filter rows based on the specified conditions\n",
    "        new_ListingStatusDF = self.modify_dates(new_ListingStatusDF, 'ipoDate', self.start_date, self.end_date)\n",
    "        \n",
    "        self.new_listing = new_ListingStatusDF\n",
    "        \n",
    "    def Get_Lists(self):\n",
    "        \n",
    "        return self.new_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorIntegrationSystem:  # This class will generate tensors on demand at random and retain their values as each creation \n",
    "    \"\"\"\n",
    "    This class is specifically for creating the tensors, adding the averages we want such as EMA and RSI to various components then finally outputting a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, startDate:str, endDate:str) -> None:\n",
    "        self.endDate = endDate\n",
    "        self.startDate = startDate\n",
    "        self.ProgressTracker = ProgressTracker() #Create progress tracker object to parse data from CSV and retain the list to randomize insertion of data\n",
    "        self.TickerDataSystem = TickerDataSystem(self.startDate, self.endDate) #This tool is specifically to create the date ranges we want from the listing_status CSV, eventually alpha vantage listings APi will be added\n",
    "        self.FormattedCombinedDataFrame = None\n",
    "        self.stockDataFrame = None\n",
    "        self.VIXDataFrame = None\n",
    "    \n",
    "    def dataFetcher(self, symbol: str, startDate: str):\n",
    "        \"\"\"\n",
    "        This will pull data from yF and test it if nothing is returned the subsequent method will test it and try again to create another ticker.\n",
    "        It will also grab the split data and hold it inside a property.\n",
    "        \"\"\"\n",
    "        #we gotta add a simple date modifier to change the enddate and add a year to it so we can pull more data as the IPORange is maxed at the self.endDate so we always have at least a year of data\n",
    "        time.sleep(0.75)\n",
    "        if symbol != \"^VIX\":\n",
    "            data = yf.download(symbol, start=startDate, end=self.endDate, auto_adjust = True)\n",
    "        else:\n",
    "            data = yf.download(symbol, start=startDate, end=self.endDate)\n",
    "            \n",
    "        if data.shape == (0, 6):\n",
    "            print(\"No Data\")\n",
    "            return None\n",
    "        else:\n",
    "            time.sleep(0.5)\n",
    "            # Create a Ticker object for Pfizer\n",
    "            tickerData = yf.Ticker(symbol)\n",
    "            return pd.DataFrame(data)\n",
    "        \n",
    "    def CreateCombinedDataFrame(self):\n",
    "        \"\"\"\n",
    "        This will create a random stock and VixData then create the formatted DataFrame and bind it to FormattedCombinedDataFrame.\n",
    "        \"\"\"\n",
    "        max_attempts = 5\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                self.CreateCombinedData()\n",
    "                break  # If successful, exit the loop\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt < max_attempts - 1:\n",
    "                    time.sleep(1)  # Wait for 1 second before next attempt\n",
    "                else:\n",
    "                    raise Exception(\"Failed to create combined data frame after several attempts.\")\n",
    "\n",
    "\n",
    "    def CreateCombinedData(self):\n",
    "        \"\"\"\n",
    "        This method will try to generate the data required for the CreateCombinedDataFrame\n",
    "        \"\"\"\n",
    "        self.VIXDataFrame = None\n",
    "        self.stockDataFrame = None\n",
    "        self.CreateNewVixData()\n",
    "        self.CreateNewStockData()\n",
    "        Formatter = FormatDataFrame(self.VIXDataFrame, self.stockDataFrame)\n",
    "        Formatter.CreateDataSet()\n",
    "        self.FormattedCombinedDataFrame = Formatter.GetCombinedData()\n",
    "            \n",
    "    \n",
    "    def CreateNewVixData(self):\n",
    "        \"\"\"\n",
    "        This method is for creating VIX data and checking if it was created via the dataFetcher and Progress Tracker.\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        while True:\n",
    "            counter += 1\n",
    "            if counter == 10:\n",
    "                return\n",
    "            vixDataFrame = self.dataFetcher(\"^VIX\", \"1994-01-01\")\n",
    "            if vixDataFrame is None:\n",
    "                continue  # Try again in the next iteration\n",
    "            else:\n",
    "                self.VIXDataFrame = vixDataFrame\n",
    "                return\n",
    "        \n",
    "    def CreateNewStockData(self):\n",
    "        \"\"\"\n",
    "        This method is for creating stock data and checking if it was created via the dataFetcher and Progress Tracker.\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        while True:\n",
    "            ticker, tickerDate = self.ProgressTracker.random_next()\n",
    "            counter += 1\n",
    "            if counter == 10:\n",
    "                return\n",
    "            stockDataFrame = self.dataFetcher(ticker, tickerDate)\n",
    "            print(f\"This Ticker is {ticker}\")\n",
    "            if stockDataFrame is None:\n",
    "                continue  # Try again in the next iteration\n",
    "            else:\n",
    "                self.stockDataFrame = stockDataFrame\n",
    "                return\n",
    "            \n",
    "    def CreateTensors(self):\n",
    "        \"\"\"\n",
    "        Creates tensors that get output as x and y tensors with the required lookback and lookforward setup as public variables outside of the object (for now)\n",
    "        \"\"\"\n",
    "        # Create a new DataFrame with selected features\n",
    "        features = self.FormattedCombinedDataFrame.values\n",
    "\n",
    "        # Standardize data\n",
    "        standardized_features = standard_scaler.fit_transform(features)\n",
    "\n",
    "        # Normalize data\n",
    "        scaled_features = min_max_scaler.fit_transform(standardized_features)\n",
    "\n",
    "        # Prepare data for PyTorch (sequences and targets)\n",
    "        num_features = scaled_features.shape[1]\n",
    "\n",
    "        x_train, y_train = [], []\n",
    "        for i in range(look_back, len(scaled_features) - look_forward + 1):\n",
    "            x_train.append(scaled_features[i - look_back:i, :])  # Include all features from the past 60 days\n",
    "            y_train.append(scaled_features[i:i + look_forward, :])  # Include all features for the next 30 days\n",
    "\n",
    "        x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "        x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], num_features))\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        x_train_tensor = torch.tensor(x_train).float()\n",
    "        y_train_tensor = torch.tensor(y_train).float()\n",
    "\n",
    "        # If you're using a GPU, move tensors to GPU (assuming 'device' is defined)\n",
    "        x_train_tensor = x_train_tensor.to(device)\n",
    "        y_train_tensor = y_train_tensor.to(device)\n",
    "        \n",
    "        return x_train_tensor, y_train_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormatDataFrame():\n",
    "    \"\"\"\n",
    "    This class is used to take in a dataFrame and add various different kinds of indicators as columns into the dataset in order to create additional data\n",
    "    for the tensor inputs in the method chain.\n",
    "    \"\"\"\n",
    "    def __init__(self, VIXDataFrame, TickerDataFrame):\n",
    "        self.VIXDataFrame = VIXDataFrame\n",
    "        self.TickerDataFrame = TickerDataFrame\n",
    "        self.CombinedDataSet = None\n",
    "        \n",
    "    def GetCombinedData(self):\n",
    "        \"\"\"\n",
    "        Returns the combined dataset after processing\n",
    "        \"\"\"\n",
    "        return self.CombinedDataSet\n",
    "        \n",
    "    def CreateDataSet(self):\n",
    "        \"\"\"\n",
    "        This method will execute all work requirements.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.TickerDataFrame is None or self.VIXDataFrame is None:\n",
    "                raise TypeError(\"Missing DataFrames\")\n",
    "\n",
    "            #print(\"Transforming Ticker Data...\")\n",
    "            self.TransformTickerData()\n",
    "            #print(\"Ticker Data Transformed.\")\n",
    "\n",
    "            #print(\"Transforming VIX Data...\")\n",
    "            self.TransformVIXData()\n",
    "            #print(\"VIX Data Transformed.\")\n",
    "\n",
    "            #print(\"Creating Combined Data Set...\")\n",
    "            self.CreateCombinedDataSet()\n",
    "            #print(f\"Combined Data Set Created with shape: {self.CombinedDataSet.shape}\")\n",
    "\n",
    "            #print(\"Validating Data...\")\n",
    "            self.ValidateData()\n",
    "            #print(\"Data Validated.\")\n",
    "\n",
    "            #print(\"Applying Fourier Transform...\")\n",
    "            self.applyFourierTransform()\n",
    "            #print(\"Fourier Transform Applied.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def ValidateData(self):\n",
    "        \"\"\"\n",
    "        Validates the combined DataFrame for NaN values and checks date alignment.\n",
    "        \"\"\"\n",
    "        if self.CombinedDataSet is None:\n",
    "            raise ValueError(\"Combined DataSet is not available for validation.\")\n",
    "\n",
    "        # Check for NaN values\n",
    "        nan_counts = self.CombinedDataSet.isna().sum()\n",
    "        if nan_counts.sum() > 0:\n",
    "            print(\"Warning: NaN values found in the following columns:\")\n",
    "            print(nan_counts[nan_counts > 0])\n",
    "\n",
    "        # Check for date alignment\n",
    "        if not self.CombinedDataSet.index.is_monotonic_increasing:\n",
    "            raise ValueError(\"Date index is not in chronological order.\")\n",
    "\n",
    "        print(\"Data validation completed.\")\n",
    "        \n",
    "    def CreateCombinedDataSet(self):\n",
    "        # Ensure 'Date' is the index and perform the merge\n",
    "        if self.TickerDataFrame.index.name != 'Date' or self.VIXDataFrame.index.name != 'Date':\n",
    "            raise ValueError(\"Date must be the index for both DataFrames.\")\n",
    "\n",
    "        # Merge the DataFrames on the 'Date' index\n",
    "        self.CombinedDataSet = pd.merge(self.TickerDataFrame, self.VIXDataFrame, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "        if self.CombinedDataSet.empty:\n",
    "            raise ValueError(\"Merging resulted in an empty DataFrame. Check if the indices align correctly.\")\n",
    "        \n",
    "\n",
    "    def TransformTickerData(self):\n",
    "        \"\"\"\n",
    "        This method should take the dataframe for the Ticker, it'll check if it exists first and if it doesn't throw an error. Else\n",
    "        it will add 20,40,60 EMA on close, and 20,40,60 EMA on volume, as well as RSI for both.\n",
    "        \"\"\"\n",
    "        if self.TickerDataFrame is None:\n",
    "            raise TypeError(\"Missing DataFrame\")\n",
    "            \n",
    "        # Calculate EMAs for Close prices\n",
    "        self.TickerDataFrame['EMA20_Close'] = ta.trend.ema_indicator(close=self.TickerDataFrame['Close'], window=20)\n",
    "        self.TickerDataFrame['EMA40_Close'] = ta.trend.ema_indicator(close=self.TickerDataFrame['Close'], window=40)\n",
    "        self.TickerDataFrame['EMA60_Close'] = ta.trend.ema_indicator(close=self.TickerDataFrame['Close'], window=60)\n",
    "\n",
    "        # Calculate EMAs for Volume\n",
    "        self.TickerDataFrame['EMA20_Volume'] = ta.trend.ema_indicator(close=self.TickerDataFrame['Volume'], window=20)\n",
    "        self.TickerDataFrame['EMA40_Volume'] = ta.trend.ema_indicator(close=self.TickerDataFrame['Volume'], window=40)\n",
    "        self.TickerDataFrame['EMA60_Volume'] = ta.trend.ema_indicator(close=self.TickerDataFrame['Volume'], window=60)\n",
    "\n",
    "        # Calculate RSIs for Close prices\n",
    "        self.TickerDataFrame['RSI20_Close'] = ta.momentum.rsi(close=self.TickerDataFrame['Close'], window=20)\n",
    "        self.TickerDataFrame['RSI40_Close'] = ta.momentum.rsi(close=self.TickerDataFrame['Close'], window=40)\n",
    "        self.TickerDataFrame['RSI60_Close'] = ta.momentum.rsi(close=self.TickerDataFrame['Close'], window=60)\n",
    "\n",
    "        # Calculate RSIs for Volume\n",
    "        self.TickerDataFrame['RSI20_Volume'] = ta.momentum.rsi(close=self.TickerDataFrame['Volume'], window=20)\n",
    "        self.TickerDataFrame['RSI40_Volume'] = ta.momentum.rsi(close=self.TickerDataFrame['Volume'], window=40)\n",
    "        self.TickerDataFrame['RSI60_Volume'] = ta.momentum.rsi(close=self.TickerDataFrame['Volume'], window=60)\n",
    "        \n",
    "        # Drop rows where any of the EMA or RSI columns contain NaN\n",
    "        columns_to_check = ['EMA20_Close', 'EMA40_Close', 'EMA60_Close', \n",
    "                            'EMA20_Volume', 'EMA40_Volume', 'EMA60_Volume', \n",
    "                            'RSI20_Close', 'RSI40_Close', 'RSI60_Close', \n",
    "                            'RSI20_Volume', 'RSI40_Volume', 'RSI60_Volume']\n",
    "\n",
    "        self.TickerDataFrame = self.TickerDataFrame.dropna(subset=columns_to_check)\n",
    "        #self.TickerDataFrame = self.TickerDataFrame.drop(columns=['Adj Close'])\n",
    "        \n",
    "    def applyFourierTransform(self):\n",
    "        \"\"\"\n",
    "        Applies a fourier transform on the date column in order to make it more consumable by the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate the shifted day of year for each date in the DataFrame\n",
    "        self.CombinedDataSet['shifted_day_of_year'] = self.CombinedDataSet.index.map(self.shifted_day_of_year)\n",
    "\n",
    "        # Define the number of days in this \"year\" - accounting for leap years\n",
    "        # This is a simplification and could be further refined for exact leap year handling\n",
    "        days_in_year = 365.25\n",
    "\n",
    "        # Add Fourier features to df for the shifted year\n",
    "        self.CombinedDataSet['cos_shifted_annual'] = np.cos(2 * np.pi * self.CombinedDataSet['shifted_day_of_year'] / days_in_year)\n",
    "        self.CombinedDataSet['sin_shifted_annual'] = np.sin(2 * np.pi * self.CombinedDataSet['shifted_day_of_year'] / days_in_year)\n",
    "        \n",
    "    # Function to calculate the shifted day of year\n",
    "    def shifted_day_of_year(self, date):\n",
    "        \"\"\"\n",
    "        This will apply the transform specifically starting March 1st as the start of the new year.\n",
    "        \"\"\"\n",
    "        # Set the start of the \"new year\" to March 1st\n",
    "        new_year_start = pd.Timestamp(year=date.year, month=3, day=1)\n",
    "        # Calculate the number of days from the start of the \"new year\"\n",
    "        day_of_year = (date - new_year_start).days + 1\n",
    "        # Handle dates before March 1st\n",
    "        if date < new_year_start:\n",
    "            # Shift to the previous year\n",
    "            previous_year_start = pd.Timestamp(year=date.year-1, month=3, day=1)\n",
    "            day_of_year = (date - previous_year_start).days + 1\n",
    "        return day_of_year\n",
    "\n",
    "    def TransformVIXData(self):\n",
    "        \"\"\"\n",
    "        This will transform the vix data adding EMA and RSI to the closed price at specific ranges as well as applying a fourier transform to the date, starting March 1st as new year.\n",
    "        \"\"\"\n",
    "        if self.VIXDataFrame is None:\n",
    "            raise TypeError(\"Missing DataFrame\")\n",
    "        \n",
    "        # Calculate EMAs for Close prices\n",
    "        self.VIXDataFrame['EMA20_Close'] = ta.trend.ema_indicator(close=self.VIXDataFrame['Close'], window=20)\n",
    "        self.VIXDataFrame['EMA40_Close'] = ta.trend.ema_indicator(close=self.VIXDataFrame['Close'], window=40)\n",
    "        self.VIXDataFrame['EMA60_Close'] = ta.trend.ema_indicator(close=self.VIXDataFrame['Close'], window=60)\n",
    "\n",
    "        # Calculate RSIs for Close prices\n",
    "        self.VIXDataFrame['RSI20_Close'] = ta.momentum.rsi(close=self.VIXDataFrame['Close'], window=20)\n",
    "        self.VIXDataFrame['RSI40_Close'] = ta.momentum.rsi(close=self.VIXDataFrame['Close'], window=40)\n",
    "        self.VIXDataFrame['RSI60_Close'] = ta.momentum.rsi(close=self.VIXDataFrame['Close'], window=60)\n",
    "\n",
    "        # Drop rows where any of the EMA or RSI columns contain NaN\n",
    "        columns_to_check = ['EMA20_Close', 'EMA40_Close', 'EMA60_Close', \n",
    "                            'RSI20_Close', 'RSI40_Close', 'RSI60_Close']\n",
    "\n",
    "        self.VIXDataFrame = self.VIXDataFrame.dropna(subset=columns_to_check)\n",
    "        self.VIXDataFrame = self.VIXDataFrame.drop(columns=['Adj Close', 'Volume'])\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressTracker:\n",
    "    \"\"\"\n",
    "    This object is for retaining data in a CSV and randomly returning a ticker and IPO date so we can parse it from yFinance. \n",
    "    It also makes sure that nothing is repeated so we have a range of trading data.\n",
    "    \"\"\"\n",
    "    PROGRESS_FILE = 'C:\\\\Machine Learning\\\\ProgressTracker.csv'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.create_or_load_csv()\n",
    "        self.load_progress()\n",
    "\n",
    "    def create_or_load_csv(self):\n",
    "        file_dir = os.path.dirname(ProgressTracker.PROGRESS_FILE)\n",
    "        if not os.path.exists(file_dir):\n",
    "            os.makedirs(file_dir)\n",
    "\n",
    "        if not os.path.isfile(ProgressTracker.PROGRESS_FILE):\n",
    "            empty_df = pd.DataFrame({'symbol': [], 'ipoDate': [], 'Completed': []})\n",
    "            empty_df.to_csv(ProgressTracker.PROGRESS_FILE, index=False)\n",
    "\n",
    "    def load_dataframe(self, dataframe):\n",
    "        if os.path.exists(ProgressTracker.PROGRESS_FILE):\n",
    "            os.replace(ProgressTracker.PROGRESS_FILE, 'backup_' + ProgressTracker.PROGRESS_FILE)\n",
    "        dataframe.to_csv(ProgressTracker.PROGRESS_FILE, index=False)\n",
    "\n",
    "    def mark_completed(self, symbol):\n",
    "        index = next((idx for idx, row in enumerate(self.symbols) if row[0] == symbol), None)\n",
    "        if index is not None:\n",
    "            self.completed[index] = True\n",
    "\n",
    "    def mark_uncompleted(self, symbol):\n",
    "        index = next((idx for idx, row in enumerate(self.symbols) if row[0] == symbol), None)\n",
    "        if index is not None:\n",
    "            self.completed[index] = False\n",
    "\n",
    "    def random_next(self):\n",
    "        available_symbols = [sym for sym, comp in zip(self.symbols, self.completed) if not comp]\n",
    "        if not available_symbols:  # No available symbols\n",
    "            self.clear_completed()  # Clear all completed flags\n",
    "            available_symbols = [sym for sym, comp in zip(self.symbols, self.completed) if not comp]  # Recheck available symbols\n",
    "        selected_symbol = random.choice(available_symbols)\n",
    "        index = self.symbols.index(selected_symbol)\n",
    "        self.completed[index] = True\n",
    "        self.save_progress()\n",
    "        return selected_symbol[0], selected_symbol[1]  # Return the symbol and its IPO date\n",
    "\n",
    "    def clear_completed(self):\n",
    "        self.completed = [False] * len(self.symbols)\n",
    "        self.save_progress()\n",
    "\n",
    "    def save_progress(self):\n",
    "        progress_df = pd.DataFrame(self.symbols, columns=['symbol', 'ipoDate'])\n",
    "        progress_df['Completed'] = self.completed\n",
    "        progress_df.to_csv(ProgressTracker.PROGRESS_FILE, index=False)\n",
    "\n",
    "\n",
    "    def load_progress(self):\n",
    "        if os.path.exists(ProgressTracker.PROGRESS_FILE):\n",
    "            ticker_data_frame = pd.read_csv(ProgressTracker.PROGRESS_FILE)\n",
    "            self.symbols = ticker_data_frame[['symbol', 'ipoDate']].values.tolist()\n",
    "            self.completed = ticker_data_frame['Completed'].astype(bool).tolist()\n",
    "        else:\n",
    "            self.symbols = []\n",
    "            self.completed = []\n",
    "\n",
    "    def get_symbols_df(self):\n",
    "        if os.path.exists(ProgressTracker.PROGRESS_FILE):\n",
    "            return pd.read_csv(ProgressTracker.PROGRESS_FILE)\n",
    "        else:\n",
    "            df = pd.DataFrame({'symbol': [], 'ipoDate': [], 'Completed': []})\n",
    "            df.to_csv(ProgressTracker.PROGRESS_FILE, index=False)\n",
    "            return df\n",
    "        \n",
    "        \n",
    "# tracker = ProgressTracker()\n",
    "\n",
    "# # Mark IBM as completed\n",
    "# tracker.mark_completed('IBM')\n",
    "\n",
    "# # Randomly select a symbol to process\n",
    "# ticker_to_process = tracker.random_next()\n",
    "# print(ticker_to_process)\n",
    "\n",
    "# # Clear all completed items\n",
    "# tracker.clear_completed()\n",
    "\n",
    "# # Load progress from the CSV file\n",
    "# tracker.load_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding self.fc:\n",
    "\n",
    "    What is self.fc?\n",
    "        self.fc is a class attribute you're defining in the DCGANGenerator class.\n",
    "        It stands for a \"fully connected\" layer or layers in neural network terminology. However, in your case, self.fc is not just a single fully connected layer but a sequential container (nn.Sequential) of several layers.\n",
    "\n",
    "    Role of nn.Sequential:\n",
    "        nn.Sequential is a PyTorch module that sequentially applies a list of modules. It is used for encapsulating a simple sequence of layers or operations, automatically handling the forward pass through these layers in the order they are added.\n",
    "        In nn.Sequential, the output of one layer is automatically passed as input to the next.\n",
    "\n",
    "    Understanding the Sequence of Methods:\n",
    "        Linear Layer (nn.Linear): This is the first layer in the sequence. It's a fully connected layer that linearly transforms the input data (input_size) to a higher dimensional space (hidden_layer_size * 4 * 4 * 256). This expansion is typical in GAN generators to prepare for reshaping and further processing in subsequent layers.\n",
    "        Batch Normalization Layer (nn.BatchNorm1d): Follows the linear layer. Batch normalization is used to stabilize and speed up training by normalizing the output of the previous layer. It can help mitigate issues like vanishing or exploding gradients.\n",
    "        Activation Function (nn.ReLU): This is the Rectified Linear Unit activation function. It introduces non-linearity into the model, allowing it to learn more complex patterns. The ReLU function is defined as f(x) = max(0, x) and is applied element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAdvancedCNNBlock(nn.Module):\n",
    "    def __init__(self, input_channels, intermediate_channels):\n",
    "        super().__init__()\n",
    "        self.model1 = models.resnet50(pretrained=False)\n",
    "        self.model1.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model1.norm1 = nn.Identity()\n",
    "        self.model2 = models.densenet121(pretrained=False)\n",
    "        self.model3 = models.inception_v3(pretrained=False, aux_logits=None, transform_inputs=False)\n",
    "        self.model3.AuxLogits = None\n",
    "        self.model4 = models.efficientnet_b7(pretrained=False)\n",
    "        self.model4._blocks[0].conv = nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "        self.adapter_block = nn.Sequential(\n",
    "            nn.Conv2d(intermediate_channels*4, intermediate_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(intermediate_channels),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        x = self.model2(x)\n",
    "        x = self.model3(x)\n",
    "        x = self.model4(x)\n",
    "        x = torch.cat([x[0][-1], x[-1]], dim=1)\n",
    "        x = self.adapter_block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTCNModule(nn.Module):\n",
    "    def __init__(self, input_channels, nb_filters, kernel_size, dilations, dropout_p):\n",
    "        super().__init__()\n",
    "        self.tcn = tcn_module.TemporalConvolution(input_channels, nb_filters, kernel_size, dilations, dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.tcn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridLSTMGRUAttentionWithCNN(nn.Module): # THIS IS MY IN PRODUCTION MODEL\n",
    "    def __init__(self, input_size: int, hidden_layer_size: int, num_layers: int, output_size: int, dropout_rate: float, num_heads: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # CNN layers\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=hidden_layer_size // 2, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm1d(hidden_layer_size // 2),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.cnn_output_size = hidden_layer_size // 2  # Output size of CNN layers\n",
    "\n",
    "        # Define LSTM and GRU layers alternatively\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            is_last_layer = i == num_layers - 1\n",
    "            layer_dropout = 0 if is_last_layer else dropout_rate\n",
    "            #input_dim = self.cnn_output_size if i == 0 else (hidden_layer_size if i % 2 == 0 else 2 * hidden_layer_size)\n",
    "            input_dim = self.cnn_output_size if i == 0 else 2 * hidden_layer_size\n",
    "            rnn_type = nn.LSTM if i % 2 == 0 else nn.GRU\n",
    "            self.rnn_layers.append(rnn_type(input_dim, hidden_layer_size, batch_first=True, bidirectional=True, dropout=layer_dropout))\n",
    "\n",
    "        # Multi-Head Attention layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=2 * hidden_layer_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(2 * hidden_layer_size, 1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_layer_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, output_size)\n",
    "        )\n",
    "\n",
    "        # Manual dropout layers between RNN layers\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(dropout_rate) for _ in range(num_layers - 1)])\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.fill_(0)\n",
    "            elif isinstance(m, nn.GRU):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(\"Pre-Permuation RNN Layer Shape:\", x.shape)  # Print statement here\n",
    "        # Apply CNN layers\n",
    "        x = x.permute(0, 2, 1)  # Change axes order for CNN operation\n",
    "        #print(\"Pre-First RNN Layer Shape:\", x.shape)  # Print statement here\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.permute(0, 2, 1)  # Restore original axes order\n",
    "        #print(\"Post-CNN Shape:\", x.shape)  # Debug print statement\n",
    "\n",
    "        # Process RNN layers\n",
    "        for i, rnn in enumerate(self.rnn_layers):\n",
    "            #print(f\"Pre-RNN Layer {i} Shape:\", x.shape)  # Print statement here\n",
    "            #rnn.flatten_parameters()  # Flatten RNN parameters\n",
    "            x, _ = rnn(x)\n",
    "            #print(f\"Post-RNN Layer {i} Shape:\", x.shape)  # Debug print statement\n",
    "            if i < self.num_layers - 1:  # Apply dropout manually between RNN layers, but not after the last layer\n",
    "                x = self.dropout_layers[i](x)\n",
    "\n",
    "        # Separate forward and backward outputs\n",
    "        fw_outputs, bw_outputs = x.chunk(2, dim=2)\n",
    "\n",
    "        # Concatenate forward and backward outputs\n",
    "        outputs = torch.cat([fw_outputs, bw_outputs], dim=-1)\n",
    "\n",
    "        # Apply Multi-Head Attention\n",
    "        attn_output, _ = self.multihead_attn(outputs, outputs, outputs)\n",
    "        #print(\"Post-Attention Shape:\", attn_output.shape)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_weights = F.softmax(self.attention(attn_output), dim=1)\n",
    "        x = torch.sum(attn_output * attention_weights, dim=1)\n",
    "        #print(\"Post-Weighted Attention Shape:\", x.shape)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        outputs = self.fc(x)\n",
    "        #print(\"Post-Fully Connected Shape:\", outputs.shape)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridLSTMGRUAttentionWithCNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_layer_size: int, num_layers: int, output_size: int, dropout_rate: float, num_heads: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # CNN layers\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=hidden_layer_size // 2, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm1d(hidden_layer_size // 2),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.cnn_output_size = hidden_layer_size // 2  # Output size of CNN layers\n",
    "\n",
    "        self.my_tcn = MyTCNModule(input_channels=hidden_layer_size // 2, nb_filters=64, kernel_size=2, dilations=[1, 2, 4, 8, 16], dropout_p=0.2)\n",
    "\n",
    "        # Define LSTM and GRU layers alternatively\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            is_last_layer = i == num_layers - 1\n",
    "            layer_dropout = 0 if is_last_layer else dropout_rate\n",
    "            #input_dim = self.cnn_output_size if i == 0 else (hidden_layer_size if i % 2 == 0 else 2 * hidden_layer_size)\n",
    "            input_dim = self.cnn_output_size if i == 0 else 2 * hidden_layer_size\n",
    "            rnn_type = nn.LSTM if i % 2 == 0 else nn.GRU\n",
    "            self.rnn_layers.append(rnn_type(input_dim, hidden_layer_size, batch_first=True, bidirectional=True, dropout=layer_dropout))\n",
    "\n",
    "        # Multi-Head Attention layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=2 * hidden_layer_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(2 * hidden_layer_size, 1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_layer_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, output_size)\n",
    "        )\n",
    "\n",
    "        # Manual dropout layers between RNN layers\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(dropout_rate) for _ in range(num_layers - 1)])\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.fill_(0)\n",
    "            elif isinstance(m, nn.GRU):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(\"Pre-Permuation RNN Layer Shape:\", x.shape)  # Print statement here\n",
    "        # Apply CNN layers\n",
    "        x = x.permute(0, 2, 1)  # Change axes order for CNN operation\n",
    "        #print(\"Pre-First RNN Layer Shape:\", x.shape)  # Print statement here\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.permute(0, 2, 1)  # Restore original axes order\n",
    "        #print(\"Post-CNN Shape:\", x.shape)  # Debug print statement\n",
    "        \n",
    "        # Pass the CNN output through the TCN module\n",
    "        x = self.my_tcn(x)\n",
    "        # Process RNN layers\n",
    "        for i, rnn in enumerate(self.rnn_layers):\n",
    "            #print(f\"Pre-RNN Layer {i} Shape:\", x.shape)  # Print statement here\n",
    "            #rnn.flatten_parameters()  # Flatten RNN parameters\n",
    "            x, _ = rnn(x)\n",
    "            #print(f\"Post-RNN Layer {i} Shape:\", x.shape)  # Debug print statement\n",
    "            if i < self.num_layers - 1:  # Apply dropout manually between RNN layers, but not after the last layer\n",
    "                x = self.dropout_layers[i](x)\n",
    "\n",
    "        # Separate forward and backward outputs\n",
    "        fw_outputs, bw_outputs = x.chunk(2, dim=2)\n",
    "\n",
    "        # Concatenate forward and backward outputs\n",
    "        outputs = torch.cat([fw_outputs, bw_outputs], dim=-1)\n",
    "\n",
    "        # Apply Multi-Head Attention\n",
    "        attn_output, _ = self.multihead_attn(outputs, outputs, outputs)\n",
    "        #print(\"Post-Attention Shape:\", attn_output.shape)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_weights = F.softmax(self.attention(attn_output), dim=1)\n",
    "        x = torch.sum(attn_output * attention_weights, dim=1)\n",
    "        #print(\"Post-Weighted Attention Shape:\", x.shape)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        outputs = self.fc(x)\n",
    "        #print(\"Post-Fully Connected Shape:\", outputs.shape)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MtG\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already defined your AdvancedLSTM class with appropriate changes\n",
    "\n",
    "# Define the number of features and prediction length\n",
    "num_features = 30  # Number of features in the dataset\n",
    "prediction_length = 30  # Number of days you want to predict\n",
    "\n",
    "# Instantiate the model\n",
    "model = HybridLSTMGRUAttentionWithCNN(input_size=30, \n",
    "                     hidden_layer_size=600, \n",
    "                     num_layers=30, \n",
    "                     output_size=prediction_length * num_features,\n",
    "                     dropout_rate= 0.5,\n",
    "                     num_heads= 6, kernel_size= 6).to(device)  # output_size is 30 days * 9 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Decay L2 Reg\n",
    "\n",
    "The weight_decay parameter in neural network training, which applies L2 regularization, has a significant impact on how the model learns. Here's how changing it affects the training process:\n",
    "\n",
    "    Increasing Weight Decay:\n",
    "        This means you're increasing the strength of L2 regularization.\n",
    "        It penalizes larger weights more heavily, encouraging the model to keep the weights smaller and more uniform.\n",
    "        This can help prevent overfitting, especially in complex models or when you have limited training data, as it encourages the model to be simpler.\n",
    "        However, if the weight decay is set too high, it might lead to underfitting, where the model becomes overly simple and fails to capture the underlying patterns in the data.\n",
    "\n",
    "    Decreasing Weight Decay:\n",
    "        Lowering the weight decay reduces the strength of L2 regularization.\n",
    "        It allows the model to have larger weights, which can be beneficial if the model is too simple and underfitting.\n",
    "        This gives the model more flexibility to learn from the data, which can be useful if your model is not capturing complex patterns in the training data.\n",
    "        On the flip side, too little regularization might lead to overfitting, where the model learns the noise in the training data rather than the actual signal.\n",
    "\n",
    "In summary, adjusting the weight decay is a balancing act:\n",
    "\n",
    "    Increase it if your model is overfitting (i.e., it performs well on training data but poorly on validation/test data).\n",
    "    Decrease it if your model is underfitting (i.e., it's not performing well even on the training data).\n",
    "\n",
    "Finding the right level of regularization is crucial for training effective neural network models. It often requires some experimentation and tuning, alongside monitoring the model’s performance on both training and validation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate\n",
    "\n",
    "Adjusting the learning rate, which is a key hyperparameter in neural network training, significantly impacts the learning dynamics. Here's how changing it affects the training process:\n",
    "\n",
    "    Increasing the Learning Rate:\n",
    "        Faster Learning: A higher learning rate can speed up the learning process. The model's weights are updated more significantly with each iteration, potentially leading to faster convergence.\n",
    "        Risk of Instability: However, a too high learning rate can cause the training to become unstable. The model might overshoot the optimal points in the loss landscape, leading to erratic behavior in loss and other metrics.\n",
    "        Potential to Skip Optimal Solutions: If the learning rate is excessively high, the optimizer might skip over minima, failing to converge to the best solution.\n",
    "\n",
    "    Decreasing the Learning Rate:\n",
    "        More Stable, Gradual Learning: A lower learning rate results in smaller, more precise updates to the weights. This can lead to more stable convergence and finer adjustments to the model parameters.\n",
    "        Risk of Slow Convergence: While stability is increased, the downside is that learning might be slower. The model might take a longer time to converge to an optimal solution.\n",
    "        Potential for Getting Stuck in Local Minima: With a very low learning rate, there's a risk that the model might get stuck in local minima or take an excessively long time to escape flat regions in the loss landscape.\n",
    "\n",
    "In summary, the learning rate needs to be carefully chosen:\n",
    "\n",
    "    Increase it if the training process is too slow, and there are no signs of convergence.\n",
    "    Decrease it if the training process is unstable, with erratic loss values or failure to improve the loss over epochs.\n",
    "\n",
    "Typically, it's common to start with a higher learning rate and then reduce it as training progresses (using learning rate schedulers). This approach allows for rapid learning initially and more refined adjustments later in the training. Experimenting with different learning rates and monitoring the training/validation performance is essential to find the best setting for your specific model and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 Regularization techniques\n",
    "\n",
    "L1 and L2 regularization are powerful techniques to prevent overfitting in machine learning models, including neural networks. Here's how to discern when and how to manipulate these regularization parameters:\n",
    "L1 Regularization (Lasso):\n",
    "\n",
    "    Characteristics: L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients. It can lead to sparse models where some weights become exactly zero, effectively performing feature selection.\n",
    "    When to Use:\n",
    "        If you suspect or know that only a subset of features is important, L1 can help in identifying them.\n",
    "        Useful in models with high dimensionality (more features than samples).\n",
    "    Adjustment Strategy:\n",
    "        Increase the L1 penalty (lambda) if the model is overfitting (i.e., performing well on training data but poorly on validation data).\n",
    "        Decrease if the model is underfitting or too many features are being zeroed out, causing loss of important information.\n",
    "\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "    Characteristics: L2 regularization adds a penalty equal to the square of the magnitude of coefficients. It doesn’t encourage sparse models but distributes the error among all the features.\n",
    "    When to Use:\n",
    "        Generally useful in most scenarios where overfitting is a concern.\n",
    "        Particularly effective in neural networks to prevent weights from becoming too large, leading to overfitting.\n",
    "    Adjustment Strategy:\n",
    "        Increase the L2 penalty (weight decay) if the model is overfitting.\n",
    "        Decrease if the model is underfitting or if training doesn’t converge.\n",
    "\n",
    "Practical Tips for Manipulating L1/L2:\n",
    "\n",
    "    Start with L2: L2 regularization is usually the first go-to method as it's less aggressive in terms of feature reduction and generally improves generalization.\n",
    "    Combining L1 and L2: Elastic Net regularization combines both L1 and L2 penalties. It can be a good middle ground if you’re unsure which to choose.\n",
    "    Scale of Regularization Parameters: Start with smaller values (e.g., 0.01, 0.001) and increase gradually. Very high values can overly constrain the model, leading to underfitting.\n",
    "    Monitoring Performance: Regularly monitor training and validation loss. If validation loss decreases but training loss increases, it could indicate over-regularization.\n",
    "    Automated Hyperparameter Tuning: Consider using techniques like grid search, random search, or Bayesian optimization for systematic tuning of these parameters.\n",
    "    Regularization and Learning Rate: When adjusting regularization parameters, keep an eye on the learning rate. A higher learning rate may require stronger regularization to combat overfitting.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "The decision to adjust L1 or L2 regularization depends largely on the specific characteristics of your problem and dataset. Regularization is a balancing act – too little might lead to overfitting, too much to underfitting. Careful experimentation and monitoring are key to finding the right levels of L1 and L2 regularization for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 REG more details\n",
    "\n",
    "L1 regularization, also known as Lasso regularization, encourages sparsity in the parameter weights of the model. This means it not only penalizes the magnitude of the weights to prevent overfitting but also can drive some of those weights to zero, effectively \"unlearning\" or removing some features from the model.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "    The L1 penalty is the sum of the absolute values of the weights.\n",
    "    When applied during training, it can cause some weights to shrink to zero, especially for less important or redundant features.\n",
    "    This can be particularly useful when you have a high-dimensional dataset with many features that may not all be relevant to the prediction task.\n",
    "\n",
    "The consequence of this \"unlearning\" is that L1 regularization can be seen as a form of automatic feature selection. By driving certain weights to zero, the model ends up using fewer features, which can make the model simpler, faster, and more interpretable. However, this also means that L1 regularization can lead to a biased model if the penalty is too strong, as it might remove features that are actually important.\n",
    "\n",
    "The key with L1 (and all forms of regularization) is to find the right balance between fitting the training data and maintaining a model that can generalize well to unseen data. This balance is usually found through hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model(model, filepath=r'C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher_Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filepath=r'C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher_Model.pth')\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddDataToTensorboard(avg_tpb, avg_loss, avg_mse, avg_mae, avg_rmse, avg_r2, model, global_step):\n",
    "    \"\"\"\n",
    "    This method will populate the tensorboard with variables we want to watch.\n",
    "    :param writer: The SummaryWriter object from TensorBoard.\n",
    "    :param avg_tpb: Average time per batch.\n",
    "    :param avg_loss: Average loss.\n",
    "    :param avg_mse: Average mean squared error.\n",
    "    :param avg_mae: Average mean absolute error.\n",
    "    :param avg_rmse: Average root mean squared error.\n",
    "    :param avg_r2: Average R-squared value.\n",
    "    :param model: The PyTorch model from which to log parameters and gradients.\n",
    "    :param global_step: The global step value to track progress in the logs.\n",
    "    \"\"\"\n",
    "    writer.add_scalar('Average Time Per Batch', avg_tpb, global_step)\n",
    "    writer.add_scalar('Average Loss', avg_loss, global_step)\n",
    "    writer.add_scalar('Average MSE', avg_mse, global_step)\n",
    "    writer.add_scalar('Average MAE', avg_mae, global_step)\n",
    "    writer.add_scalar('Average RMSE', avg_rmse, global_step)\n",
    "    writer.add_scalar('Average R2', avg_r2, global_step)\n",
    "    \n",
    "    # Make sure the model's parameters have gradients before trying to log them\n",
    "    for tag, value in model.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        if value.grad is not None:\n",
    "            writer.add_histogram(tag, value.data.cpu().numpy(), global_step)\n",
    "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), global_step)\n",
    "\n",
    "# Function to calculate and log feature metrics\n",
    "def calculate_and_log_feature_metrics(y_true, y_pred, feature_names, global_step):\n",
    "    \"\"\"\n",
    "    Calculate and log metrics for individual features.\n",
    "    \"\"\"\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_true = y_true[:, i]\n",
    "        feature_pred = y_pred[:, i]\n",
    "        \n",
    "        mse = mean_squared_error(feature_true, feature_pred)\n",
    "        mae = mean_absolute_error(feature_true, feature_pred)\n",
    "        r2 = r2_score(feature_true, feature_pred)\n",
    "        rmse = np.sqrt(mse)  # Calculate RMSE\n",
    "\n",
    "        # Log the metrics for the current feature\n",
    "        writer.add_scalar(f'MSE/{feature_name}', mse, global_step)\n",
    "        writer.add_scalar(f'MAE/{feature_name}', mae, global_step)\n",
    "        writer.add_scalar(f'R2/{feature_name}', r2, global_step)\n",
    "        writer.add_scalar(f'RMSE/{feature_name}', rmse, global_step)\n",
    "\n",
    "def linear_interpolate(x, x_min, x_max, y_min, y_max):\n",
    "    \"\"\" Linearly interpolates a value 'x' in range [x_min, x_max] to range [y_min, y_max] \"\"\"\n",
    "    return y_min + (y_max - y_min) * ((x - x_min) / (x_max - x_min))\n",
    "\n",
    "def FindOptimalTraining(x_train_tensor):\n",
    "    \"\"\"\n",
    "    Takes the x_train_tensor and checks the row count to modify batch size and learning rate based on the iteration to optimize work.\n",
    "    \"\"\"\n",
    "    row_count = x_train_tensor.size()[0]\n",
    "\n",
    "    # Ensure row_count is within expected bounds\n",
    "    row_count = max(100, min(5000, row_count))\n",
    "\n",
    "    # Linearly interpolate values\n",
    "    batch_size = int(linear_interpolate(row_count, 100, 5000, 8, 64))\n",
    "    learning_rate = linear_interpolate(row_count, 100, 5000, 0.00247, 0.0010)\n",
    "    weight_decay = 0.0000005806\n",
    "    #weight_decay = linear_interpolate(row_count, 100, 5000, 0.0000005806, 0.0000005806)\n",
    "\n",
    "    # Ensure batch size does not exceed the dataset size\n",
    "    batch_size = min(batch_size, row_count)\n",
    "    \n",
    "    print(f\"Learning Rate: {learning_rate:.6f}, Batch Size: {batch_size}, Weight Decay: {weight_decay:.10f} , X-Tensor Shape: {x_train_tensor.shape}\")\n",
    "    return (learning_rate, batch_size, weight_decay)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filepath='model_checkpoint.pth'):\n",
    "    \"\"\"\n",
    "    Save a model checkpoint.\n",
    "\n",
    "    :param model: PyTorch model to save.\n",
    "    :param optimizer: Optimizer whose state we want to save.\n",
    "    :param epoch: Current epoch number.\n",
    "    :param loss: Current loss value.\n",
    "    :param filepath: Path to save the checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss if loss is not None else 'N/A',  # Optionally store the loss value\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved to {filepath}\")\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    Load a model checkpoint.\n",
    "\n",
    "    :param filepath: Path to the checkpoint file.\n",
    "    :param model: PyTorch model to load the parameters into.\n",
    "    :param optimizer: Optimizer to load the state into.\n",
    "    :param device: The device to load the model onto.\n",
    "    :return: The checkpoint epoch and loss.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint.get('loss', None)  # Retrieve the loss if available, else None\n",
    "    print(f\"Checkpoint loaded from {filepath} at epoch {epoch} with loss {loss}\")\n",
    "    return epoch, loss\n",
    "\n",
    "def save_model(model, filepath='model.pth'):\n",
    "    \"\"\"\n",
    "    Save a model's state_dict.\n",
    "\n",
    "    :param model: PyTorch model to save.\n",
    "    :param filepath: Path to save the model.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath):\n",
    "    \"\"\"\n",
    "    Load a model's state_dict from a given filepath.\n",
    "\n",
    "    :param filepath: Path to the model file.\n",
    "    :return: Loaded PyTorch model.\n",
    "    \"\"\"\n",
    "    prediction_length = 30\n",
    "    num_features = 30\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define your model architecture\n",
    "    model = HybridLSTMGRUAttentionWithCNN(input_size=30, \n",
    "                                          hidden_layer_size=600, \n",
    "                                          num_layers=30, \n",
    "                                          output_size=prediction_length * num_features,\n",
    "                                          dropout_rate=0.5,\n",
    "                                          num_heads=6, \n",
    "                                          kernel_size=6).to(device)\n",
    "    \n",
    "    try:\n",
    "        # Load the saved model state into the model\n",
    "        model_state = torch.load(filepath, map_location=device)\n",
    "        model.load_state_dict(model_state['model_state_dict'])  # Ensure this matches the key used when saving\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "    except KeyError:\n",
    "        print(\"Error: Incompatible state dict keys. Please check the model architecture and the state dict.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_data_for_prediction(normalized_data, look_back):\n",
    "    \"\"\"\n",
    "    Prepare data for prediction based on the look_back period.\n",
    "    :param normalized_data: Normalized data (2D array where rows are timesteps and columns are features).\n",
    "    :param look_back: Number of timesteps to look back for making the prediction.\n",
    "    :return: Data prepared for prediction.\n",
    "    \"\"\"\n",
    "    # Use the last 'look_back' timesteps to make the prediction\n",
    "    return normalized_data[-look_back:]\n",
    "\n",
    "def preprocess_input(input_data, fitted_standard_scaler, fitted_min_max_scaler):\n",
    "    \"\"\"\n",
    "    Standardize and normalize new input data using the already fitted scalers from the training data.\n",
    "    :param input_data: New data to be preprocessed (2D array where rows are timesteps and columns are features).\n",
    "    :param fitted_standard_scaler: The StandardScaler instance fitted on the training data.\n",
    "    :param fitted_min_max_scaler: The MinMaxScaler instance fitted on the training data.\n",
    "    :return: Preprocessed data.\n",
    "    \"\"\"\n",
    "    standardized_data = fitted_standard_scaler.transform(input_data)\n",
    "    normalized_data = fitted_min_max_scaler.transform(standardized_data)\n",
    "    return normalized_data\n",
    "\n",
    "# Example usage:\n",
    "# new_raw_data = ...  # New data to be predicted (should be a 2D array)\n",
    "# preprocessed_data = preprocess_input(new_raw_data, standard_scaler, min_max_scaler)\n",
    "\n",
    "def predict(model, prepared_data, device):\n",
    "    \"\"\"\n",
    "    Make a prediction based on prepared input data.\n",
    "    :param model: Trained LSTM model.\n",
    "    :param prepared_data: Prepared data for prediction (should be a 2D array of shape (look_back, number_of_features)).\n",
    "    :param device: The device (CPU or CUDA) for running the prediction.\n",
    "    :return: Predicted values for future sequence.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        if not isinstance(prepared_data, np.ndarray):\n",
    "            raise ValueError(\"prepared_data should be a numpy array\")\n",
    "        if prepared_data.shape[1] != 30:  # Ensure the second dimension matches the number of features\n",
    "            raise ValueError(\"prepared_data shape second dimension must be 9\")\n",
    "        \n",
    "        # Convert prepared data to a PyTorch tensor and add a batch dimension\n",
    "        input_tensor = torch.from_numpy(prepared_data).float().to(device)\n",
    "        input_tensor = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "        # Get the prediction from the model\n",
    "        prediction = model(input_tensor)\n",
    "\n",
    "        # Move the prediction back to the CPU if it was on CUDA\n",
    "        prediction = prediction.cpu()\n",
    "\n",
    "        # Reshape the output to remove batch dimension and get the actual prediction values\n",
    "        prediction = prediction.view(-1, 30)  # Reshape to match your output format (e.g., (30, 9))\n",
    "        return prediction.numpy()  # Convert to numpy array for easier handling\n",
    "    \n",
    "def revert_scaling(predictions, min_max_scaler, standard_scaler, feature_columns):\n",
    "    \"\"\"\n",
    "    Revert the scaling of the predictions to their original scale.\n",
    "\n",
    "    :param predictions: The predictions from the model, assumed to be a numpy array.\n",
    "    :param min_max_scaler: The MinMaxScaler instance used for scaling the data.\n",
    "    :param standard_scaler: The StandardScaler instance used for scaling the data.\n",
    "    :param feature_columns: List of feature names corresponding to the columns.\n",
    "    :return: DataFrame with predictions reverted to original scale.\n",
    "    \"\"\"\n",
    "    # Inverse MinMax Scaling\n",
    "    predictions = min_max_scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Inverse Standard Scaling\n",
    "    predictions = standard_scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions, columns=feature_columns)\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have the fitted min_max_scaler, standard_scaler, and the list of feature columns\n",
    "# predictions = ... # Output from the predict function\n",
    "# reverted_predictions_df = revert_scaling(predictions, min_max_scaler, standard_scaler, feature_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher2_Checkpoint-0.pth at epoch 24 with loss 20.021221751677686\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is MNRO\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001000, Batch Size: 64, Weight Decay: 0.0000005806 , X-Tensor Shape: torch.Size([5895, 60, 30])\n",
      "Iteration: 0, Epoch: 0, Avg Time per Batch: 0.3788 sec, Loss: 5.92551101, MSE: 0.0536, MAE: 0.1739, RMSE: 0.2315, R2: -0.9482\n",
      "Iteration: 0, Epoch: 1, Avg Time per Batch: 0.3676 sec, Loss: 5.44078856, MSE: 0.0527, MAE: 0.1685, RMSE: 0.2295, R2: -0.8569\n",
      "Iteration: 0, Epoch: 2, Avg Time per Batch: 0.3600 sec, Loss: 5.31585516, MSE: 0.0525, MAE: 0.1663, RMSE: 0.2292, R2: -0.8244\n",
      "Iteration: 0, Epoch: 3, Avg Time per Batch: 0.3606 sec, Loss: 5.27750414, MSE: 0.0526, MAE: 0.1651, RMSE: 0.2293, R2: -0.8097\n",
      "Iteration: 0, Epoch: 4, Avg Time per Batch: 0.3608 sec, Loss: 5.27412072, MSE: 0.0526, MAE: 0.1644, RMSE: 0.2294, R2: -0.8021\n",
      "Iteration: 0, Epoch: 5, Avg Time per Batch: 0.3633 sec, Loss: 5.28880797, MSE: 0.0527, MAE: 0.1639, RMSE: 0.2295, R2: -0.7966\n",
      "Iteration: 0, Epoch: 6, Avg Time per Batch: 0.3571 sec, Loss: 5.31091803, MSE: 0.0527, MAE: 0.1636, RMSE: 0.2295, R2: -0.7927\n",
      "Iteration: 0, Epoch: 7, Avg Time per Batch: 0.3601 sec, Loss: 5.33786264, MSE: 0.0527, MAE: 0.1633, RMSE: 0.2296, R2: -0.7890\n",
      "Iteration: 0, Epoch: 8, Avg Time per Batch: 0.3539 sec, Loss: 5.36764583, MSE: 0.0527, MAE: 0.1631, RMSE: 0.2296, R2: -0.7872\n",
      "Iteration: 0, Epoch: 9, Avg Time per Batch: 0.3542 sec, Loss: 5.40011509, MSE: 0.0527, MAE: 0.1629, RMSE: 0.2297, R2: -0.7860\n",
      "Iteration: 0, Epoch: 10, Avg Time per Batch: 0.3546 sec, Loss: 5.43311906, MSE: 0.0528, MAE: 0.1628, RMSE: 0.2297, R2: -0.7844\n",
      "Iteration: 0, Epoch: 11, Avg Time per Batch: 0.3522 sec, Loss: 5.46634955, MSE: 0.0528, MAE: 0.1627, RMSE: 0.2297, R2: -0.7829\n",
      "Iteration: 0, Epoch: 12, Avg Time per Batch: 0.3532 sec, Loss: 5.49965676, MSE: 0.0528, MAE: 0.1626, RMSE: 0.2298, R2: -0.7822\n",
      "Iteration: 0, Epoch: 13, Avg Time per Batch: 0.3565 sec, Loss: 5.53234523, MSE: 0.0528, MAE: 0.1625, RMSE: 0.2298, R2: -0.7805\n",
      "Iteration: 0, Epoch: 14, Avg Time per Batch: 0.3681 sec, Loss: 5.56547581, MSE: 0.0528, MAE: 0.1625, RMSE: 0.2298, R2: -0.7799\n",
      "Iteration: 0, Epoch: 15, Avg Time per Batch: 0.3831 sec, Loss: 5.59711021, MSE: 0.0528, MAE: 0.1624, RMSE: 0.2298, R2: -0.7795\n",
      "Iteration: 0, Epoch: 16, Avg Time per Batch: 0.3776 sec, Loss: 5.62815092, MSE: 0.0528, MAE: 0.1624, RMSE: 0.2298, R2: -0.7788\n"
     ]
    }
   ],
   "source": [
    "# Train the model <<< THIS SEEMS TO WORK WELL!!!!\n",
    "\n",
    "epochs = 25\n",
    "model.train()\n",
    "iteration = 0\n",
    "l1_lambda = 0.00025 #0.001 was this earlier\n",
    "start_time = time.time()\n",
    "tpb_list = []\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.0010\n",
    "weightDecay = 0.00001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weightDecay)\n",
    "filepath = f\"C:\\\\Machine Learning\\\\SPY VIX 9 Input Output Model\\\\MegaNumMuncher2_Checkpoint-0.pth\"\n",
    "load_checkpoint(filepath, model, optimizer, device)\n",
    "\n",
    "for iteration in range(50):\n",
    "    TensorIntSystem = TensorIntegrationSystem(\"1994-01-01\", \"2018-01-01\")\n",
    "    TensorIntSystem.CreateCombinedDataFrame()\n",
    "    # Extract feature names from the DataFrame\n",
    "    feature_names = TensorIntSystem.FormattedCombinedDataFrame.columns.tolist()\n",
    "    \n",
    "    x_train_tensor, y_train_tensor = TensorIntSystem.CreateTensors()\n",
    "    new_learning_rate, batch_size, new_weightDecay = FindOptimalTraining(x_train_tensor)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_learning_rate\n",
    "        param_group['weight_decay'] = new_weightDecay\n",
    "\n",
    "    # Initialize metrics for each iteration\n",
    "    running_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_r2 = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        permutation = torch.randperm(x_train_tensor.size(0))\n",
    "\n",
    "        for i in range(0, x_train_tensor.size(0), batch_size):\n",
    "            batch_start_time = time.time()\n",
    "\n",
    "            # Adjust batch size for the last batch\n",
    "            #end_idx = min(i + batch_size, x_train_tensor.size(0))\n",
    "            \n",
    "            end_idx = i + batch_size\n",
    "            # Adjust for the last batch if it has fewer samples\n",
    "            # Skip the batch if it does not fit the expected shape\n",
    "            if end_idx > x_train_tensor.size(0) - batch_size:\n",
    "                continue\n",
    "\n",
    "            indices = permutation[i:end_idx]\n",
    "        \n",
    "            batch_seq = x_train_tensor[indices].to(device)\n",
    "            batch_label = y_train_tensor[indices].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_seq)\n",
    "            y_pred = y_pred.view(batch_label.size())\n",
    "            loss = loss_function(y_pred, batch_label)\n",
    "\n",
    "            l1_penalty = sum(p.abs().sum() for p in model.parameters())\n",
    "            total_loss = loss + l1_lambda * l1_penalty\n",
    "            running_loss += total_loss.item()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_pred_flat = y_pred.view(y_pred.size(0), -1).cpu().detach().numpy()\n",
    "            batch_label_flat = batch_label.view(batch_label.size(0), -1).cpu().detach().numpy()\n",
    "\n",
    "            mse = mean_squared_error(batch_label_flat, y_pred_flat)\n",
    "            mae = mean_absolute_error(batch_label_flat, y_pred_flat)\n",
    "            r2 = r2_score(batch_label_flat, y_pred_flat)\n",
    "\n",
    "            total_mse += mse\n",
    "            total_mae += mae\n",
    "            total_r2 += r2\n",
    "            num_batches += 1\n",
    "\n",
    "            batch_end_time = time.time()\n",
    "            tpb_list.append(batch_end_time - batch_start_time)\n",
    "            \n",
    "            # Assuming `writer` is your SummaryWriter instance and `global_step` is defined\n",
    "            calculate_and_log_feature_metrics(batch_label_flat, y_pred_flat, feature_names, global_step)\n",
    "\n",
    "        # Calculate average metrics for each epoch\n",
    "        avg_tpb = sum(tpb_list) / len(tpb_list) if tpb_list else 0\n",
    "        avg_mse = total_mse / num_batches if num_batches else float('nan')\n",
    "        avg_mae = total_mae / num_batches if num_batches else float('nan')\n",
    "        avg_r2 = total_r2 / num_batches if num_batches else float('nan')\n",
    "        avg_rmse = sqrt(avg_mse) if avg_mse >= 0 else float('nan')\n",
    "        avg_loss = running_loss / num_batches if num_batches else float('nan')\n",
    "        global_step += 1\n",
    "        \n",
    "        print(f'Iteration: {iteration}, Epoch: {epoch}, Avg Time per Batch: {avg_tpb:.4f} sec, '\n",
    "              f'Loss: {avg_loss:.8f}, MSE: {avg_mse:.4f}, MAE: {avg_mae:.4f}, '\n",
    "              f'RMSE: {avg_rmse:.4f}, R2: {avg_r2:.4f}')\n",
    "        AddDataToTensorboard(avg_tpb, avg_loss, avg_mse, avg_mae, avg_rmse, avg_r2, model, global_step) #This will map data to tensorboard\n",
    "        \n",
    "        tpb_list.clear()\n",
    "        # At the end of your training loop\n",
    "        with open('C:\\\\Machine Learning\\\\SPY VIX 9 Input Output Model\\\\last_step.txt', 'w') as f:\n",
    "            f.write(str(global_step))\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    print(f'Epoch {epoch} completed in {epoch_end_time - epoch_start_time:.2f} seconds')\n",
    "\n",
    "    del x_train_tensor, y_train_tensor  # Delete model and tensors\n",
    "    torch.cuda.empty_cache()  # Clear cache\n",
    "    import gc\n",
    "    gc.collect()  # Invoke garbage collector\n",
    "\n",
    "    save_checkpoint(model, optimizer, epoch, avg_loss, filepath)\n",
    "    print(f'Checkpoint saved at Iteration: {iteration}, Epoch: {epoch}, Loss: {avg_loss:.10f}')\n",
    "    \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTUNA TESTER\n",
    "\n",
    "# {'learning_rate': 0.0013167734585262175,\n",
    "#  'weight_decay': 1.2816205616802949e-10,\n",
    "#  'batch_size': 24}\n",
    "\n",
    "def CreateTensors():\n",
    "    \"\"\"\n",
    "    Testing different sizes in Optuna\n",
    "    \"\"\"\n",
    "    TensorIntSystem = TensorIntegrationSystem(\"1994-01-01\", \"2018-01-01\")\n",
    "    TensorIntSystem.CreateCombinedDataFrame()\n",
    "    x_train_tensor, y_train_tensor = TensorIntSystem.CreateTensors()\n",
    "    return x_train_tensor, y_train_tensor\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized by Optuna\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-10, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [24, 32, 40, 48, 56, 64])\n",
    "\n",
    "    epochs = 5\n",
    "    l1_lambda = 0.001\n",
    "    tpb_list = []\n",
    "    \n",
    "    model = HybridLSTMGRUAttentionWithCNN(input_size=30, \n",
    "                                        hidden_layer_size=600, \n",
    "                                        num_layers=30, \n",
    "                                        output_size=30 * 30,\n",
    "                                        dropout_rate=0.5,\n",
    "                                        num_heads=6, \n",
    "                                        kernel_size=6).to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    x_train_tensor, y_train_tensor = CreateTensors()\n",
    "    \n",
    "    # Initialize metrics for each iteration\n",
    "    running_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_r2 = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        permutation = torch.randperm(x_train_tensor.size(0))\n",
    "\n",
    "        for i in range(0, x_train_tensor.size(0), batch_size):\n",
    "            batch_start_time = time.time()\n",
    "\n",
    "            end_idx = i + batch_size\n",
    "            # Adjust for the last batch if it has fewer samples\n",
    "            # Skip the batch if it does not fit the expected shape\n",
    "            if end_idx > x_train_tensor.size(0) - batch_size:\n",
    "                continue\n",
    "\n",
    "            indices = permutation[i:end_idx]\n",
    "        \n",
    "            batch_seq = x_train_tensor[indices].to(device)\n",
    "            batch_label = y_train_tensor[indices].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_seq)\n",
    "            y_pred = y_pred.view(batch_label.size())\n",
    "            loss = loss_function(y_pred, batch_label)\n",
    "\n",
    "            l1_penalty = sum(p.abs().sum() for p in model.parameters())\n",
    "            total_loss = loss + l1_lambda * l1_penalty\n",
    "            running_loss += total_loss.item()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_pred_flat = y_pred.view(y_pred.size(0), -1).cpu().detach().numpy()\n",
    "            batch_label_flat = batch_label.view(batch_label.size(0), -1).cpu().detach().numpy()\n",
    "\n",
    "            mse = mean_squared_error(batch_label_flat, y_pred_flat)\n",
    "            mae = mean_absolute_error(batch_label_flat, y_pred_flat)\n",
    "            r2 = r2_score(batch_label_flat, y_pred_flat)\n",
    "\n",
    "            total_mse += mse\n",
    "            total_mae += mae\n",
    "            total_r2 += r2\n",
    "            num_batches += 1\n",
    "\n",
    "            batch_end_time = time.time()\n",
    "            tpb_list.append(batch_end_time - batch_start_time)\n",
    "\n",
    "        # Calculate average metrics for each epoch\n",
    "        avg_tpb = sum(tpb_list) / len(tpb_list) if tpb_list else 0\n",
    "        avg_mse = total_mse / num_batches if num_batches else float('nan')\n",
    "        avg_mae = total_mae / num_batches if num_batches else float('nan')\n",
    "        avg_r2 = total_r2 / num_batches if num_batches else float('nan')\n",
    "        avg_rmse = sqrt(avg_mse) if avg_mse >= 0 else float('nan')\n",
    "        avg_loss = running_loss / num_batches if num_batches else float('nan')\n",
    "        \n",
    "        # Inside the epoch loop\n",
    "        print(f'Trial: {trial.number}, Epoch: {epoch}, Avg Time per Batch: {avg_tpb:.4f} sec, '\n",
    "            f'Loss: {avg_loss:.8f}, MSE: {avg_mse:.4f}, MAE: {avg_mae:.4f}, '\n",
    "            f'RMSE: {avg_rmse:.4f}, R2: {avg_r2:.4f}')\n",
    "        \n",
    "        tpb_list.clear()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    print(f'Epoch {epoch} completed in {epoch_end_time - epoch_start_time:.2f} seconds')\n",
    "\n",
    "    del x_train_tensor, y_train_tensor  # Delete model and tensors\n",
    "    torch.cuda.empty_cache()  # Clear cache\n",
    "    import gc\n",
    "    gc.collect()  # Invoke garbage collector\n",
    "\n",
    "    #print(f'Checkpoint saved at Epoch: {epoch}, Loss: {avg_loss:.10f}')\n",
    "    \n",
    "    # At the end of the objective function\n",
    "    return avg_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0013167734585262175,\n",
       " 'weight_decay': 1.2816205616802949e-10,\n",
       " 'batch_size': 24}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FrozenTrial(number=12, state=1, values=[63.34481708026323], datetime_start=datetime.datetime(2024, 1, 6, 22, 59, 1, 534618), datetime_complete=datetime.datetime(2024, 1, 6, 23, 9, 2, 250366), params={'learning_rate': 0.0013167734585262175, 'weight_decay': 1.2816205616802949e-10, 'batch_size': 24}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None), 'weight_decay': FloatDistribution(high=0.001, log=True, low=1e-10, step=None), 'batch_size': CategoricalDistribution(choices=(8, 16, 24, 32, 40, 48, 56, 64))}, trial_id=12, value=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study object and specify the direction is 'minimize'.\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Optimize the study, the objective function is being called here.\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a new raw data array to predict\n",
    "new_raw_data = df[['TickerOpen', 'TickerHigh', 'TickerLow', 'TickerClose', 'TickerVolume', 'VIXOpen', 'VIXHigh', 'VIXClose', 'VIXLow']].tail(60).values\n",
    "\n",
    "# Preprocess the new data (standardize and normalize)\n",
    "preprocessed_data = preprocess_input(new_raw_data, standard_scaler, min_max_scaler)\n",
    "\n",
    "# Prepare the data for prediction\n",
    "prepared_data = prepare_data_for_prediction(preprocessed_data, look_back)\n",
    "\n",
    "# Make a prediction\n",
    "prepared_data_tensor = torch.from_numpy(prepared_data).float().to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prepared_data_tensor = prepared_data_tensor.unsqueeze(0)\n",
    "    prediction = model(prepared_data_tensor)\n",
    "    prediction = prediction.cpu().numpy()\n",
    "\n",
    "# Reshape the predictions to match your output format\n",
    "predictions_reshaped = prediction.reshape(-1, num_features)  # Adjust this as per your model's output\n",
    "\n",
    "# Apply the inverse transformations to revert the predictions to original scale\n",
    "predictions_min_max_inversed = min_max_scaler.inverse_transform(predictions_reshaped)\n",
    "original_scale_predictions = standard_scaler.inverse_transform(predictions_min_max_inversed)\n",
    "\n",
    "# Create a DataFrame for the reverted predictions\n",
    "reverted_df = pd.DataFrame(original_scale_predictions, columns=df.columns[:num_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['TickerOpen', 'TickerHigh', 'TickerLow', 'TickerClose', 'TickerVolume', 'VIXOpen', 'VIXHigh', 'VIXClose', 'VIXLow']].tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverted_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
