{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "import math\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import ta\n",
    "#KMPFXIZR8T9GDJ1I Alpha Vantage key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How we can adjust the model\n",
    "\n",
    "#1 we can actually tune the learning rate\n",
    "#2 we can tune L1 regularization\n",
    "#2 We can tune L2 Regularization\n",
    "#3 batch size we can tune\n",
    "#4 we can change epoch run time\n",
    "#5 we can validate the model via data from 2018 and on as planned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "We're gonna need to make a class for the generation of the tensors, I want specific things, I need to randomize the datasets because it promotes diversity, I also need to ensure that the dataframe that is hosting the list\n",
    "retains its values outside of the program thus I need some kind of a save object, we're gonna need to save the credentials inside a json file, the class will have to retain the object and be loadable and saveable when the model is saved\n",
    "this way when we run a new epoch we can remake the object from its saved state and then continue training, once all the data is used a column inside the dataframe within this data object will be wiped and reset, this will continue forever, until\n",
    "we choose to stop.\n",
    "\n",
    "This object will be used in the tensor creation process and it will simply output the tensors we need to use for training and also create the data we need for those tensors prior to extracting numpy frame values, standardizing and normalizing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Set pandas to display numbers without scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "\n",
    "look_back = 60\n",
    "look_forward = 30\n",
    "standard_scaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the beginning of your notebook or training loop\n",
    "if os.path.exists('C:\\\\Machine Learning\\\\SPY VIX 9 Input Output Model\\\\last_step.txt'):\n",
    "    with open('C:\\\\Machine Learning\\\\SPY VIX 9 Input Output Model\\\\last_step.txt', 'r') as f:\n",
    "        global_step = int(f.read())\n",
    "else:\n",
    "    global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TickerDataSystem(): #Simple system to create our list of data and return yFinance datasets\n",
    "    def __init__(self, start_date = None, end_date = None):\n",
    "        self.start_date = pd.to_datetime(start_date) #\"1994-01-01\"\n",
    "        self.end_date = pd.to_datetime(end_date) #\"2017-01-01\"  This will specifically be used to cap off data for a specific criteria\n",
    "        self.new_listing = None\n",
    "\n",
    "    def modify_dates(self, df, date_column, start_date, end_date): #Specifically to drop specific dates before and after Vix start and end date\n",
    "        # Convert date_column to datetime\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        \n",
    "        # Set dates earlier than start_date to start_date\n",
    "        df.loc[df[date_column] < start_date, date_column] = start_date\n",
    "        \n",
    "        # Remove rows with dates later than end_date\n",
    "        df = df[df[date_column] <= end_date]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def CreateListingStatus(self):\n",
    "        # Specify the path to your CSV file\n",
    "        file_path = r'C:\\Machine Learning\\listing_status.csv' #from alpha vantage listing_status add api call later using BS4\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        listing_statusDF = pd.read_csv(file_path)\n",
    "\n",
    "        # List of columns to drop\n",
    "        columns_to_drop = ['name', 'exchange', 'assetType', 'delistingDate', 'status', '6']\n",
    "\n",
    "        # Drop the columns\n",
    "        new_ListingStatusDF = listing_statusDF.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "        # Drop the last 30 rows\n",
    "        new_ListingStatusDF = new_ListingStatusDF.iloc[:-30]\n",
    "\n",
    "        # Modify dates and filter rows based on the specified conditions\n",
    "        new_ListingStatusDF = self.modify_dates(new_ListingStatusDF, 'ipoDate', self.start_date, self.end_date)\n",
    "        \n",
    "        self.new_listing = new_ListingStatusDF\n",
    "        \n",
    "    def Get_Lists(self):\n",
    "        \n",
    "        return self.new_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorIntegrationSystem:  # This class will generate tensors on demand at random and retain their values as each creation \n",
    "    \"\"\"\n",
    "    This class is specifically for creating the tensors, adding the averages we want such as EMA and RSI to various components then finally outputting a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, startDate:str, endDate:str) -> None:\n",
    "        self.endDate = endDate\n",
    "        self.startDate = startDate\n",
    "        self.ProgressTracker = ProgressTracker() #Create progress tracker object to parse data from CSV and retain the list to randomize insertion of data\n",
    "        self.TickerDataSystem = TickerDataSystem(self.startDate, self.endDate) #This tool is specifically to create the date ranges we want from the listing_status CSV, eventually alpha vantage listings APi will be added\n",
    "        self.FormattedCombinedDataFrame = None\n",
    "        self.stockDataFrame = None\n",
    "        self.VIXDataFrame = None\n",
    "    \n",
    "    def dataFetcher(self, symbol: str, startDate: str):\n",
    "        \"\"\"\n",
    "        This will pull data from yF and test it if nothing is returned the subsequent method will test it and try again to create another ticker.\n",
    "        It will also grab the split data and hold it inside a property.\n",
    "        \"\"\"\n",
    "        #we gotta add a simple date modifier to change the enddate and add a year to it so we can pull more data as the IPORange is maxed at the self.endDate so we always have at least a year of data\n",
    "        time.sleep(0.75)\n",
    "        if symbol != \"^VIX\":\n",
    "            data = yf.download(symbol, start=startDate, end=self.endDate, auto_adjust = True)\n",
    "        else:\n",
    "            data = yf.download(symbol, start=startDate, end=self.endDate)\n",
    "            \n",
    "        if data.shape == (0, 6):\n",
    "            print(\"No Data\")\n",
    "            return None\n",
    "        else:\n",
    "            time.sleep(0.5)\n",
    "            # Create a Ticker object for Pfizer\n",
    "            tickerData = yf.Ticker(symbol)\n",
    "            return pd.DataFrame(data)\n",
    "        \n",
    "    def CreateCombinedDataFrame(self):\n",
    "        \"\"\"\n",
    "        This will create a random stock and VixData then create the formatted DataFrame and bind it to FormattedCombinedDataFrame.\n",
    "        \"\"\"\n",
    "        max_attempts = 5\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                self.CreateCombinedData()\n",
    "                break  # If successful, exit the loop\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt < max_attempts - 1:\n",
    "                    time.sleep(1)  # Wait for 1 second before next attempt\n",
    "                else:\n",
    "                    raise Exception(\"Failed to create combined data frame after several attempts.\")\n",
    "\n",
    "\n",
    "    def CreateCombinedData(self):\n",
    "        \"\"\"\n",
    "        This method will try to generate the data required for the CreateCombinedDataFrame\n",
    "        \"\"\"\n",
    "        self.VIXDataFrame = None\n",
    "        self.stockDataFrame = None\n",
    "        self.CreateNewVixData()\n",
    "        self.CreateNewStockData()\n",
    "        Formatter = FormatDataFrame(self.VIXDataFrame, self.stockDataFrame)\n",
    "        Formatter.CreateDataSet()\n",
    "        self.FormattedCombinedDataFrame = Formatter.GetCombinedData()\n",
    "    \n",
    "    \n",
    "    def CustomTickerCreator(self, ticker, tickerDate):\n",
    "        \"\"\"\n",
    "        This method will create custom ticker data for process into the predict features.\n",
    "        \"\"\"\n",
    "        self.VIXDataFrame = None\n",
    "        self.stockDataFrame = self.dataFetcher(ticker, tickerDate)\n",
    "        self.CreateNewVixData()\n",
    "        Formatter = FormatDataFrame(self.VIXDataFrame, self.stockDataFrame)\n",
    "        Formatter.CreateDataSet()\n",
    "        self.FormattedCombinedDataFrame = Formatter.GetCombinedData()\n",
    "        \n",
    "    def CreateNewVixData(self):\n",
    "        \"\"\"\n",
    "        This method is for creating VIX data and checking if it was created via the dataFetcher and Progress Tracker.\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        while True:\n",
    "            counter += 1\n",
    "            if counter == 10:\n",
    "                return\n",
    "            vixDataFrame = self.dataFetcher(\"^VIX\", \"1994-01-01\")\n",
    "            if vixDataFrame is None:\n",
    "                continue  # Try again in the next iteration\n",
    "            else:\n",
    "                self.VIXDataFrame = vixDataFrame\n",
    "                return\n",
    "        \n",
    "    def CreateNewStockData(self):\n",
    "        \"\"\"\n",
    "        This method is for creating stock data and checking if it was created via the dataFetcher and Progress Tracker.\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        while True:\n",
    "            ticker, tickerDate = self.ProgressTracker.random_next()\n",
    "            counter += 1\n",
    "            if counter == 10:\n",
    "                return\n",
    "            stockDataFrame = self.dataFetcher(ticker, tickerDate)\n",
    "            print(f\"This Ticker is {ticker}\")\n",
    "            if stockDataFrame is None:\n",
    "                continue  # Try again in the next iteration\n",
    "            else:\n",
    "                self.stockDataFrame = stockDataFrame\n",
    "                return\n",
    "            \n",
    "    def CreateTensors(self):\n",
    "        \"\"\"\n",
    "        Creates tensors that get output as x and y tensors with the required lookback and lookforward setup as public variables outside of the object (for now)\n",
    "        \"\"\"\n",
    "        # Create a new DataFrame with selected features\n",
    "        features = self.FormattedCombinedDataFrame.values\n",
    "\n",
    "        # Standardize data\n",
    "        standardized_features = standard_scaler.fit_transform(features)\n",
    "\n",
    "        # Normalize data\n",
    "        scaled_features = min_max_scaler.fit_transform(standardized_features)\n",
    "\n",
    "        # Prepare data for PyTorch (sequences and targets)\n",
    "        num_features = scaled_features.shape[1]\n",
    "\n",
    "        x_train, y_train = [], []\n",
    "        for i in range(look_back, len(scaled_features) - look_forward + 1):\n",
    "            x_train.append(scaled_features[i - look_back:i, :])  # Include all features from the past 60 days\n",
    "            y_train.append(scaled_features[i:i + look_forward, :])  # Include all features for the next 30 days\n",
    "\n",
    "        x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "        x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], num_features))\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        x_train_tensor = torch.tensor(x_train).float()\n",
    "        y_train_tensor = torch.tensor(y_train).float()\n",
    "\n",
    "        # If you're using a GPU, move tensors to GPU (assuming 'device' is defined)\n",
    "        x_train_tensor = x_train_tensor.to(device)\n",
    "        y_train_tensor = y_train_tensor.to(device)\n",
    "        \n",
    "        return x_train_tensor, y_train_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TensorIntegrationSystem(\"1994-01-01\",\"2024-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Data validation completed.\n"
     ]
    }
   ],
   "source": [
    "df = test.CustomTickerCreator(\"AAPL\",\"1994-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test.FormattedCombinedDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df.tail(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = newdf.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[7.4243e-04, 7.5937e-04, 7.4167e-04,  ..., 7.3973e-02,\n",
       "           9.4312e-01, 7.3163e-01],\n",
       "          [7.5178e-04, 7.5009e-04, 7.2287e-04,  ..., 7.6712e-02,\n",
       "           9.3907e-01, 7.3922e-01],\n",
       "          [7.2372e-04, 7.3154e-04, 7.0406e-04,  ..., 7.9452e-02,\n",
       "           9.3489e-01, 7.4674e-01],\n",
       "          ...,\n",
       "          [4.9374e-04, 5.1283e-04, 4.9166e-04,  ..., 3.0411e-01,\n",
       "           3.2579e-01, 9.6868e-01],\n",
       "          [5.1722e-04, 5.1283e-04, 4.8222e-04,  ..., 3.0685e-01,\n",
       "           3.1775e-01, 9.6561e-01],\n",
       "          [4.9374e-04, 4.9419e-04, 4.9166e-04,  ..., 3.0959e-01,\n",
       "           3.0977e-01, 9.6241e-01]],\n",
       " \n",
       "         [[7.5178e-04, 7.5009e-04, 7.2287e-04,  ..., 7.6712e-02,\n",
       "           9.3907e-01, 7.3922e-01],\n",
       "          [7.2372e-04, 7.3154e-04, 7.0406e-04,  ..., 7.9452e-02,\n",
       "           9.3489e-01, 7.4674e-01],\n",
       "          [7.2372e-04, 7.4081e-04, 6.9466e-04,  ..., 8.2192e-02,\n",
       "           9.3058e-01, 7.5418e-01],\n",
       "          ...,\n",
       "          [5.1722e-04, 5.1283e-04, 4.8222e-04,  ..., 3.0685e-01,\n",
       "           3.1775e-01, 9.6561e-01],\n",
       "          [4.9374e-04, 4.9419e-04, 4.9166e-04,  ..., 3.0959e-01,\n",
       "           3.0977e-01, 9.6241e-01],\n",
       "          [4.9374e-04, 4.7556e-04, 4.4918e-04,  ..., 3.1233e-01,\n",
       "           3.0184e-01, 9.5907e-01]],\n",
       " \n",
       "         [[7.2372e-04, 7.3154e-04, 7.0406e-04,  ..., 7.9452e-02,\n",
       "           9.3489e-01, 7.4674e-01],\n",
       "          [7.2372e-04, 7.4081e-04, 6.9466e-04,  ..., 8.2192e-02,\n",
       "           9.3058e-01, 7.5418e-01],\n",
       "          [7.1436e-04, 7.3154e-04, 7.0406e-04,  ..., 9.3151e-02,\n",
       "           9.1208e-01, 7.8319e-01],\n",
       "          ...,\n",
       "          [4.9374e-04, 4.9419e-04, 4.9166e-04,  ..., 3.0959e-01,\n",
       "           3.0977e-01, 9.6241e-01],\n",
       "          [4.9374e-04, 4.7556e-04, 4.4918e-04,  ..., 3.1233e-01,\n",
       "           3.0184e-01, 9.5907e-01],\n",
       "          [4.5148e-04, 4.7090e-04, 4.4446e-04,  ..., 3.1507e-01,\n",
       "           2.9398e-01, 9.5559e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[8.8288e-01, 8.8111e-01, 8.8071e-01,  ..., 4.7397e-01,\n",
       "           5.4831e-03, 5.7392e-01],\n",
       "          [8.9292e-01, 8.8886e-01, 8.9344e-01,  ..., 4.7671e-01,\n",
       "           4.2848e-03, 5.6540e-01],\n",
       "          [9.0029e-01, 9.0824e-01, 9.0399e-01,  ..., 4.7945e-01,\n",
       "           3.2332e-03, 5.5686e-01],\n",
       "          ...,\n",
       "          [9.2269e-01, 9.2110e-01, 9.2164e-01,  ..., 6.9315e-01,\n",
       "           3.3185e-01, 2.9120e-02],\n",
       "          [9.2901e-01, 9.3459e-01, 9.3159e-01,  ..., 6.9589e-01,\n",
       "           3.3998e-01, 2.6298e-02],\n",
       "          [9.3836e-01, 9.3189e-01, 9.3504e-01,  ..., 7.0411e-01,\n",
       "           3.6462e-01, 1.8674e-02]],\n",
       " \n",
       "         [[8.9292e-01, 8.8886e-01, 8.9344e-01,  ..., 4.7671e-01,\n",
       "           4.2848e-03, 5.6540e-01],\n",
       "          [9.0029e-01, 9.0824e-01, 9.0399e-01,  ..., 4.7945e-01,\n",
       "           3.2332e-03, 5.5686e-01],\n",
       "          [9.1114e-01, 9.0598e-01, 8.9222e-01,  ..., 4.8219e-01,\n",
       "           2.3286e-03, 5.4831e-01],\n",
       "          ...,\n",
       "          [9.2901e-01, 9.3459e-01, 9.3159e-01,  ..., 6.9589e-01,\n",
       "           3.3998e-01, 2.6298e-02],\n",
       "          [9.3836e-01, 9.3189e-01, 9.3504e-01,  ..., 7.0411e-01,\n",
       "           3.6462e-01, 1.8674e-02],\n",
       "          [9.4786e-01, 9.4231e-01, 9.4566e-01,  ..., 7.0685e-01,\n",
       "           3.7292e-01, 1.6416e-02]],\n",
       " \n",
       "         [[9.0029e-01, 9.0824e-01, 9.0399e-01,  ..., 4.7945e-01,\n",
       "           3.2332e-03, 5.5686e-01],\n",
       "          [9.1114e-01, 9.0598e-01, 8.9222e-01,  ..., 4.8219e-01,\n",
       "           2.3286e-03, 5.4831e-01],\n",
       "          [8.9454e-01, 8.9622e-01, 8.9126e-01,  ..., 4.8493e-01,\n",
       "           1.5713e-03, 5.3974e-01],\n",
       "          ...,\n",
       "          [9.3836e-01, 9.3189e-01, 9.3504e-01,  ..., 7.0411e-01,\n",
       "           3.6462e-01, 1.8674e-02],\n",
       "          [9.4786e-01, 9.4231e-01, 9.4566e-01,  ..., 7.0685e-01,\n",
       "           3.7292e-01, 1.6416e-02],\n",
       "          [9.4862e-01, 9.4928e-01, 9.5318e-01,  ..., 7.0959e-01,\n",
       "           3.8126e-01, 1.4302e-02]]], device='cuda:0'),\n",
       " tensor([[[4.9374e-04, 4.7556e-04, 4.4918e-04,  ..., 3.1233e-01,\n",
       "           3.0184e-01, 9.5907e-01],\n",
       "          [4.5148e-04, 4.7090e-04, 4.4446e-04,  ..., 3.1507e-01,\n",
       "           2.9398e-01, 9.5559e-01],\n",
       "          [4.5617e-04, 4.7556e-04, 4.3974e-04,  ..., 3.2329e-01,\n",
       "           2.7075e-01, 9.4436e-01],\n",
       "          ...,\n",
       "          [7.6609e-04, 7.5038e-04, 7.3238e-04,  ..., 4.2192e-01,\n",
       "           5.5394e-02, 7.2877e-01],\n",
       "          [7.3791e-04, 7.3641e-04, 7.2293e-04,  ..., 4.2466e-01,\n",
       "           5.1524e-02, 7.2109e-01],\n",
       "          [7.5200e-04, 7.5504e-04, 7.6070e-04,  ..., 4.2740e-01,\n",
       "           4.7788e-02, 7.1334e-01]],\n",
       " \n",
       "         [[4.5148e-04, 4.7090e-04, 4.4446e-04,  ..., 3.1507e-01,\n",
       "           2.9398e-01, 9.5559e-01],\n",
       "          [4.5617e-04, 4.7556e-04, 4.3974e-04,  ..., 3.2329e-01,\n",
       "           2.7075e-01, 9.4436e-01],\n",
       "          [4.9374e-04, 5.0817e-04, 4.7750e-04,  ..., 3.2603e-01,\n",
       "           2.6314e-01, 9.4035e-01],\n",
       "          ...,\n",
       "          [7.3791e-04, 7.3641e-04, 7.2293e-04,  ..., 4.2466e-01,\n",
       "           5.1524e-02, 7.2109e-01],\n",
       "          [7.5200e-04, 7.5504e-04, 7.6070e-04,  ..., 4.2740e-01,\n",
       "           4.7788e-02, 7.1334e-01],\n",
       "          [7.4261e-04, 7.4107e-04, 7.5126e-04,  ..., 4.3014e-01,\n",
       "           4.4185e-02, 7.0553e-01]],\n",
       " \n",
       "         [[4.5617e-04, 4.7556e-04, 4.3974e-04,  ..., 3.2329e-01,\n",
       "           2.7075e-01, 9.4436e-01],\n",
       "          [4.9374e-04, 5.0817e-04, 4.7750e-04,  ..., 3.2603e-01,\n",
       "           2.6314e-01, 9.4035e-01],\n",
       "          [5.1252e-04, 5.0817e-04, 4.8694e-04,  ..., 3.2877e-01,\n",
       "           2.5560e-01, 9.3621e-01],\n",
       "          ...,\n",
       "          [7.5200e-04, 7.5504e-04, 7.6070e-04,  ..., 4.2740e-01,\n",
       "           4.7788e-02, 7.1334e-01],\n",
       "          [7.4261e-04, 7.4107e-04, 7.5126e-04,  ..., 4.3014e-01,\n",
       "           4.4185e-02, 7.0553e-01],\n",
       "          [7.5200e-04, 7.6436e-04, 7.5598e-04,  ..., 4.3836e-01,\n",
       "           3.4189e-02, 6.8175e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[9.4786e-01, 9.4231e-01, 9.4566e-01,  ..., 7.0685e-01,\n",
       "           3.7292e-01, 1.6416e-02],\n",
       "          [9.4862e-01, 9.4928e-01, 9.5318e-01,  ..., 7.0959e-01,\n",
       "           3.8126e-01, 1.4302e-02],\n",
       "          [9.5731e-01, 9.5660e-01, 9.5759e-01,  ..., 7.1233e-01,\n",
       "           3.8963e-01, 1.2332e-02],\n",
       "          ...,\n",
       "          [9.8565e-01, 9.7890e-01, 9.7953e-01,  ..., 8.1096e-01,\n",
       "           6.9320e-01, 3.8835e-02],\n",
       "          [9.7772e-01, 9.7128e-01, 9.7882e-01,  ..., 8.2192e-01,\n",
       "           7.2445e-01, 5.3211e-02],\n",
       "          [9.7206e-01, 9.6933e-01, 9.6999e-01,  ..., 8.2466e-01,\n",
       "           7.3211e-01, 5.7138e-02]],\n",
       " \n",
       "         [[9.4862e-01, 9.4928e-01, 9.5318e-01,  ..., 7.0959e-01,\n",
       "           3.8126e-01, 1.4302e-02],\n",
       "          [9.5731e-01, 9.5660e-01, 9.5759e-01,  ..., 7.1233e-01,\n",
       "           3.8963e-01, 1.2332e-02],\n",
       "          [9.6074e-01, 9.5369e-01, 9.5719e-01,  ..., 7.1507e-01,\n",
       "           3.9804e-01, 1.0505e-02],\n",
       "          ...,\n",
       "          [9.7772e-01, 9.7128e-01, 9.7882e-01,  ..., 8.2192e-01,\n",
       "           7.2445e-01, 5.3211e-02],\n",
       "          [9.7206e-01, 9.6933e-01, 9.6999e-01,  ..., 8.2466e-01,\n",
       "           7.3211e-01, 5.7138e-02],\n",
       "          [9.8040e-01, 9.7514e-01, 9.8055e-01,  ..., 8.2740e-01,\n",
       "           7.3969e-01, 6.1196e-02]],\n",
       " \n",
       "         [[9.5731e-01, 9.5660e-01, 9.5759e-01,  ..., 7.1233e-01,\n",
       "           3.8963e-01, 1.2332e-02],\n",
       "          [9.6074e-01, 9.5369e-01, 9.5719e-01,  ..., 7.1507e-01,\n",
       "           3.9804e-01, 1.0505e-02],\n",
       "          [9.5892e-01, 9.6136e-01, 9.6384e-01,  ..., 7.2329e-01,\n",
       "           4.2343e-01, 5.8976e-03],\n",
       "          ...,\n",
       "          [9.7206e-01, 9.6933e-01, 9.6999e-01,  ..., 8.2466e-01,\n",
       "           7.3211e-01, 5.7138e-02],\n",
       "          [9.8040e-01, 9.7514e-01, 9.8055e-01,  ..., 8.2740e-01,\n",
       "           7.3969e-01, 6.1196e-02],\n",
       "          [9.7918e-01, 9.7384e-01, 9.7324e-01,  ..., 8.3014e-01,\n",
       "           7.4720e-01, 6.5384e-02]]], device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.CreateTensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_x</th>\n",
       "      <th>High_x</th>\n",
       "      <th>Low_x</th>\n",
       "      <th>Close_x</th>\n",
       "      <th>Volume</th>\n",
       "      <th>EMA20_Close_x</th>\n",
       "      <th>EMA40_Close_x</th>\n",
       "      <th>EMA60_Close_x</th>\n",
       "      <th>RSI20_Close_x</th>\n",
       "      <th>RSI40_Close_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Close_y</th>\n",
       "      <th>EMA20_Close_y</th>\n",
       "      <th>EMA40_Close_y</th>\n",
       "      <th>EMA60_Close_y</th>\n",
       "      <th>RSI20_Close_y</th>\n",
       "      <th>RSI40_Close_y</th>\n",
       "      <th>RSI60_Close_y</th>\n",
       "      <th>shifted_day_of_year</th>\n",
       "      <th>cos_shifted_annual</th>\n",
       "      <th>sin_shifted_annual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-07-12</th>\n",
       "      <td>189.174968</td>\n",
       "      <td>191.189594</td>\n",
       "      <td>187.968198</td>\n",
       "      <td>189.264740</td>\n",
       "      <td>60750200</td>\n",
       "      <td>186.967469</td>\n",
       "      <td>182.281242</td>\n",
       "      <td>177.955451</td>\n",
       "      <td>62.503099</td>\n",
       "      <td>62.598253</td>\n",
       "      <td>...</td>\n",
       "      <td>13.540000</td>\n",
       "      <td>14.388288</td>\n",
       "      <td>15.115892</td>\n",
       "      <td>15.890125</td>\n",
       "      <td>43.573185</td>\n",
       "      <td>44.658958</td>\n",
       "      <td>45.457967</td>\n",
       "      <td>134</td>\n",
       "      <td>-0.670089</td>\n",
       "      <td>0.742281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-13</th>\n",
       "      <td>189.992798</td>\n",
       "      <td>190.680963</td>\n",
       "      <td>189.274713</td>\n",
       "      <td>190.032684</td>\n",
       "      <td>41342300</td>\n",
       "      <td>187.259395</td>\n",
       "      <td>182.659361</td>\n",
       "      <td>178.351425</td>\n",
       "      <td>63.539759</td>\n",
       "      <td>63.079535</td>\n",
       "      <td>...</td>\n",
       "      <td>13.610000</td>\n",
       "      <td>14.314166</td>\n",
       "      <td>15.042434</td>\n",
       "      <td>15.815366</td>\n",
       "      <td>43.901797</td>\n",
       "      <td>44.795659</td>\n",
       "      <td>45.539996</td>\n",
       "      <td>135</td>\n",
       "      <td>-0.682758</td>\n",
       "      <td>0.730644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-14</th>\n",
       "      <td>189.723515</td>\n",
       "      <td>190.670982</td>\n",
       "      <td>189.125121</td>\n",
       "      <td>190.182297</td>\n",
       "      <td>41573900</td>\n",
       "      <td>187.537766</td>\n",
       "      <td>183.026333</td>\n",
       "      <td>178.739323</td>\n",
       "      <td>63.745310</td>\n",
       "      <td>63.174223</td>\n",
       "      <td>...</td>\n",
       "      <td>13.340000</td>\n",
       "      <td>14.221388</td>\n",
       "      <td>14.959388</td>\n",
       "      <td>15.734207</td>\n",
       "      <td>42.887713</td>\n",
       "      <td>44.362148</td>\n",
       "      <td>45.272916</td>\n",
       "      <td>136</td>\n",
       "      <td>-0.695225</td>\n",
       "      <td>0.718792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-17</th>\n",
       "      <td>191.389049</td>\n",
       "      <td>193.802619</td>\n",
       "      <td>191.299292</td>\n",
       "      <td>193.473495</td>\n",
       "      <td>50520200</td>\n",
       "      <td>188.103074</td>\n",
       "      <td>183.535951</td>\n",
       "      <td>179.222411</td>\n",
       "      <td>67.931691</td>\n",
       "      <td>65.188552</td>\n",
       "      <td>...</td>\n",
       "      <td>13.480000</td>\n",
       "      <td>14.150779</td>\n",
       "      <td>14.887223</td>\n",
       "      <td>15.660298</td>\n",
       "      <td>43.598794</td>\n",
       "      <td>44.647029</td>\n",
       "      <td>45.441638</td>\n",
       "      <td>139</td>\n",
       "      <td>-0.731378</td>\n",
       "      <td>0.681972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-18</th>\n",
       "      <td>192.835210</td>\n",
       "      <td>193.812596</td>\n",
       "      <td>191.907678</td>\n",
       "      <td>193.214188</td>\n",
       "      <td>48353800</td>\n",
       "      <td>188.589847</td>\n",
       "      <td>184.008060</td>\n",
       "      <td>179.681157</td>\n",
       "      <td>67.287306</td>\n",
       "      <td>64.901678</td>\n",
       "      <td>...</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>14.069753</td>\n",
       "      <td>14.809797</td>\n",
       "      <td>15.582912</td>\n",
       "      <td>42.876310</td>\n",
       "      <td>44.347594</td>\n",
       "      <td>45.259198</td>\n",
       "      <td>140</td>\n",
       "      <td>-0.743001</td>\n",
       "      <td>0.669290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-19</th>\n",
       "      <td>192.585876</td>\n",
       "      <td>197.702207</td>\n",
       "      <td>192.137062</td>\n",
       "      <td>194.580551</td>\n",
       "      <td>80507300</td>\n",
       "      <td>189.160390</td>\n",
       "      <td>184.523791</td>\n",
       "      <td>180.169662</td>\n",
       "      <td>68.922418</td>\n",
       "      <td>65.717028</td>\n",
       "      <td>...</td>\n",
       "      <td>13.760000</td>\n",
       "      <td>14.040253</td>\n",
       "      <td>14.758588</td>\n",
       "      <td>15.523144</td>\n",
       "      <td>45.314081</td>\n",
       "      <td>45.309005</td>\n",
       "      <td>45.824467</td>\n",
       "      <td>141</td>\n",
       "      <td>-0.754404</td>\n",
       "      <td>0.656411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-20</th>\n",
       "      <td>194.570572</td>\n",
       "      <td>195.946903</td>\n",
       "      <td>191.987471</td>\n",
       "      <td>192.615799</td>\n",
       "      <td>59581200</td>\n",
       "      <td>189.489476</td>\n",
       "      <td>184.918523</td>\n",
       "      <td>180.577732</td>\n",
       "      <td>64.074717</td>\n",
       "      <td>63.540106</td>\n",
       "      <td>...</td>\n",
       "      <td>13.990000</td>\n",
       "      <td>14.035467</td>\n",
       "      <td>14.721096</td>\n",
       "      <td>15.472877</td>\n",
       "      <td>46.515380</td>\n",
       "      <td>45.789263</td>\n",
       "      <td>46.107438</td>\n",
       "      <td>142</td>\n",
       "      <td>-0.765584</td>\n",
       "      <td>0.643337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-21</th>\n",
       "      <td>193.583223</td>\n",
       "      <td>194.450902</td>\n",
       "      <td>190.720854</td>\n",
       "      <td>191.428970</td>\n",
       "      <td>71917800</td>\n",
       "      <td>189.674190</td>\n",
       "      <td>185.236106</td>\n",
       "      <td>180.933510</td>\n",
       "      <td>61.331770</td>\n",
       "      <td>62.262298</td>\n",
       "      <td>...</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>13.993994</td>\n",
       "      <td>14.666408</td>\n",
       "      <td>15.411471</td>\n",
       "      <td>44.760358</td>\n",
       "      <td>45.100495</td>\n",
       "      <td>45.695861</td>\n",
       "      <td>143</td>\n",
       "      <td>-0.776537</td>\n",
       "      <td>0.630072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-24</th>\n",
       "      <td>192.895048</td>\n",
       "      <td>194.391054</td>\n",
       "      <td>191.738132</td>\n",
       "      <td>192.236801</td>\n",
       "      <td>45377800</td>\n",
       "      <td>189.918248</td>\n",
       "      <td>185.577604</td>\n",
       "      <td>181.304110</td>\n",
       "      <td>62.482500</td>\n",
       "      <td>62.784774</td>\n",
       "      <td>...</td>\n",
       "      <td>13.910000</td>\n",
       "      <td>13.985994</td>\n",
       "      <td>14.629510</td>\n",
       "      <td>15.362243</td>\n",
       "      <td>46.450845</td>\n",
       "      <td>45.765579</td>\n",
       "      <td>46.084894</td>\n",
       "      <td>146</td>\n",
       "      <td>-0.808005</td>\n",
       "      <td>0.589176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-25</th>\n",
       "      <td>192.815256</td>\n",
       "      <td>193.922301</td>\n",
       "      <td>192.406344</td>\n",
       "      <td>193.104477</td>\n",
       "      <td>37283200</td>\n",
       "      <td>190.221699</td>\n",
       "      <td>185.944768</td>\n",
       "      <td>181.691007</td>\n",
       "      <td>63.703722</td>\n",
       "      <td>63.343849</td>\n",
       "      <td>...</td>\n",
       "      <td>13.860000</td>\n",
       "      <td>13.973995</td>\n",
       "      <td>14.591973</td>\n",
       "      <td>15.312989</td>\n",
       "      <td>46.210746</td>\n",
       "      <td>45.674045</td>\n",
       "      <td>46.030805</td>\n",
       "      <td>147</td>\n",
       "      <td>-0.818020</td>\n",
       "      <td>0.575190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-26</th>\n",
       "      <td>193.154355</td>\n",
       "      <td>195.119111</td>\n",
       "      <td>192.805296</td>\n",
       "      <td>193.982147</td>\n",
       "      <td>47471900</td>\n",
       "      <td>190.579837</td>\n",
       "      <td>186.336835</td>\n",
       "      <td>182.093996</td>\n",
       "      <td>64.919561</td>\n",
       "      <td>63.906383</td>\n",
       "      <td>...</td>\n",
       "      <td>13.190000</td>\n",
       "      <td>13.899329</td>\n",
       "      <td>14.523584</td>\n",
       "      <td>15.243383</td>\n",
       "      <td>43.070545</td>\n",
       "      <td>44.452144</td>\n",
       "      <td>45.306179</td>\n",
       "      <td>148</td>\n",
       "      <td>-0.827793</td>\n",
       "      <td>0.561034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-27</th>\n",
       "      <td>195.498099</td>\n",
       "      <td>196.674950</td>\n",
       "      <td>192.037337</td>\n",
       "      <td>192.705551</td>\n",
       "      <td>47460200</td>\n",
       "      <td>190.782286</td>\n",
       "      <td>186.647504</td>\n",
       "      <td>182.441915</td>\n",
       "      <td>61.752426</td>\n",
       "      <td>62.476063</td>\n",
       "      <td>...</td>\n",
       "      <td>14.410000</td>\n",
       "      <td>13.947964</td>\n",
       "      <td>14.518043</td>\n",
       "      <td>15.216059</td>\n",
       "      <td>49.631071</td>\n",
       "      <td>47.095406</td>\n",
       "      <td>46.855384</td>\n",
       "      <td>149</td>\n",
       "      <td>-0.837321</td>\n",
       "      <td>0.546711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-28</th>\n",
       "      <td>194.151694</td>\n",
       "      <td>196.106482</td>\n",
       "      <td>193.623106</td>\n",
       "      <td>195.308609</td>\n",
       "      <td>48291400</td>\n",
       "      <td>191.213364</td>\n",
       "      <td>187.069997</td>\n",
       "      <td>182.863774</td>\n",
       "      <td>65.377797</td>\n",
       "      <td>64.153926</td>\n",
       "      <td>...</td>\n",
       "      <td>13.330000</td>\n",
       "      <td>13.889110</td>\n",
       "      <td>14.460090</td>\n",
       "      <td>15.154221</td>\n",
       "      <td>44.818280</td>\n",
       "      <td>45.144928</td>\n",
       "      <td>45.690299</td>\n",
       "      <td>150</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>0.532227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31</th>\n",
       "      <td>195.537995</td>\n",
       "      <td>195.966858</td>\n",
       "      <td>194.740122</td>\n",
       "      <td>195.926956</td>\n",
       "      <td>38824100</td>\n",
       "      <td>191.662278</td>\n",
       "      <td>187.502044</td>\n",
       "      <td>183.292075</td>\n",
       "      <td>66.179392</td>\n",
       "      <td>64.540228</td>\n",
       "      <td>...</td>\n",
       "      <td>13.630000</td>\n",
       "      <td>13.864433</td>\n",
       "      <td>14.419598</td>\n",
       "      <td>15.104246</td>\n",
       "      <td>46.339773</td>\n",
       "      <td>45.784631</td>\n",
       "      <td>46.069119</td>\n",
       "      <td>153</td>\n",
       "      <td>-0.872929</td>\n",
       "      <td>0.487847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01</th>\n",
       "      <td>195.717515</td>\n",
       "      <td>196.206201</td>\n",
       "      <td>194.760064</td>\n",
       "      <td>195.089188</td>\n",
       "      <td>35175100</td>\n",
       "      <td>191.988650</td>\n",
       "      <td>187.872149</td>\n",
       "      <td>183.678866</td>\n",
       "      <td>64.064040</td>\n",
       "      <td>63.587984</td>\n",
       "      <td>...</td>\n",
       "      <td>13.930000</td>\n",
       "      <td>13.870678</td>\n",
       "      <td>14.395715</td>\n",
       "      <td>15.065746</td>\n",
       "      <td>47.853259</td>\n",
       "      <td>46.425421</td>\n",
       "      <td>46.448977</td>\n",
       "      <td>154</td>\n",
       "      <td>-0.881192</td>\n",
       "      <td>0.472759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-02</th>\n",
       "      <td>194.520703</td>\n",
       "      <td>194.660329</td>\n",
       "      <td>191.339209</td>\n",
       "      <td>192.067261</td>\n",
       "      <td>50389300</td>\n",
       "      <td>191.996137</td>\n",
       "      <td>188.076788</td>\n",
       "      <td>183.953895</td>\n",
       "      <td>57.130371</td>\n",
       "      <td>60.296690</td>\n",
       "      <td>...</td>\n",
       "      <td>16.090000</td>\n",
       "      <td>14.082042</td>\n",
       "      <td>14.478363</td>\n",
       "      <td>15.099328</td>\n",
       "      <td>57.037165</td>\n",
       "      <td>50.726105</td>\n",
       "      <td>49.075280</td>\n",
       "      <td>155</td>\n",
       "      <td>-0.889193</td>\n",
       "      <td>0.457531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-03</th>\n",
       "      <td>191.059955</td>\n",
       "      <td>191.857813</td>\n",
       "      <td>190.182293</td>\n",
       "      <td>190.661011</td>\n",
       "      <td>61235200</td>\n",
       "      <td>191.868982</td>\n",
       "      <td>188.202848</td>\n",
       "      <td>184.173801</td>\n",
       "      <td>54.254059</td>\n",
       "      <td>58.843037</td>\n",
       "      <td>...</td>\n",
       "      <td>15.920000</td>\n",
       "      <td>14.257085</td>\n",
       "      <td>14.548687</td>\n",
       "      <td>15.126236</td>\n",
       "      <td>56.216928</td>\n",
       "      <td>50.399520</td>\n",
       "      <td>48.883399</td>\n",
       "      <td>156</td>\n",
       "      <td>-0.896932</td>\n",
       "      <td>0.442168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-04</th>\n",
       "      <td>185.026063</td>\n",
       "      <td>186.881111</td>\n",
       "      <td>181.435642</td>\n",
       "      <td>181.505463</td>\n",
       "      <td>115799700</td>\n",
       "      <td>190.881980</td>\n",
       "      <td>187.876146</td>\n",
       "      <td>184.086314</td>\n",
       "      <td>40.336444</td>\n",
       "      <td>50.683734</td>\n",
       "      <td>...</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>14.527839</td>\n",
       "      <td>14.673141</td>\n",
       "      <td>15.190949</td>\n",
       "      <td>60.379924</td>\n",
       "      <td>52.573301</td>\n",
       "      <td>50.256304</td>\n",
       "      <td>157</td>\n",
       "      <td>-0.904405</td>\n",
       "      <td>0.426674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-07</th>\n",
       "      <td>181.645075</td>\n",
       "      <td>182.642413</td>\n",
       "      <td>176.877804</td>\n",
       "      <td>178.373810</td>\n",
       "      <td>97576100</td>\n",
       "      <td>189.690726</td>\n",
       "      <td>187.412618</td>\n",
       "      <td>183.899019</td>\n",
       "      <td>36.925860</td>\n",
       "      <td>48.332572</td>\n",
       "      <td>...</td>\n",
       "      <td>15.770000</td>\n",
       "      <td>14.646140</td>\n",
       "      <td>14.726647</td>\n",
       "      <td>15.209934</td>\n",
       "      <td>54.258995</td>\n",
       "      <td>50.038194</td>\n",
       "      <td>48.755342</td>\n",
       "      <td>160</td>\n",
       "      <td>-0.925211</td>\n",
       "      <td>0.379453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-08</th>\n",
       "      <td>179.211566</td>\n",
       "      <td>179.790024</td>\n",
       "      <td>177.107183</td>\n",
       "      <td>179.321274</td>\n",
       "      <td>67823000</td>\n",
       "      <td>188.703159</td>\n",
       "      <td>187.017918</td>\n",
       "      <td>183.748929</td>\n",
       "      <td>38.579756</td>\n",
       "      <td>49.065748</td>\n",
       "      <td>...</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>14.774127</td>\n",
       "      <td>14.788274</td>\n",
       "      <td>15.235510</td>\n",
       "      <td>55.052368</td>\n",
       "      <td>50.443605</td>\n",
       "      <td>49.011508</td>\n",
       "      <td>161</td>\n",
       "      <td>-0.931601</td>\n",
       "      <td>0.363482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-09</th>\n",
       "      <td>180.388433</td>\n",
       "      <td>180.448271</td>\n",
       "      <td>176.538710</td>\n",
       "      <td>177.715576</td>\n",
       "      <td>60378500</td>\n",
       "      <td>187.656722</td>\n",
       "      <td>186.564145</td>\n",
       "      <td>183.551114</td>\n",
       "      <td>36.855747</td>\n",
       "      <td>47.884655</td>\n",
       "      <td>...</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>14.887067</td>\n",
       "      <td>14.845431</td>\n",
       "      <td>15.259264</td>\n",
       "      <td>54.915646</td>\n",
       "      <td>50.386422</td>\n",
       "      <td>48.977556</td>\n",
       "      <td>162</td>\n",
       "      <td>-0.937716</td>\n",
       "      <td>0.347403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-10</th>\n",
       "      <td>179.002129</td>\n",
       "      <td>180.268752</td>\n",
       "      <td>177.127145</td>\n",
       "      <td>177.496155</td>\n",
       "      <td>54686900</td>\n",
       "      <td>186.689049</td>\n",
       "      <td>186.121804</td>\n",
       "      <td>183.352591</td>\n",
       "      <td>36.620353</td>\n",
       "      <td>47.723646</td>\n",
       "      <td>...</td>\n",
       "      <td>15.850000</td>\n",
       "      <td>14.978775</td>\n",
       "      <td>14.894434</td>\n",
       "      <td>15.278633</td>\n",
       "      <td>54.394250</td>\n",
       "      <td>50.172532</td>\n",
       "      <td>48.851368</td>\n",
       "      <td>163</td>\n",
       "      <td>-0.943553</td>\n",
       "      <td>0.331221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-11</th>\n",
       "      <td>177.086708</td>\n",
       "      <td>178.384986</td>\n",
       "      <td>176.317717</td>\n",
       "      <td>177.556076</td>\n",
       "      <td>51988100</td>\n",
       "      <td>185.819242</td>\n",
       "      <td>185.703964</td>\n",
       "      <td>183.162541</td>\n",
       "      <td>36.736504</td>\n",
       "      <td>47.772832</td>\n",
       "      <td>...</td>\n",
       "      <td>14.840000</td>\n",
       "      <td>14.965558</td>\n",
       "      <td>14.891779</td>\n",
       "      <td>15.264251</td>\n",
       "      <td>49.822294</td>\n",
       "      <td>48.243912</td>\n",
       "      <td>47.703742</td>\n",
       "      <td>164</td>\n",
       "      <td>-0.949111</td>\n",
       "      <td>0.314942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-14</th>\n",
       "      <td>177.735847</td>\n",
       "      <td>179.453585</td>\n",
       "      <td>177.076712</td>\n",
       "      <td>179.223892</td>\n",
       "      <td>43675600</td>\n",
       "      <td>185.191114</td>\n",
       "      <td>185.387863</td>\n",
       "      <td>183.033405</td>\n",
       "      <td>39.960197</td>\n",
       "      <td>49.138956</td>\n",
       "      <td>...</td>\n",
       "      <td>14.820000</td>\n",
       "      <td>14.951696</td>\n",
       "      <td>14.888278</td>\n",
       "      <td>15.249686</td>\n",
       "      <td>49.735156</td>\n",
       "      <td>48.206277</td>\n",
       "      <td>47.681185</td>\n",
       "      <td>167</td>\n",
       "      <td>-0.964094</td>\n",
       "      <td>0.265563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-15</th>\n",
       "      <td>178.644649</td>\n",
       "      <td>179.243850</td>\n",
       "      <td>176.817055</td>\n",
       "      <td>177.216522</td>\n",
       "      <td>43622600</td>\n",
       "      <td>184.431629</td>\n",
       "      <td>184.989261</td>\n",
       "      <td>182.842688</td>\n",
       "      <td>37.536858</td>\n",
       "      <td>47.601890</td>\n",
       "      <td>...</td>\n",
       "      <td>16.459999</td>\n",
       "      <td>15.095344</td>\n",
       "      <td>14.964947</td>\n",
       "      <td>15.289368</td>\n",
       "      <td>56.327922</td>\n",
       "      <td>51.395129</td>\n",
       "      <td>49.665914</td>\n",
       "      <td>168</td>\n",
       "      <td>-0.968519</td>\n",
       "      <td>0.248940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-16</th>\n",
       "      <td>176.896953</td>\n",
       "      <td>178.305086</td>\n",
       "      <td>176.267777</td>\n",
       "      <td>176.337692</td>\n",
       "      <td>46964900</td>\n",
       "      <td>183.660778</td>\n",
       "      <td>184.567233</td>\n",
       "      <td>182.629409</td>\n",
       "      <td>36.516325</td>\n",
       "      <td>46.942555</td>\n",
       "      <td>...</td>\n",
       "      <td>16.780001</td>\n",
       "      <td>15.255787</td>\n",
       "      <td>15.053486</td>\n",
       "      <td>15.338241</td>\n",
       "      <td>57.473559</td>\n",
       "      <td>51.986721</td>\n",
       "      <td>50.041973</td>\n",
       "      <td>169</td>\n",
       "      <td>-0.972658</td>\n",
       "      <td>0.232243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-17</th>\n",
       "      <td>176.906941</td>\n",
       "      <td>177.276449</td>\n",
       "      <td>173.251752</td>\n",
       "      <td>173.771072</td>\n",
       "      <td>66062900</td>\n",
       "      <td>182.718901</td>\n",
       "      <td>184.040591</td>\n",
       "      <td>182.338972</td>\n",
       "      <td>33.699703</td>\n",
       "      <td>45.072537</td>\n",
       "      <td>...</td>\n",
       "      <td>17.889999</td>\n",
       "      <td>15.506664</td>\n",
       "      <td>15.191853</td>\n",
       "      <td>15.421905</td>\n",
       "      <td>61.190832</td>\n",
       "      <td>53.979502</td>\n",
       "      <td>51.324810</td>\n",
       "      <td>170</td>\n",
       "      <td>-0.976509</td>\n",
       "      <td>0.215477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-18</th>\n",
       "      <td>172.073301</td>\n",
       "      <td>174.869620</td>\n",
       "      <td>171.733752</td>\n",
       "      <td>174.260422</td>\n",
       "      <td>61114200</td>\n",
       "      <td>181.913332</td>\n",
       "      <td>183.563510</td>\n",
       "      <td>182.074101</td>\n",
       "      <td>34.710396</td>\n",
       "      <td>45.497109</td>\n",
       "      <td>...</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>15.677458</td>\n",
       "      <td>15.294689</td>\n",
       "      <td>15.483482</td>\n",
       "      <td>58.337707</td>\n",
       "      <td>52.785143</td>\n",
       "      <td>50.622165</td>\n",
       "      <td>171</td>\n",
       "      <td>-0.980071</td>\n",
       "      <td>0.198648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-21</th>\n",
       "      <td>174.839667</td>\n",
       "      <td>175.898270</td>\n",
       "      <td>173.511415</td>\n",
       "      <td>175.608643</td>\n",
       "      <td>46311900</td>\n",
       "      <td>181.312885</td>\n",
       "      <td>183.175467</td>\n",
       "      <td>181.862119</td>\n",
       "      <td>37.474653</td>\n",
       "      <td>46.662134</td>\n",
       "      <td>...</td>\n",
       "      <td>17.129999</td>\n",
       "      <td>15.815796</td>\n",
       "      <td>15.384216</td>\n",
       "      <td>15.537466</td>\n",
       "      <td>57.524205</td>\n",
       "      <td>52.442233</td>\n",
       "      <td>50.419907</td>\n",
       "      <td>174</td>\n",
       "      <td>-0.989013</td>\n",
       "      <td>0.147827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-22</th>\n",
       "      <td>176.827036</td>\n",
       "      <td>177.446216</td>\n",
       "      <td>176.018105</td>\n",
       "      <td>176.996811</td>\n",
       "      <td>42084200</td>\n",
       "      <td>180.901830</td>\n",
       "      <td>182.874069</td>\n",
       "      <td>181.702601</td>\n",
       "      <td>40.217889</td>\n",
       "      <td>47.839559</td>\n",
       "      <td>...</td>\n",
       "      <td>16.969999</td>\n",
       "      <td>15.925720</td>\n",
       "      <td>15.461572</td>\n",
       "      <td>15.584435</td>\n",
       "      <td>56.740329</td>\n",
       "      <td>52.115420</td>\n",
       "      <td>50.227828</td>\n",
       "      <td>175</td>\n",
       "      <td>-0.991410</td>\n",
       "      <td>0.130793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-23</th>\n",
       "      <td>178.285129</td>\n",
       "      <td>181.311141</td>\n",
       "      <td>178.095376</td>\n",
       "      <td>180.881699</td>\n",
       "      <td>52722800</td>\n",
       "      <td>180.899913</td>\n",
       "      <td>182.776881</td>\n",
       "      <td>181.675686</td>\n",
       "      <td>47.060189</td>\n",
       "      <td>50.947620</td>\n",
       "      <td>...</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>15.930889</td>\n",
       "      <td>15.486861</td>\n",
       "      <td>15.597404</td>\n",
       "      <td>52.114910</td>\n",
       "      <td>50.132746</td>\n",
       "      <td>49.051985</td>\n",
       "      <td>176</td>\n",
       "      <td>-0.993513</td>\n",
       "      <td>0.113720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-24</th>\n",
       "      <td>180.432283</td>\n",
       "      <td>180.861725</td>\n",
       "      <td>175.778410</td>\n",
       "      <td>176.147934</td>\n",
       "      <td>54945800</td>\n",
       "      <td>180.447344</td>\n",
       "      <td>182.453517</td>\n",
       "      <td>181.494448</td>\n",
       "      <td>41.035977</td>\n",
       "      <td>47.416584</td>\n",
       "      <td>...</td>\n",
       "      <td>17.200001</td>\n",
       "      <td>16.051757</td>\n",
       "      <td>15.570429</td>\n",
       "      <td>15.649948</td>\n",
       "      <td>56.694283</td>\n",
       "      <td>52.420581</td>\n",
       "      <td>50.504092</td>\n",
       "      <td>177</td>\n",
       "      <td>-0.995322</td>\n",
       "      <td>0.096613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-25</th>\n",
       "      <td>177.146623</td>\n",
       "      <td>178.914283</td>\n",
       "      <td>175.588678</td>\n",
       "      <td>178.375000</td>\n",
       "      <td>51449600</td>\n",
       "      <td>180.249978</td>\n",
       "      <td>182.254565</td>\n",
       "      <td>181.392171</td>\n",
       "      <td>44.551114</td>\n",
       "      <td>49.118204</td>\n",
       "      <td>...</td>\n",
       "      <td>15.680000</td>\n",
       "      <td>16.016352</td>\n",
       "      <td>15.575774</td>\n",
       "      <td>15.650933</td>\n",
       "      <td>50.376127</td>\n",
       "      <td>49.517578</td>\n",
       "      <td>48.743844</td>\n",
       "      <td>178</td>\n",
       "      <td>-0.996837</td>\n",
       "      <td>0.079477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-28</th>\n",
       "      <td>179.853052</td>\n",
       "      <td>180.352394</td>\n",
       "      <td>178.315085</td>\n",
       "      <td>179.952927</td>\n",
       "      <td>43820700</td>\n",
       "      <td>180.221687</td>\n",
       "      <td>182.142290</td>\n",
       "      <td>181.344983</td>\n",
       "      <td>46.911514</td>\n",
       "      <td>50.287247</td>\n",
       "      <td>...</td>\n",
       "      <td>15.080000</td>\n",
       "      <td>15.927175</td>\n",
       "      <td>15.551589</td>\n",
       "      <td>15.632214</td>\n",
       "      <td>48.146658</td>\n",
       "      <td>48.431706</td>\n",
       "      <td>48.071270</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.999609</td>\n",
       "      <td>0.027950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-29</th>\n",
       "      <td>179.463564</td>\n",
       "      <td>184.656719</td>\n",
       "      <td>179.263830</td>\n",
       "      <td>183.877747</td>\n",
       "      <td>53003900</td>\n",
       "      <td>180.569883</td>\n",
       "      <td>182.226947</td>\n",
       "      <td>181.428024</td>\n",
       "      <td>52.235174</td>\n",
       "      <td>53.039744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.450000</td>\n",
       "      <td>15.786492</td>\n",
       "      <td>15.497853</td>\n",
       "      <td>15.593453</td>\n",
       "      <td>45.901391</td>\n",
       "      <td>47.314337</td>\n",
       "      <td>47.373292</td>\n",
       "      <td>182</td>\n",
       "      <td>-0.999942</td>\n",
       "      <td>0.010751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-30</th>\n",
       "      <td>184.696681</td>\n",
       "      <td>187.602856</td>\n",
       "      <td>184.496947</td>\n",
       "      <td>187.403107</td>\n",
       "      <td>60813900</td>\n",
       "      <td>181.220666</td>\n",
       "      <td>182.479442</td>\n",
       "      <td>181.623929</td>\n",
       "      <td>56.371722</td>\n",
       "      <td>55.318840</td>\n",
       "      <td>...</td>\n",
       "      <td>13.880000</td>\n",
       "      <td>15.604921</td>\n",
       "      <td>15.418934</td>\n",
       "      <td>15.537274</td>\n",
       "      <td>43.949455</td>\n",
       "      <td>46.322617</td>\n",
       "      <td>46.748753</td>\n",
       "      <td>183</td>\n",
       "      <td>-0.999979</td>\n",
       "      <td>-0.006451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-31</th>\n",
       "      <td>187.592859</td>\n",
       "      <td>188.871173</td>\n",
       "      <td>187.233332</td>\n",
       "      <td>187.622818</td>\n",
       "      <td>60794500</td>\n",
       "      <td>181.830395</td>\n",
       "      <td>182.730339</td>\n",
       "      <td>181.820614</td>\n",
       "      <td>56.618192</td>\n",
       "      <td>55.457024</td>\n",
       "      <td>...</td>\n",
       "      <td>13.570000</td>\n",
       "      <td>15.411119</td>\n",
       "      <td>15.328742</td>\n",
       "      <td>15.472774</td>\n",
       "      <td>42.904948</td>\n",
       "      <td>45.787283</td>\n",
       "      <td>46.410355</td>\n",
       "      <td>184</td>\n",
       "      <td>-0.999720</td>\n",
       "      <td>-0.023651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-01</th>\n",
       "      <td>189.240698</td>\n",
       "      <td>189.670125</td>\n",
       "      <td>188.032284</td>\n",
       "      <td>189.210739</td>\n",
       "      <td>45732600</td>\n",
       "      <td>182.533285</td>\n",
       "      <td>183.046456</td>\n",
       "      <td>182.062913</td>\n",
       "      <td>58.405832</td>\n",
       "      <td>56.455273</td>\n",
       "      <td>...</td>\n",
       "      <td>13.090000</td>\n",
       "      <td>15.190060</td>\n",
       "      <td>15.219535</td>\n",
       "      <td>15.394650</td>\n",
       "      <td>41.304966</td>\n",
       "      <td>44.962097</td>\n",
       "      <td>45.887323</td>\n",
       "      <td>185</td>\n",
       "      <td>-0.999166</td>\n",
       "      <td>-0.040844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-05</th>\n",
       "      <td>188.032279</td>\n",
       "      <td>189.730039</td>\n",
       "      <td>187.363162</td>\n",
       "      <td>189.450409</td>\n",
       "      <td>45280000</td>\n",
       "      <td>183.192059</td>\n",
       "      <td>183.358844</td>\n",
       "      <td>182.305126</td>\n",
       "      <td>58.676372</td>\n",
       "      <td>56.605819</td>\n",
       "      <td>...</td>\n",
       "      <td>14.010000</td>\n",
       "      <td>15.077674</td>\n",
       "      <td>15.160533</td>\n",
       "      <td>15.349252</td>\n",
       "      <td>45.412000</td>\n",
       "      <td>46.845275</td>\n",
       "      <td>47.050436</td>\n",
       "      <td>189</td>\n",
       "      <td>-0.993993</td>\n",
       "      <td>-0.109446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-06</th>\n",
       "      <td>188.152109</td>\n",
       "      <td>188.601529</td>\n",
       "      <td>181.231234</td>\n",
       "      <td>182.669342</td>\n",
       "      <td>81755800</td>\n",
       "      <td>183.142276</td>\n",
       "      <td>183.325209</td>\n",
       "      <td>182.317067</td>\n",
       "      <td>49.154475</td>\n",
       "      <td>51.444569</td>\n",
       "      <td>...</td>\n",
       "      <td>14.450000</td>\n",
       "      <td>15.017895</td>\n",
       "      <td>15.125873</td>\n",
       "      <td>15.319768</td>\n",
       "      <td>47.269503</td>\n",
       "      <td>47.722684</td>\n",
       "      <td>47.598249</td>\n",
       "      <td>190</td>\n",
       "      <td>-0.991963</td>\n",
       "      <td>-0.126528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-07</th>\n",
       "      <td>174.949512</td>\n",
       "      <td>177.975539</td>\n",
       "      <td>173.311670</td>\n",
       "      <td>177.326385</td>\n",
       "      <td>112488800</td>\n",
       "      <td>182.588382</td>\n",
       "      <td>183.032584</td>\n",
       "      <td>182.153438</td>\n",
       "      <td>43.323469</td>\n",
       "      <td>47.914072</td>\n",
       "      <td>...</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>14.959048</td>\n",
       "      <td>15.090465</td>\n",
       "      <td>15.289612</td>\n",
       "      <td>47.077881</td>\n",
       "      <td>47.631048</td>\n",
       "      <td>47.541409</td>\n",
       "      <td>191</td>\n",
       "      <td>-0.989640</td>\n",
       "      <td>-0.143572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-08</th>\n",
       "      <td>178.115346</td>\n",
       "      <td>180.002859</td>\n",
       "      <td>177.556070</td>\n",
       "      <td>177.945557</td>\n",
       "      <td>65551300</td>\n",
       "      <td>182.146208</td>\n",
       "      <td>182.784436</td>\n",
       "      <td>182.015475</td>\n",
       "      <td>44.131913</td>\n",
       "      <td>48.335490</td>\n",
       "      <td>...</td>\n",
       "      <td>13.840000</td>\n",
       "      <td>14.852472</td>\n",
       "      <td>15.029466</td>\n",
       "      <td>15.242083</td>\n",
       "      <td>44.930554</td>\n",
       "      <td>46.603111</td>\n",
       "      <td>46.903454</td>\n",
       "      <td>192</td>\n",
       "      <td>-0.987024</td>\n",
       "      <td>-0.160575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-11</th>\n",
       "      <td>179.833081</td>\n",
       "      <td>180.062774</td>\n",
       "      <td>177.106662</td>\n",
       "      <td>179.124008</td>\n",
       "      <td>58953100</td>\n",
       "      <td>181.858379</td>\n",
       "      <td>182.605879</td>\n",
       "      <td>181.920673</td>\n",
       "      <td>45.684123</td>\n",
       "      <td>49.138786</td>\n",
       "      <td>...</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>14.752237</td>\n",
       "      <td>14.969492</td>\n",
       "      <td>15.194802</td>\n",
       "      <td>44.776992</td>\n",
       "      <td>46.529545</td>\n",
       "      <td>46.857780</td>\n",
       "      <td>195</td>\n",
       "      <td>-0.977426</td>\n",
       "      <td>-0.211276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-12</th>\n",
       "      <td>179.253844</td>\n",
       "      <td>179.893002</td>\n",
       "      <td>174.589990</td>\n",
       "      <td>176.068039</td>\n",
       "      <td>90370200</td>\n",
       "      <td>181.306918</td>\n",
       "      <td>182.286960</td>\n",
       "      <td>181.728783</td>\n",
       "      <td>42.463663</td>\n",
       "      <td>47.187405</td>\n",
       "      <td>...</td>\n",
       "      <td>14.230000</td>\n",
       "      <td>14.702500</td>\n",
       "      <td>14.933420</td>\n",
       "      <td>15.163169</td>\n",
       "      <td>46.833204</td>\n",
       "      <td>47.444252</td>\n",
       "      <td>47.417556</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.973648</td>\n",
       "      <td>-0.228058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-13</th>\n",
       "      <td>176.277751</td>\n",
       "      <td>177.066720</td>\n",
       "      <td>173.751081</td>\n",
       "      <td>173.980789</td>\n",
       "      <td>84267900</td>\n",
       "      <td>180.609192</td>\n",
       "      <td>181.881781</td>\n",
       "      <td>181.474751</td>\n",
       "      <td>40.415330</td>\n",
       "      <td>45.910236</td>\n",
       "      <td>...</td>\n",
       "      <td>13.480000</td>\n",
       "      <td>14.586071</td>\n",
       "      <td>14.862521</td>\n",
       "      <td>15.107983</td>\n",
       "      <td>43.836437</td>\n",
       "      <td>46.035453</td>\n",
       "      <td>46.547861</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.969581</td>\n",
       "      <td>-0.244772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-14</th>\n",
       "      <td>173.771073</td>\n",
       "      <td>175.868316</td>\n",
       "      <td>173.351627</td>\n",
       "      <td>175.508789</td>\n",
       "      <td>60895800</td>\n",
       "      <td>180.123439</td>\n",
       "      <td>181.570903</td>\n",
       "      <td>181.279145</td>\n",
       "      <td>42.550795</td>\n",
       "      <td>46.987555</td>\n",
       "      <td>...</td>\n",
       "      <td>12.820000</td>\n",
       "      <td>14.417874</td>\n",
       "      <td>14.762886</td>\n",
       "      <td>15.032967</td>\n",
       "      <td>41.383506</td>\n",
       "      <td>44.833880</td>\n",
       "      <td>45.796171</td>\n",
       "      <td>198</td>\n",
       "      <td>-0.965227</td>\n",
       "      <td>-0.261414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-15</th>\n",
       "      <td>176.247791</td>\n",
       "      <td>176.267769</td>\n",
       "      <td>173.591303</td>\n",
       "      <td>174.779724</td>\n",
       "      <td>109205100</td>\n",
       "      <td>179.614514</td>\n",
       "      <td>181.239626</td>\n",
       "      <td>181.066049</td>\n",
       "      <td>41.798415</td>\n",
       "      <td>46.533992</td>\n",
       "      <td>...</td>\n",
       "      <td>13.790000</td>\n",
       "      <td>14.358076</td>\n",
       "      <td>14.715428</td>\n",
       "      <td>14.992214</td>\n",
       "      <td>46.053514</td>\n",
       "      <td>46.922185</td>\n",
       "      <td>47.073605</td>\n",
       "      <td>199</td>\n",
       "      <td>-0.960587</td>\n",
       "      <td>-0.277979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-18</th>\n",
       "      <td>176.247795</td>\n",
       "      <td>179.143988</td>\n",
       "      <td>175.938205</td>\n",
       "      <td>177.735840</td>\n",
       "      <td>67257600</td>\n",
       "      <td>179.435593</td>\n",
       "      <td>181.068710</td>\n",
       "      <td>180.956862</td>\n",
       "      <td>45.882534</td>\n",
       "      <td>48.597425</td>\n",
       "      <td>...</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.323974</td>\n",
       "      <td>14.680529</td>\n",
       "      <td>14.959683</td>\n",
       "      <td>47.015504</td>\n",
       "      <td>47.364613</td>\n",
       "      <td>47.346805</td>\n",
       "      <td>202</td>\n",
       "      <td>-0.944969</td>\n",
       "      <td>-0.327160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-19</th>\n",
       "      <td>177.286433</td>\n",
       "      <td>179.393657</td>\n",
       "      <td>176.896946</td>\n",
       "      <td>178.834396</td>\n",
       "      <td>51826900</td>\n",
       "      <td>179.378336</td>\n",
       "      <td>180.959719</td>\n",
       "      <td>180.887273</td>\n",
       "      <td>47.328366</td>\n",
       "      <td>49.342589</td>\n",
       "      <td>...</td>\n",
       "      <td>14.110000</td>\n",
       "      <td>14.303595</td>\n",
       "      <td>14.652698</td>\n",
       "      <td>14.931824</td>\n",
       "      <td>47.531392</td>\n",
       "      <td>47.599270</td>\n",
       "      <td>47.491186</td>\n",
       "      <td>203</td>\n",
       "      <td>-0.939201</td>\n",
       "      <td>-0.343367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-20</th>\n",
       "      <td>179.024138</td>\n",
       "      <td>179.463562</td>\n",
       "      <td>175.169216</td>\n",
       "      <td>175.259109</td>\n",
       "      <td>58436200</td>\n",
       "      <td>178.986029</td>\n",
       "      <td>180.681640</td>\n",
       "      <td>180.702743</td>\n",
       "      <td>43.359810</td>\n",
       "      <td>47.065132</td>\n",
       "      <td>...</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>14.383253</td>\n",
       "      <td>14.676469</td>\n",
       "      <td>14.938650</td>\n",
       "      <td>52.125809</td>\n",
       "      <td>49.750701</td>\n",
       "      <td>48.827380</td>\n",
       "      <td>204</td>\n",
       "      <td>-0.933156</td>\n",
       "      <td>-0.359472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-21</th>\n",
       "      <td>174.320351</td>\n",
       "      <td>176.068049</td>\n",
       "      <td>173.631257</td>\n",
       "      <td>173.701157</td>\n",
       "      <td>63047900</td>\n",
       "      <td>178.482707</td>\n",
       "      <td>180.341129</td>\n",
       "      <td>180.473183</td>\n",
       "      <td>41.753876</td>\n",
       "      <td>46.113873</td>\n",
       "      <td>...</td>\n",
       "      <td>17.540001</td>\n",
       "      <td>14.683896</td>\n",
       "      <td>14.816154</td>\n",
       "      <td>15.023940</td>\n",
       "      <td>60.590034</td>\n",
       "      <td>54.240630</td>\n",
       "      <td>51.737559</td>\n",
       "      <td>205</td>\n",
       "      <td>-0.926834</td>\n",
       "      <td>-0.375470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-22</th>\n",
       "      <td>174.440176</td>\n",
       "      <td>176.847008</td>\n",
       "      <td>173.820996</td>\n",
       "      <td>174.560013</td>\n",
       "      <td>56725400</td>\n",
       "      <td>178.109117</td>\n",
       "      <td>180.059123</td>\n",
       "      <td>180.279309</td>\n",
       "      <td>42.979380</td>\n",
       "      <td>46.722713</td>\n",
       "      <td>...</td>\n",
       "      <td>17.200001</td>\n",
       "      <td>14.923525</td>\n",
       "      <td>14.932439</td>\n",
       "      <td>15.095286</td>\n",
       "      <td>59.033606</td>\n",
       "      <td>53.545453</td>\n",
       "      <td>51.317112</td>\n",
       "      <td>206</td>\n",
       "      <td>-0.920239</td>\n",
       "      <td>-0.391358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-25</th>\n",
       "      <td>173.970796</td>\n",
       "      <td>176.737156</td>\n",
       "      <td>173.920859</td>\n",
       "      <td>175.848328</td>\n",
       "      <td>46172700</td>\n",
       "      <td>177.893804</td>\n",
       "      <td>179.853719</td>\n",
       "      <td>180.134031</td>\n",
       "      <td>44.812808</td>\n",
       "      <td>47.633005</td>\n",
       "      <td>...</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>15.111760</td>\n",
       "      <td>15.028417</td>\n",
       "      <td>15.154457</td>\n",
       "      <td>57.657957</td>\n",
       "      <td>52.931516</td>\n",
       "      <td>50.945615</td>\n",
       "      <td>209</td>\n",
       "      <td>-0.898825</td>\n",
       "      <td>-0.438307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-26</th>\n",
       "      <td>174.589987</td>\n",
       "      <td>174.969477</td>\n",
       "      <td>171.434141</td>\n",
       "      <td>171.733749</td>\n",
       "      <td>64588900</td>\n",
       "      <td>177.307132</td>\n",
       "      <td>179.457622</td>\n",
       "      <td>179.858612</td>\n",
       "      <td>40.441250</td>\n",
       "      <td>45.108393</td>\n",
       "      <td>...</td>\n",
       "      <td>18.940001</td>\n",
       "      <td>15.476355</td>\n",
       "      <td>15.219226</td>\n",
       "      <td>15.278573</td>\n",
       "      <td>63.710919</td>\n",
       "      <td>56.416690</td>\n",
       "      <td>53.284252</td>\n",
       "      <td>210</td>\n",
       "      <td>-0.891153</td>\n",
       "      <td>-0.453703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-27</th>\n",
       "      <td>172.392870</td>\n",
       "      <td>172.812316</td>\n",
       "      <td>168.827576</td>\n",
       "      <td>170.205750</td>\n",
       "      <td>66921800</td>\n",
       "      <td>176.630810</td>\n",
       "      <td>179.006312</td>\n",
       "      <td>179.542124</td>\n",
       "      <td>38.955725</td>\n",
       "      <td>44.215792</td>\n",
       "      <td>...</td>\n",
       "      <td>18.219999</td>\n",
       "      <td>15.737654</td>\n",
       "      <td>15.365606</td>\n",
       "      <td>15.375013</td>\n",
       "      <td>60.497883</td>\n",
       "      <td>54.943995</td>\n",
       "      <td>52.387820</td>\n",
       "      <td>211</td>\n",
       "      <td>-0.883217</td>\n",
       "      <td>-0.468965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-28</th>\n",
       "      <td>169.117194</td>\n",
       "      <td>171.803657</td>\n",
       "      <td>167.399456</td>\n",
       "      <td>170.465424</td>\n",
       "      <td>56294400</td>\n",
       "      <td>176.043630</td>\n",
       "      <td>178.589683</td>\n",
       "      <td>179.244527</td>\n",
       "      <td>39.354233</td>\n",
       "      <td>44.407534</td>\n",
       "      <td>...</td>\n",
       "      <td>17.340000</td>\n",
       "      <td>15.890259</td>\n",
       "      <td>15.461917</td>\n",
       "      <td>15.439439</td>\n",
       "      <td>56.811796</td>\n",
       "      <td>53.203046</td>\n",
       "      <td>51.314796</td>\n",
       "      <td>212</td>\n",
       "      <td>-0.875019</td>\n",
       "      <td>-0.484089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-29</th>\n",
       "      <td>171.793673</td>\n",
       "      <td>172.842295</td>\n",
       "      <td>170.115876</td>\n",
       "      <td>170.984741</td>\n",
       "      <td>51814200</td>\n",
       "      <td>175.561831</td>\n",
       "      <td>178.218710</td>\n",
       "      <td>178.973715</td>\n",
       "      <td>40.176374</td>\n",
       "      <td>44.796731</td>\n",
       "      <td>...</td>\n",
       "      <td>17.520000</td>\n",
       "      <td>16.045472</td>\n",
       "      <td>15.562312</td>\n",
       "      <td>15.507654</td>\n",
       "      <td>57.371035</td>\n",
       "      <td>53.512070</td>\n",
       "      <td>51.521343</td>\n",
       "      <td>213</td>\n",
       "      <td>-0.866562</td>\n",
       "      <td>-0.499069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-02</th>\n",
       "      <td>170.994723</td>\n",
       "      <td>174.070672</td>\n",
       "      <td>170.705096</td>\n",
       "      <td>173.521393</td>\n",
       "      <td>52164500</td>\n",
       "      <td>175.367504</td>\n",
       "      <td>177.989573</td>\n",
       "      <td>178.794950</td>\n",
       "      <td>44.074532</td>\n",
       "      <td>46.667288</td>\n",
       "      <td>...</td>\n",
       "      <td>17.610001</td>\n",
       "      <td>16.194475</td>\n",
       "      <td>15.662199</td>\n",
       "      <td>15.576584</td>\n",
       "      <td>57.659593</td>\n",
       "      <td>53.668966</td>\n",
       "      <td>51.625697</td>\n",
       "      <td>216</td>\n",
       "      <td>-0.839665</td>\n",
       "      <td>-0.543105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-03</th>\n",
       "      <td>172.033357</td>\n",
       "      <td>173.401565</td>\n",
       "      <td>170.595264</td>\n",
       "      <td>172.173172</td>\n",
       "      <td>49594600</td>\n",
       "      <td>175.063282</td>\n",
       "      <td>177.705846</td>\n",
       "      <td>178.577843</td>\n",
       "      <td>42.524287</td>\n",
       "      <td>45.820908</td>\n",
       "      <td>...</td>\n",
       "      <td>19.780001</td>\n",
       "      <td>16.535953</td>\n",
       "      <td>15.863067</td>\n",
       "      <td>15.714401</td>\n",
       "      <td>63.867193</td>\n",
       "      <td>57.237933</td>\n",
       "      <td>54.050905</td>\n",
       "      <td>217</td>\n",
       "      <td>-0.830198</td>\n",
       "      <td>-0.557468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-04</th>\n",
       "      <td>170.864893</td>\n",
       "      <td>173.980798</td>\n",
       "      <td>170.745055</td>\n",
       "      <td>173.431519</td>\n",
       "      <td>53020300</td>\n",
       "      <td>174.907876</td>\n",
       "      <td>177.497342</td>\n",
       "      <td>178.409111</td>\n",
       "      <td>44.444097</td>\n",
       "      <td>46.745486</td>\n",
       "      <td>...</td>\n",
       "      <td>18.580000</td>\n",
       "      <td>16.730625</td>\n",
       "      <td>15.995601</td>\n",
       "      <td>15.808355</td>\n",
       "      <td>58.845181</td>\n",
       "      <td>54.841861</td>\n",
       "      <td>52.568785</td>\n",
       "      <td>218</td>\n",
       "      <td>-0.820486</td>\n",
       "      <td>-0.571667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open_x     High_x      Low_x    Close_x     Volume  \\\n",
       "Date                                                                \n",
       "2023-07-12 189.174968 191.189594 187.968198 189.264740   60750200   \n",
       "2023-07-13 189.992798 190.680963 189.274713 190.032684   41342300   \n",
       "2023-07-14 189.723515 190.670982 189.125121 190.182297   41573900   \n",
       "2023-07-17 191.389049 193.802619 191.299292 193.473495   50520200   \n",
       "2023-07-18 192.835210 193.812596 191.907678 193.214188   48353800   \n",
       "2023-07-19 192.585876 197.702207 192.137062 194.580551   80507300   \n",
       "2023-07-20 194.570572 195.946903 191.987471 192.615799   59581200   \n",
       "2023-07-21 193.583223 194.450902 190.720854 191.428970   71917800   \n",
       "2023-07-24 192.895048 194.391054 191.738132 192.236801   45377800   \n",
       "2023-07-25 192.815256 193.922301 192.406344 193.104477   37283200   \n",
       "2023-07-26 193.154355 195.119111 192.805296 193.982147   47471900   \n",
       "2023-07-27 195.498099 196.674950 192.037337 192.705551   47460200   \n",
       "2023-07-28 194.151694 196.106482 193.623106 195.308609   48291400   \n",
       "2023-07-31 195.537995 195.966858 194.740122 195.926956   38824100   \n",
       "2023-08-01 195.717515 196.206201 194.760064 195.089188   35175100   \n",
       "2023-08-02 194.520703 194.660329 191.339209 192.067261   50389300   \n",
       "2023-08-03 191.059955 191.857813 190.182293 190.661011   61235200   \n",
       "2023-08-04 185.026063 186.881111 181.435642 181.505463  115799700   \n",
       "2023-08-07 181.645075 182.642413 176.877804 178.373810   97576100   \n",
       "2023-08-08 179.211566 179.790024 177.107183 179.321274   67823000   \n",
       "2023-08-09 180.388433 180.448271 176.538710 177.715576   60378500   \n",
       "2023-08-10 179.002129 180.268752 177.127145 177.496155   54686900   \n",
       "2023-08-11 177.086708 178.384986 176.317717 177.556076   51988100   \n",
       "2023-08-14 177.735847 179.453585 177.076712 179.223892   43675600   \n",
       "2023-08-15 178.644649 179.243850 176.817055 177.216522   43622600   \n",
       "2023-08-16 176.896953 178.305086 176.267777 176.337692   46964900   \n",
       "2023-08-17 176.906941 177.276449 173.251752 173.771072   66062900   \n",
       "2023-08-18 172.073301 174.869620 171.733752 174.260422   61114200   \n",
       "2023-08-21 174.839667 175.898270 173.511415 175.608643   46311900   \n",
       "2023-08-22 176.827036 177.446216 176.018105 176.996811   42084200   \n",
       "2023-08-23 178.285129 181.311141 178.095376 180.881699   52722800   \n",
       "2023-08-24 180.432283 180.861725 175.778410 176.147934   54945800   \n",
       "2023-08-25 177.146623 178.914283 175.588678 178.375000   51449600   \n",
       "2023-08-28 179.853052 180.352394 178.315085 179.952927   43820700   \n",
       "2023-08-29 179.463564 184.656719 179.263830 183.877747   53003900   \n",
       "2023-08-30 184.696681 187.602856 184.496947 187.403107   60813900   \n",
       "2023-08-31 187.592859 188.871173 187.233332 187.622818   60794500   \n",
       "2023-09-01 189.240698 189.670125 188.032284 189.210739   45732600   \n",
       "2023-09-05 188.032279 189.730039 187.363162 189.450409   45280000   \n",
       "2023-09-06 188.152109 188.601529 181.231234 182.669342   81755800   \n",
       "2023-09-07 174.949512 177.975539 173.311670 177.326385  112488800   \n",
       "2023-09-08 178.115346 180.002859 177.556070 177.945557   65551300   \n",
       "2023-09-11 179.833081 180.062774 177.106662 179.124008   58953100   \n",
       "2023-09-12 179.253844 179.893002 174.589990 176.068039   90370200   \n",
       "2023-09-13 176.277751 177.066720 173.751081 173.980789   84267900   \n",
       "2023-09-14 173.771073 175.868316 173.351627 175.508789   60895800   \n",
       "2023-09-15 176.247791 176.267769 173.591303 174.779724  109205100   \n",
       "2023-09-18 176.247795 179.143988 175.938205 177.735840   67257600   \n",
       "2023-09-19 177.286433 179.393657 176.896946 178.834396   51826900   \n",
       "2023-09-20 179.024138 179.463562 175.169216 175.259109   58436200   \n",
       "2023-09-21 174.320351 176.068049 173.631257 173.701157   63047900   \n",
       "2023-09-22 174.440176 176.847008 173.820996 174.560013   56725400   \n",
       "2023-09-25 173.970796 176.737156 173.920859 175.848328   46172700   \n",
       "2023-09-26 174.589987 174.969477 171.434141 171.733749   64588900   \n",
       "2023-09-27 172.392870 172.812316 168.827576 170.205750   66921800   \n",
       "2023-09-28 169.117194 171.803657 167.399456 170.465424   56294400   \n",
       "2023-09-29 171.793673 172.842295 170.115876 170.984741   51814200   \n",
       "2023-10-02 170.994723 174.070672 170.705096 173.521393   52164500   \n",
       "2023-10-03 172.033357 173.401565 170.595264 172.173172   49594600   \n",
       "2023-10-04 170.864893 173.980798 170.745055 173.431519   53020300   \n",
       "\n",
       "            EMA20_Close_x  EMA40_Close_x  EMA60_Close_x  RSI20_Close_x  \\\n",
       "Date                                                                     \n",
       "2023-07-12     186.967469     182.281242     177.955451      62.503099   \n",
       "2023-07-13     187.259395     182.659361     178.351425      63.539759   \n",
       "2023-07-14     187.537766     183.026333     178.739323      63.745310   \n",
       "2023-07-17     188.103074     183.535951     179.222411      67.931691   \n",
       "2023-07-18     188.589847     184.008060     179.681157      67.287306   \n",
       "2023-07-19     189.160390     184.523791     180.169662      68.922418   \n",
       "2023-07-20     189.489476     184.918523     180.577732      64.074717   \n",
       "2023-07-21     189.674190     185.236106     180.933510      61.331770   \n",
       "2023-07-24     189.918248     185.577604     181.304110      62.482500   \n",
       "2023-07-25     190.221699     185.944768     181.691007      63.703722   \n",
       "2023-07-26     190.579837     186.336835     182.093996      64.919561   \n",
       "2023-07-27     190.782286     186.647504     182.441915      61.752426   \n",
       "2023-07-28     191.213364     187.069997     182.863774      65.377797   \n",
       "2023-07-31     191.662278     187.502044     183.292075      66.179392   \n",
       "2023-08-01     191.988650     187.872149     183.678866      64.064040   \n",
       "2023-08-02     191.996137     188.076788     183.953895      57.130371   \n",
       "2023-08-03     191.868982     188.202848     184.173801      54.254059   \n",
       "2023-08-04     190.881980     187.876146     184.086314      40.336444   \n",
       "2023-08-07     189.690726     187.412618     183.899019      36.925860   \n",
       "2023-08-08     188.703159     187.017918     183.748929      38.579756   \n",
       "2023-08-09     187.656722     186.564145     183.551114      36.855747   \n",
       "2023-08-10     186.689049     186.121804     183.352591      36.620353   \n",
       "2023-08-11     185.819242     185.703964     183.162541      36.736504   \n",
       "2023-08-14     185.191114     185.387863     183.033405      39.960197   \n",
       "2023-08-15     184.431629     184.989261     182.842688      37.536858   \n",
       "2023-08-16     183.660778     184.567233     182.629409      36.516325   \n",
       "2023-08-17     182.718901     184.040591     182.338972      33.699703   \n",
       "2023-08-18     181.913332     183.563510     182.074101      34.710396   \n",
       "2023-08-21     181.312885     183.175467     181.862119      37.474653   \n",
       "2023-08-22     180.901830     182.874069     181.702601      40.217889   \n",
       "2023-08-23     180.899913     182.776881     181.675686      47.060189   \n",
       "2023-08-24     180.447344     182.453517     181.494448      41.035977   \n",
       "2023-08-25     180.249978     182.254565     181.392171      44.551114   \n",
       "2023-08-28     180.221687     182.142290     181.344983      46.911514   \n",
       "2023-08-29     180.569883     182.226947     181.428024      52.235174   \n",
       "2023-08-30     181.220666     182.479442     181.623929      56.371722   \n",
       "2023-08-31     181.830395     182.730339     181.820614      56.618192   \n",
       "2023-09-01     182.533285     183.046456     182.062913      58.405832   \n",
       "2023-09-05     183.192059     183.358844     182.305126      58.676372   \n",
       "2023-09-06     183.142276     183.325209     182.317067      49.154475   \n",
       "2023-09-07     182.588382     183.032584     182.153438      43.323469   \n",
       "2023-09-08     182.146208     182.784436     182.015475      44.131913   \n",
       "2023-09-11     181.858379     182.605879     181.920673      45.684123   \n",
       "2023-09-12     181.306918     182.286960     181.728783      42.463663   \n",
       "2023-09-13     180.609192     181.881781     181.474751      40.415330   \n",
       "2023-09-14     180.123439     181.570903     181.279145      42.550795   \n",
       "2023-09-15     179.614514     181.239626     181.066049      41.798415   \n",
       "2023-09-18     179.435593     181.068710     180.956862      45.882534   \n",
       "2023-09-19     179.378336     180.959719     180.887273      47.328366   \n",
       "2023-09-20     178.986029     180.681640     180.702743      43.359810   \n",
       "2023-09-21     178.482707     180.341129     180.473183      41.753876   \n",
       "2023-09-22     178.109117     180.059123     180.279309      42.979380   \n",
       "2023-09-25     177.893804     179.853719     180.134031      44.812808   \n",
       "2023-09-26     177.307132     179.457622     179.858612      40.441250   \n",
       "2023-09-27     176.630810     179.006312     179.542124      38.955725   \n",
       "2023-09-28     176.043630     178.589683     179.244527      39.354233   \n",
       "2023-09-29     175.561831     178.218710     178.973715      40.176374   \n",
       "2023-10-02     175.367504     177.989573     178.794950      44.074532   \n",
       "2023-10-03     175.063282     177.705846     178.577843      42.524287   \n",
       "2023-10-04     174.907876     177.497342     178.409111      44.444097   \n",
       "\n",
       "            RSI40_Close_x  ...   Close_y  EMA20_Close_y  EMA40_Close_y  \\\n",
       "Date                       ...                                           \n",
       "2023-07-12      62.598253  ... 13.540000      14.388288      15.115892   \n",
       "2023-07-13      63.079535  ... 13.610000      14.314166      15.042434   \n",
       "2023-07-14      63.174223  ... 13.340000      14.221388      14.959388   \n",
       "2023-07-17      65.188552  ... 13.480000      14.150779      14.887223   \n",
       "2023-07-18      64.901678  ... 13.300000      14.069753      14.809797   \n",
       "2023-07-19      65.717028  ... 13.760000      14.040253      14.758588   \n",
       "2023-07-20      63.540106  ... 13.990000      14.035467      14.721096   \n",
       "2023-07-21      62.262298  ... 13.600000      13.993994      14.666408   \n",
       "2023-07-24      62.784774  ... 13.910000      13.985994      14.629510   \n",
       "2023-07-25      63.343849  ... 13.860000      13.973995      14.591973   \n",
       "2023-07-26      63.906383  ... 13.190000      13.899329      14.523584   \n",
       "2023-07-27      62.476063  ... 14.410000      13.947964      14.518043   \n",
       "2023-07-28      64.153926  ... 13.330000      13.889110      14.460090   \n",
       "2023-07-31      64.540228  ... 13.630000      13.864433      14.419598   \n",
       "2023-08-01      63.587984  ... 13.930000      13.870678      14.395715   \n",
       "2023-08-02      60.296690  ... 16.090000      14.082042      14.478363   \n",
       "2023-08-03      58.843037  ... 15.920000      14.257085      14.548687   \n",
       "2023-08-04      50.683734  ... 17.100000      14.527839      14.673141   \n",
       "2023-08-07      48.332572  ... 15.770000      14.646140      14.726647   \n",
       "2023-08-08      49.065748  ... 15.990000      14.774127      14.788274   \n",
       "2023-08-09      47.884655  ... 15.960000      14.887067      14.845431   \n",
       "2023-08-10      47.723646  ... 15.850000      14.978775      14.894434   \n",
       "2023-08-11      47.772832  ... 14.840000      14.965558      14.891779   \n",
       "2023-08-14      49.138956  ... 14.820000      14.951696      14.888278   \n",
       "2023-08-15      47.601890  ... 16.459999      15.095344      14.964947   \n",
       "2023-08-16      46.942555  ... 16.780001      15.255787      15.053486   \n",
       "2023-08-17      45.072537  ... 17.889999      15.506664      15.191853   \n",
       "2023-08-18      45.497109  ... 17.299999      15.677458      15.294689   \n",
       "2023-08-21      46.662134  ... 17.129999      15.815796      15.384216   \n",
       "2023-08-22      47.839559  ... 16.969999      15.925720      15.461572   \n",
       "2023-08-23      50.947620  ... 15.980000      15.930889      15.486861   \n",
       "2023-08-24      47.416584  ... 17.200001      16.051757      15.570429   \n",
       "2023-08-25      49.118204  ... 15.680000      16.016352      15.575774   \n",
       "2023-08-28      50.287247  ... 15.080000      15.927175      15.551589   \n",
       "2023-08-29      53.039744  ... 14.450000      15.786492      15.497853   \n",
       "2023-08-30      55.318840  ... 13.880000      15.604921      15.418934   \n",
       "2023-08-31      55.457024  ... 13.570000      15.411119      15.328742   \n",
       "2023-09-01      56.455273  ... 13.090000      15.190060      15.219535   \n",
       "2023-09-05      56.605819  ... 14.010000      15.077674      15.160533   \n",
       "2023-09-06      51.444569  ... 14.450000      15.017895      15.125873   \n",
       "2023-09-07      47.914072  ... 14.400000      14.959048      15.090465   \n",
       "2023-09-08      48.335490  ... 13.840000      14.852472      15.029466   \n",
       "2023-09-11      49.138786  ... 13.800000      14.752237      14.969492   \n",
       "2023-09-12      47.187405  ... 14.230000      14.702500      14.933420   \n",
       "2023-09-13      45.910236  ... 13.480000      14.586071      14.862521   \n",
       "2023-09-14      46.987555  ... 12.820000      14.417874      14.762886   \n",
       "2023-09-15      46.533992  ... 13.790000      14.358076      14.715428   \n",
       "2023-09-18      48.597425  ... 14.000000      14.323974      14.680529   \n",
       "2023-09-19      49.342589  ... 14.110000      14.303595      14.652698   \n",
       "2023-09-20      47.065132  ... 15.140000      14.383253      14.676469   \n",
       "2023-09-21      46.113873  ... 17.540001      14.683896      14.816154   \n",
       "2023-09-22      46.722713  ... 17.200001      14.923525      14.932439   \n",
       "2023-09-25      47.633005  ... 16.900000      15.111760      15.028417   \n",
       "2023-09-26      45.108393  ... 18.940001      15.476355      15.219226   \n",
       "2023-09-27      44.215792  ... 18.219999      15.737654      15.365606   \n",
       "2023-09-28      44.407534  ... 17.340000      15.890259      15.461917   \n",
       "2023-09-29      44.796731  ... 17.520000      16.045472      15.562312   \n",
       "2023-10-02      46.667288  ... 17.610001      16.194475      15.662199   \n",
       "2023-10-03      45.820908  ... 19.780001      16.535953      15.863067   \n",
       "2023-10-04      46.745486  ... 18.580000      16.730625      15.995601   \n",
       "\n",
       "            EMA60_Close_y  RSI20_Close_y  RSI40_Close_y  RSI60_Close_y  \\\n",
       "Date                                                                     \n",
       "2023-07-12      15.890125      43.573185      44.658958      45.457967   \n",
       "2023-07-13      15.815366      43.901797      44.795659      45.539996   \n",
       "2023-07-14      15.734207      42.887713      44.362148      45.272916   \n",
       "2023-07-17      15.660298      43.598794      44.647029      45.441638   \n",
       "2023-07-18      15.582912      42.876310      44.347594      45.259198   \n",
       "2023-07-19      15.523144      45.314081      45.309005      45.824467   \n",
       "2023-07-20      15.472877      46.515380      45.789263      46.107438   \n",
       "2023-07-21      15.411471      44.760358      45.100495      45.695861   \n",
       "2023-07-24      15.362243      46.450845      45.765579      46.084894   \n",
       "2023-07-25      15.312989      46.210746      45.674045      46.030805   \n",
       "2023-07-26      15.243383      43.070545      44.452144      45.306179   \n",
       "2023-07-27      15.216059      49.631071      47.095406      46.855384   \n",
       "2023-07-28      15.154221      44.818280      45.144928      45.690299   \n",
       "2023-07-31      15.104246      46.339773      45.784631      46.069119   \n",
       "2023-08-01      15.065746      47.853259      46.425421      46.448977   \n",
       "2023-08-02      15.099328      57.037165      50.726105      49.075280   \n",
       "2023-08-03      15.126236      56.216928      50.399520      48.883399   \n",
       "2023-08-04      15.190949      60.379924      52.573301      50.256304   \n",
       "2023-08-07      15.209934      54.258995      50.038194      48.755342   \n",
       "2023-08-08      15.235510      55.052368      50.443605      49.011508   \n",
       "2023-08-09      15.259264      54.915646      50.386422      48.977556   \n",
       "2023-08-10      15.278633      54.394250      50.172532      48.851368   \n",
       "2023-08-11      15.264251      49.822294      48.243912      47.703742   \n",
       "2023-08-14      15.249686      49.735156      48.206277      47.681185   \n",
       "2023-08-15      15.289368      56.327922      51.395129      49.665914   \n",
       "2023-08-16      15.338241      57.473559      51.986721      50.041973   \n",
       "2023-08-17      15.421905      61.190832      53.979502      51.324810   \n",
       "2023-08-18      15.483482      58.337707      52.785143      50.622165   \n",
       "2023-08-21      15.537466      57.524205      52.442233      50.419907   \n",
       "2023-08-22      15.584435      56.740329      52.115420      50.227828   \n",
       "2023-08-23      15.597404      52.114910      50.132746      49.051985   \n",
       "2023-08-24      15.649948      56.694283      52.420581      50.504092   \n",
       "2023-08-25      15.650933      50.376127      49.517578      48.743844   \n",
       "2023-08-28      15.632214      48.146658      48.431706      48.071270   \n",
       "2023-08-29      15.593453      45.901391      47.314337      47.373292   \n",
       "2023-08-30      15.537274      43.949455      46.322617      46.748753   \n",
       "2023-08-31      15.472774      42.904948      45.787283      46.410355   \n",
       "2023-09-01      15.394650      41.304966      44.962097      45.887323   \n",
       "2023-09-05      15.349252      45.412000      46.845275      47.050436   \n",
       "2023-09-06      15.319768      47.269503      47.722684      47.598249   \n",
       "2023-09-07      15.289612      47.077881      47.631048      47.541409   \n",
       "2023-09-08      15.242083      44.930554      46.603111      46.903454   \n",
       "2023-09-11      15.194802      44.776992      46.529545      46.857780   \n",
       "2023-09-12      15.163169      46.833204      47.444252      47.417556   \n",
       "2023-09-13      15.107983      43.836437      46.035453      46.547861   \n",
       "2023-09-14      15.032967      41.383506      44.833880      45.796171   \n",
       "2023-09-15      14.992214      46.053514      46.922185      47.073605   \n",
       "2023-09-18      14.959683      47.015504      47.364613      47.346805   \n",
       "2023-09-19      14.931824      47.531392      47.599270      47.491186   \n",
       "2023-09-20      14.938650      52.125809      49.750701      48.827380   \n",
       "2023-09-21      15.023940      60.590034      54.240630      51.737559   \n",
       "2023-09-22      15.095286      59.033606      53.545453      51.317112   \n",
       "2023-09-25      15.154457      57.657957      52.931516      50.945615   \n",
       "2023-09-26      15.278573      63.710919      56.416690      53.284252   \n",
       "2023-09-27      15.375013      60.497883      54.943995      52.387820   \n",
       "2023-09-28      15.439439      56.811796      53.203046      51.314796   \n",
       "2023-09-29      15.507654      57.371035      53.512070      51.521343   \n",
       "2023-10-02      15.576584      57.659593      53.668966      51.625697   \n",
       "2023-10-03      15.714401      63.867193      57.237933      54.050905   \n",
       "2023-10-04      15.808355      58.845181      54.841861      52.568785   \n",
       "\n",
       "            shifted_day_of_year  cos_shifted_annual  sin_shifted_annual  \n",
       "Date                                                                     \n",
       "2023-07-12                  134           -0.670089            0.742281  \n",
       "2023-07-13                  135           -0.682758            0.730644  \n",
       "2023-07-14                  136           -0.695225            0.718792  \n",
       "2023-07-17                  139           -0.731378            0.681972  \n",
       "2023-07-18                  140           -0.743001            0.669290  \n",
       "2023-07-19                  141           -0.754404            0.656411  \n",
       "2023-07-20                  142           -0.765584            0.643337  \n",
       "2023-07-21                  143           -0.776537            0.630072  \n",
       "2023-07-24                  146           -0.808005            0.589176  \n",
       "2023-07-25                  147           -0.818020            0.575190  \n",
       "2023-07-26                  148           -0.827793            0.561034  \n",
       "2023-07-27                  149           -0.837321            0.546711  \n",
       "2023-07-28                  150           -0.846602            0.532227  \n",
       "2023-07-31                  153           -0.872929            0.487847  \n",
       "2023-08-01                  154           -0.881192            0.472759  \n",
       "2023-08-02                  155           -0.889193            0.457531  \n",
       "2023-08-03                  156           -0.896932            0.442168  \n",
       "2023-08-04                  157           -0.904405            0.426674  \n",
       "2023-08-07                  160           -0.925211            0.379453  \n",
       "2023-08-08                  161           -0.931601            0.363482  \n",
       "2023-08-09                  162           -0.937716            0.347403  \n",
       "2023-08-10                  163           -0.943553            0.331221  \n",
       "2023-08-11                  164           -0.949111            0.314942  \n",
       "2023-08-14                  167           -0.964094            0.265563  \n",
       "2023-08-15                  168           -0.968519            0.248940  \n",
       "2023-08-16                  169           -0.972658            0.232243  \n",
       "2023-08-17                  170           -0.976509            0.215477  \n",
       "2023-08-18                  171           -0.980071            0.198648  \n",
       "2023-08-21                  174           -0.989013            0.147827  \n",
       "2023-08-22                  175           -0.991410            0.130793  \n",
       "2023-08-23                  176           -0.993513            0.113720  \n",
       "2023-08-24                  177           -0.995322            0.096613  \n",
       "2023-08-25                  178           -0.996837            0.079477  \n",
       "2023-08-28                  181           -0.999609            0.027950  \n",
       "2023-08-29                  182           -0.999942            0.010751  \n",
       "2023-08-30                  183           -0.999979           -0.006451  \n",
       "2023-08-31                  184           -0.999720           -0.023651  \n",
       "2023-09-01                  185           -0.999166           -0.040844  \n",
       "2023-09-05                  189           -0.993993           -0.109446  \n",
       "2023-09-06                  190           -0.991963           -0.126528  \n",
       "2023-09-07                  191           -0.989640           -0.143572  \n",
       "2023-09-08                  192           -0.987024           -0.160575  \n",
       "2023-09-11                  195           -0.977426           -0.211276  \n",
       "2023-09-12                  196           -0.973648           -0.228058  \n",
       "2023-09-13                  197           -0.969581           -0.244772  \n",
       "2023-09-14                  198           -0.965227           -0.261414  \n",
       "2023-09-15                  199           -0.960587           -0.277979  \n",
       "2023-09-18                  202           -0.944969           -0.327160  \n",
       "2023-09-19                  203           -0.939201           -0.343367  \n",
       "2023-09-20                  204           -0.933156           -0.359472  \n",
       "2023-09-21                  205           -0.926834           -0.375470  \n",
       "2023-09-22                  206           -0.920239           -0.391358  \n",
       "2023-09-25                  209           -0.898825           -0.438307  \n",
       "2023-09-26                  210           -0.891153           -0.453703  \n",
       "2023-09-27                  211           -0.883217           -0.468965  \n",
       "2023-09-28                  212           -0.875019           -0.484089  \n",
       "2023-09-29                  213           -0.866562           -0.499069  \n",
       "2023-10-02                  216           -0.839665           -0.543105  \n",
       "2023-10-03                  217           -0.830198           -0.557468  \n",
       "2023-10-04                  218           -0.820486           -0.571667  \n",
       "\n",
       "[60 rows x 24 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormatDataFrame():\n",
    "    \"\"\"\n",
    "    This class is used to take in a dataFrame and add various different kinds of indicators as columns into the dataset in order to create additional data\n",
    "    for the tensor inputs in the method chain.\n",
    "    \"\"\"\n",
    "    def __init__(self, VIXDataFrame, TickerDataFrame):\n",
    "        self.VIXDataFrame = VIXDataFrame\n",
    "        self.TickerDataFrame = TickerDataFrame\n",
    "        self.CombinedDataSet = None\n",
    "        \n",
    "    def GetCombinedData(self):\n",
    "        \"\"\"\n",
    "        Returns the combined dataset after processing\n",
    "        \"\"\"\n",
    "        return self.CombinedDataSet\n",
    "        \n",
    "    def CreateDataSet(self):\n",
    "        \"\"\"\n",
    "        This method will execute all work requirements.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.TickerDataFrame is None or self.VIXDataFrame is None:\n",
    "                raise TypeError(\"Missing DataFrames\")\n",
    "\n",
    "            #print(\"Transforming Ticker Data...\")\n",
    "            self.TransformTickerData()\n",
    "            #print(\"Ticker Data Transformed.\")\n",
    "\n",
    "            #print(\"Transforming VIX Data...\")\n",
    "            self.TransformVIXData()\n",
    "            #print(\"VIX Data Transformed.\")\n",
    "\n",
    "            #print(\"Creating Combined Data Set...\")\n",
    "            self.CreateCombinedDataSet()\n",
    "            #print(f\"Combined Data Set Created with shape: {self.CombinedDataSet.shape}\")\n",
    "\n",
    "            #print(\"Validating Data...\")\n",
    "            self.ValidateData()\n",
    "            #print(\"Data Validated.\")\n",
    "\n",
    "            #print(\"Applying Fourier Transform...\")\n",
    "            self.applyFourierTransform()\n",
    "            #print(\"Fourier Transform Applied.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def ValidateData(self):\n",
    "        \"\"\"\n",
    "        Validates the combined DataFrame for NaN values and checks date alignment.\n",
    "        \"\"\"\n",
    "        if self.CombinedDataSet is None:\n",
    "            raise ValueError(\"Combined DataSet is not available for validation.\")\n",
    "\n",
    "        # Check for NaN values\n",
    "        nan_counts = self.CombinedDataSet.isna().sum()\n",
    "        if nan_counts.sum() > 0:\n",
    "            print(\"Warning: NaN values found in the following columns:\")\n",
    "            print(nan_counts[nan_counts > 0])\n",
    "\n",
    "        # Check for date alignment\n",
    "        if not self.CombinedDataSet.index.is_monotonic_increasing:\n",
    "            raise ValueError(\"Date index is not in chronological order.\")\n",
    "\n",
    "        print(\"Data validation completed.\")\n",
    "        \n",
    "    def CreateCombinedDataSet(self):\n",
    "        # Ensure 'Date' is the index and perform the merge\n",
    "        if self.TickerDataFrame.index.name != 'Date' or self.VIXDataFrame.index.name != 'Date':\n",
    "            raise ValueError(\"Date must be the index for both DataFrames.\")\n",
    "\n",
    "        # Merge the DataFrames on the 'Date' index\n",
    "        self.CombinedDataSet = pd.merge(self.TickerDataFrame, self.VIXDataFrame, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "        if self.CombinedDataSet.empty:\n",
    "            raise ValueError(\"Merging resulted in an empty DataFrame. Check if the indices align correctly.\")\n",
    "        \n",
    "\n",
    "    def TransformTickerData(self):\n",
    "        \"\"\"\n",
    "        This method should take the dataframe for the Ticker, it'll check if it exists first and if it doesn't throw an error. Else\n",
    "        it will add 20,40,60 EMA on close, and 20,40,60 EMA on volume, as well as RSI for both.\n",
    "        \"\"\"\n",
    "        if self.TickerDataFrame is None:\n",
    "            raise TypeError(\"Missing DataFrame\")\n",
    "            \n",
    "        # Calculate EMAs for Close prices\n",
    "        self.TickerDataFrame['EMA20_Close'] = ta.trend.ema_indicator(close=self.TickerDataFrame['Close'], window=20)\n",
    "        self.TickerDataFrame['EMA40_Close'] = ta.trend.ema_indicator(close=self.TickerDataFrame['Close'], window=40)\n",
    "        self.TickerDataFrame['EMA60_Close'] = ta.trend.ema_indicator(close=self.TickerDataFrame['Close'], window=60)\n",
    "\n",
    "        # Calculate RSIs for Close prices\n",
    "        self.TickerDataFrame['RSI20_Close'] = ta.momentum.rsi(close=self.TickerDataFrame['Close'], window=20)\n",
    "        self.TickerDataFrame['RSI40_Close'] = ta.momentum.rsi(close=self.TickerDataFrame['Close'], window=40)\n",
    "        self.TickerDataFrame['RSI60_Close'] = ta.momentum.rsi(close=self.TickerDataFrame['Close'], window=60)\n",
    "        \n",
    "        # Drop rows where any of the EMA or RSI columns contain NaN\n",
    "        columns_to_check = ['EMA20_Close', 'EMA40_Close', 'EMA60_Close', \n",
    "                            'RSI20_Close', 'RSI40_Close', 'RSI60_Close']\n",
    "\n",
    "        self.TickerDataFrame = self.TickerDataFrame.dropna(subset=columns_to_check)\n",
    "        #self.TickerDataFrame = self.TickerDataFrame.drop(columns=['Adj Close'])\n",
    "        \n",
    "    def applyFourierTransform(self):\n",
    "        \"\"\"\n",
    "        Applies a fourier transform on the date column in order to make it more consumable by the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate the shifted day of year for each date in the DataFrame\n",
    "        self.CombinedDataSet['shifted_day_of_year'] = self.CombinedDataSet.index.map(self.shifted_day_of_year)\n",
    "\n",
    "        # Define the number of days in this \"year\" - accounting for leap years\n",
    "        # This is a simplification and could be further refined for exact leap year handling\n",
    "        days_in_year = 365.25\n",
    "\n",
    "        # Add Fourier features to df for the shifted year\n",
    "        self.CombinedDataSet['cos_shifted_annual'] = np.cos(2 * np.pi * self.CombinedDataSet['shifted_day_of_year'] / days_in_year)\n",
    "        self.CombinedDataSet['sin_shifted_annual'] = np.sin(2 * np.pi * self.CombinedDataSet['shifted_day_of_year'] / days_in_year)\n",
    "        \n",
    "    # Function to calculate the shifted day of year\n",
    "    def shifted_day_of_year(self, date):\n",
    "        \"\"\"\n",
    "        This will apply the transform specifically starting March 1st as the start of the new year.\n",
    "        \"\"\"\n",
    "        # Set the start of the \"new year\" to March 1st\n",
    "        new_year_start = pd.Timestamp(year=date.year, month=3, day=1)\n",
    "        # Calculate the number of days from the start of the \"new year\"\n",
    "        day_of_year = (date - new_year_start).days + 1\n",
    "        # Handle dates before March 1st\n",
    "        if date < new_year_start:\n",
    "            # Shift to the previous year\n",
    "            previous_year_start = pd.Timestamp(year=date.year-1, month=3, day=1)\n",
    "            day_of_year = (date - previous_year_start).days + 1\n",
    "        return day_of_year\n",
    "\n",
    "    def TransformVIXData(self):\n",
    "        \"\"\"\n",
    "        This will transform the vix data adding EMA and RSI to the closed price at specific ranges as well as applying a fourier transform to the date, starting March 1st as new year.\n",
    "        \"\"\"\n",
    "        if self.VIXDataFrame is None:\n",
    "            raise TypeError(\"Missing DataFrame\")\n",
    "        \n",
    "        # Calculate EMAs for Close prices\n",
    "        self.VIXDataFrame['EMA20_Close'] = ta.trend.ema_indicator(close=self.VIXDataFrame['Close'], window=20)\n",
    "        self.VIXDataFrame['EMA40_Close'] = ta.trend.ema_indicator(close=self.VIXDataFrame['Close'], window=40)\n",
    "        self.VIXDataFrame['EMA60_Close'] = ta.trend.ema_indicator(close=self.VIXDataFrame['Close'], window=60)\n",
    "\n",
    "        # Calculate RSIs for Close prices\n",
    "        self.VIXDataFrame['RSI20_Close'] = ta.momentum.rsi(close=self.VIXDataFrame['Close'], window=20)\n",
    "        self.VIXDataFrame['RSI40_Close'] = ta.momentum.rsi(close=self.VIXDataFrame['Close'], window=40)\n",
    "        self.VIXDataFrame['RSI60_Close'] = ta.momentum.rsi(close=self.VIXDataFrame['Close'], window=60)\n",
    "\n",
    "        # Drop rows where any of the EMA or RSI columns contain NaN\n",
    "        columns_to_check = ['EMA20_Close', 'EMA40_Close', 'EMA60_Close', \n",
    "                            'RSI20_Close', 'RSI40_Close', 'RSI60_Close']\n",
    "\n",
    "        self.VIXDataFrame = self.VIXDataFrame.dropna(subset=columns_to_check)\n",
    "        self.VIXDataFrame = self.VIXDataFrame.drop(columns=['Adj Close', 'Volume'])\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressTracker:\n",
    "    \"\"\"\n",
    "    This object is for retaining data in a CSV and randomly returning a ticker and IPO date so we can parse it from yFinance. \n",
    "    It also makes sure that nothing is repeated so we have a range of trading data.\n",
    "    \"\"\"\n",
    "    PROGRESS_FILE = 'C:\\\\Machine Learning\\\\ProgressTracker.csv'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.create_or_load_csv()\n",
    "        self.load_progress()\n",
    "\n",
    "    def create_or_load_csv(self):\n",
    "        file_dir = os.path.dirname(ProgressTracker.PROGRESS_FILE)\n",
    "        if not os.path.exists(file_dir):\n",
    "            os.makedirs(file_dir)\n",
    "\n",
    "        if not os.path.isfile(ProgressTracker.PROGRESS_FILE):\n",
    "            empty_df = pd.DataFrame({'symbol': [], 'ipoDate': [], 'Completed': []})\n",
    "            empty_df.to_csv(ProgressTracker.PROGRESS_FILE, index=False)\n",
    "\n",
    "    def load_dataframe(self, dataframe):\n",
    "        if os.path.exists(ProgressTracker.PROGRESS_FILE):\n",
    "            os.replace(ProgressTracker.PROGRESS_FILE, 'backup_' + ProgressTracker.PROGRESS_FILE)\n",
    "        dataframe.to_csv(ProgressTracker.PROGRESS_FILE, index=False)\n",
    "\n",
    "    def mark_completed(self, symbol):\n",
    "        index = next((idx for idx, row in enumerate(self.symbols) if row[0] == symbol), None)\n",
    "        if index is not None:\n",
    "            self.completed[index] = True\n",
    "\n",
    "    def mark_uncompleted(self, symbol):\n",
    "        index = next((idx for idx, row in enumerate(self.symbols) if row[0] == symbol), None)\n",
    "        if index is not None:\n",
    "            self.completed[index] = False\n",
    "\n",
    "    def random_next(self):\n",
    "        available_symbols = [sym for sym, comp in zip(self.symbols, self.completed) if not comp]\n",
    "        if not available_symbols:  # No available symbols\n",
    "            self.clear_completed()  # Clear all completed flags\n",
    "            available_symbols = [sym for sym, comp in zip(self.symbols, self.completed) if not comp]  # Recheck available symbols\n",
    "        selected_symbol = random.choice(available_symbols)\n",
    "        index = self.symbols.index(selected_symbol)\n",
    "        self.completed[index] = True\n",
    "        self.save_progress()\n",
    "        return selected_symbol[0], selected_symbol[1]  # Return the symbol and its IPO date\n",
    "\n",
    "    def clear_completed(self):\n",
    "        self.completed = [False] * len(self.symbols)\n",
    "        self.save_progress()\n",
    "\n",
    "    def save_progress(self):\n",
    "        progress_df = pd.DataFrame(self.symbols, columns=['symbol', 'ipoDate'])\n",
    "        progress_df['Completed'] = self.completed\n",
    "        progress_df.to_csv(ProgressTracker.PROGRESS_FILE, index=False)\n",
    "\n",
    "\n",
    "    def load_progress(self):\n",
    "        if os.path.exists(ProgressTracker.PROGRESS_FILE):\n",
    "            ticker_data_frame = pd.read_csv(ProgressTracker.PROGRESS_FILE)\n",
    "            self.symbols = ticker_data_frame[['symbol', 'ipoDate']].values.tolist()\n",
    "            self.completed = ticker_data_frame['Completed'].astype(bool).tolist()\n",
    "        else:\n",
    "            self.symbols = []\n",
    "            self.completed = []\n",
    "\n",
    "    def get_symbols_df(self):\n",
    "        if os.path.exists(ProgressTracker.PROGRESS_FILE):\n",
    "            return pd.read_csv(ProgressTracker.PROGRESS_FILE)\n",
    "        else:\n",
    "            df = pd.DataFrame({'symbol': [], 'ipoDate': [], 'Completed': []})\n",
    "            df.to_csv(ProgressTracker.PROGRESS_FILE, index=False)\n",
    "            return df\n",
    "        \n",
    "        \n",
    "# tracker = ProgressTracker()\n",
    "\n",
    "# # Mark IBM as completed\n",
    "# tracker.mark_completed('IBM')\n",
    "\n",
    "# # Randomly select a symbol to process\n",
    "# ticker_to_process = tracker.random_next()\n",
    "# print(ticker_to_process)\n",
    "\n",
    "# # Clear all completed items\n",
    "# tracker.clear_completed()\n",
    "\n",
    "# # Load progress from the CSV file\n",
    "# tracker.load_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding self.fc:\n",
    "\n",
    "    What is self.fc?\n",
    "        self.fc is a class attribute you're defining in the DCGANGenerator class.\n",
    "        It stands for a \"fully connected\" layer or layers in neural network terminology. However, in your case, self.fc is not just a single fully connected layer but a sequential container (nn.Sequential) of several layers.\n",
    "\n",
    "    Role of nn.Sequential:\n",
    "        nn.Sequential is a PyTorch module that sequentially applies a list of modules. It is used for encapsulating a simple sequence of layers or operations, automatically handling the forward pass through these layers in the order they are added.\n",
    "        In nn.Sequential, the output of one layer is automatically passed as input to the next.\n",
    "\n",
    "    Understanding the Sequence of Methods:\n",
    "        Linear Layer (nn.Linear): This is the first layer in the sequence. It's a fully connected layer that linearly transforms the input data (input_size) to a higher dimensional space (hidden_layer_size * 4 * 4 * 256). This expansion is typical in GAN generators to prepare for reshaping and further processing in subsequent layers.\n",
    "        Batch Normalization Layer (nn.BatchNorm1d): Follows the linear layer. Batch normalization is used to stabilize and speed up training by normalizing the output of the previous layer. It can help mitigate issues like vanishing or exploding gradients.\n",
    "        Activation Function (nn.ReLU): This is the Rectified Linear Unit activation function. It introduces non-linearity into the model, allowing it to learn more complex patterns. The ReLU function is defined as f(x) = max(0, x) and is applied element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAdvancedCNNBlock(nn.Module):\n",
    "    def __init__(self, input_channels, intermediate_channels):\n",
    "        super().__init__()\n",
    "        self.model1 = models.resnet50(pretrained=False)\n",
    "        self.model1.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model1.norm1 = nn.Identity()\n",
    "        self.model2 = models.densenet121(pretrained=False)\n",
    "        self.model3 = models.inception_v3(pretrained=False, aux_logits=None, transform_inputs=False)\n",
    "        self.model3.AuxLogits = None\n",
    "        self.model4 = models.efficientnet_b7(pretrained=False)\n",
    "        self.model4._blocks[0].conv = nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "        self.adapter_block = nn.Sequential(\n",
    "            nn.Conv2d(intermediate_channels*4, intermediate_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(intermediate_channels),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        x = self.model2(x)\n",
    "        x = self.model3(x)\n",
    "        x = self.model4(x)\n",
    "        x = torch.cat([x[0][-1], x[-1]], dim=1)\n",
    "        x = self.adapter_block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTCNModule(nn.Module):\n",
    "    def __init__(self, input_channels, nb_filters, kernel_size, dilations, dropout_p):\n",
    "        super().__init__()\n",
    "        self.tcn = tcn_module.TemporalConvolution(input_channels, nb_filters, kernel_size, dilations, dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.tcn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridLSTMGRUAttentionWithCNN(nn.Module): # THIS IS MY IN PRODUCTION MODEL\n",
    "    def __init__(self, input_size: int, hidden_layer_size: int, num_layers: int, output_size: int, dropout_rate: float, num_heads: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # CNN layers\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=hidden_layer_size // 2, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm1d(hidden_layer_size // 2),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.cnn_output_size = hidden_layer_size // 2  # Output size of CNN layers\n",
    "\n",
    "        # Define LSTM and GRU layers alternatively\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            is_last_layer = i == num_layers - 1\n",
    "            layer_dropout = 0 if is_last_layer else dropout_rate\n",
    "            #input_dim = self.cnn_output_size if i == 0 else (hidden_layer_size if i % 2 == 0 else 2 * hidden_layer_size)\n",
    "            input_dim = self.cnn_output_size if i == 0 else 2 * hidden_layer_size\n",
    "            rnn_type = nn.LSTM if i % 2 == 0 else nn.GRU\n",
    "            self.rnn_layers.append(rnn_type(input_dim, hidden_layer_size, batch_first=True, bidirectional=True, dropout=layer_dropout))\n",
    "\n",
    "        # Multi-Head Attention layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=2 * hidden_layer_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(2 * hidden_layer_size, 1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_layer_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, output_size)\n",
    "        )\n",
    "\n",
    "        # Manual dropout layers between RNN layers\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(dropout_rate) for _ in range(num_layers - 1)])\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.fill_(0)\n",
    "            elif isinstance(m, nn.GRU):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(\"Pre-Permuation RNN Layer Shape:\", x.shape)  # Print statement here\n",
    "        # Apply CNN layers\n",
    "        x = x.permute(0, 2, 1)  # Change axes order for CNN operation\n",
    "        #print(\"Pre-First RNN Layer Shape:\", x.shape)  # Print statement here\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.permute(0, 2, 1)  # Restore original axes order\n",
    "        #print(\"Post-CNN Shape:\", x.shape)  # Debug print statement\n",
    "\n",
    "        # Process RNN layers\n",
    "        for i, rnn in enumerate(self.rnn_layers):\n",
    "            #print(f\"Pre-RNN Layer {i} Shape:\", x.shape)  # Print statement here\n",
    "            #rnn.flatten_parameters()  # Flatten RNN parameters\n",
    "            x, _ = rnn(x)\n",
    "            #print(f\"Post-RNN Layer {i} Shape:\", x.shape)  # Debug print statement\n",
    "            if i < self.num_layers - 1:  # Apply dropout manually between RNN layers, but not after the last layer\n",
    "                x = self.dropout_layers[i](x)\n",
    "\n",
    "        # Separate forward and backward outputs\n",
    "        fw_outputs, bw_outputs = x.chunk(2, dim=2)\n",
    "\n",
    "        # Concatenate forward and backward outputs\n",
    "        outputs = torch.cat([fw_outputs, bw_outputs], dim=-1)\n",
    "\n",
    "        # Apply Multi-Head Attention\n",
    "        attn_output, _ = self.multihead_attn(outputs, outputs, outputs)\n",
    "        #print(\"Post-Attention Shape:\", attn_output.shape)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_weights = F.softmax(self.attention(attn_output), dim=1)\n",
    "        x = torch.sum(attn_output * attention_weights, dim=1)\n",
    "        #print(\"Post-Weighted Attention Shape:\", x.shape)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        outputs = self.fc(x)\n",
    "        #print(\"Post-Fully Connected Shape:\", outputs.shape)\n",
    "        return outputs\n",
    "\n",
    "#MODEL IN PRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridLSTMGRUAttentionWithCNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_layer_size: int, num_layers: int, output_size: int, dropout_rate: float, num_heads: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # CNN layers\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=hidden_layer_size // 2, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm1d(hidden_layer_size // 2),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.cnn_output_size = hidden_layer_size // 2  # Output size of CNN layers\n",
    "\n",
    "        self.my_tcn = MyTCNModule(input_channels=hidden_layer_size // 2, nb_filters=64, kernel_size=2, dilations=[1, 2, 4, 8, 16], dropout_p=0.2)\n",
    "\n",
    "        # Define LSTM and GRU layers alternatively\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            is_last_layer = i == num_layers - 1\n",
    "            layer_dropout = 0 if is_last_layer else dropout_rate\n",
    "            #input_dim = self.cnn_output_size if i == 0 else (hidden_layer_size if i % 2 == 0 else 2 * hidden_layer_size)\n",
    "            input_dim = self.cnn_output_size if i == 0 else 2 * hidden_layer_size\n",
    "            rnn_type = nn.LSTM if i % 2 == 0 else nn.GRU\n",
    "            self.rnn_layers.append(rnn_type(input_dim, hidden_layer_size, batch_first=True, bidirectional=True, dropout=layer_dropout))\n",
    "\n",
    "        # Multi-Head Attention layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=2 * hidden_layer_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(2 * hidden_layer_size, 1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_layer_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, output_size)\n",
    "        )\n",
    "\n",
    "        # Manual dropout layers between RNN layers\n",
    "        self.dropout_layers = nn.ModuleList([nn.Dropout(dropout_rate) for _ in range(num_layers - 1)])\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.fill_(0)\n",
    "            elif isinstance(m, nn.GRU):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(\"Pre-Permuation RNN Layer Shape:\", x.shape)  # Print statement here\n",
    "        # Apply CNN layers\n",
    "        x = x.permute(0, 2, 1)  # Change axes order for CNN operation\n",
    "        #print(\"Pre-First RNN Layer Shape:\", x.shape)  # Print statement here\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.permute(0, 2, 1)  # Restore original axes order\n",
    "        #print(\"Post-CNN Shape:\", x.shape)  # Debug print statement\n",
    "        \n",
    "        # Pass the CNN output through the TCN module\n",
    "        x = self.my_tcn(x)\n",
    "        # Process RNN layers\n",
    "        for i, rnn in enumerate(self.rnn_layers):\n",
    "            #print(f\"Pre-RNN Layer {i} Shape:\", x.shape)  # Print statement here\n",
    "            #rnn.flatten_parameters()  # Flatten RNN parameters\n",
    "            x, _ = rnn(x)\n",
    "            #print(f\"Post-RNN Layer {i} Shape:\", x.shape)  # Debug print statement\n",
    "            if i < self.num_layers - 1:  # Apply dropout manually between RNN layers, but not after the last layer\n",
    "                x = self.dropout_layers[i](x)\n",
    "\n",
    "        # Separate forward and backward outputs\n",
    "        fw_outputs, bw_outputs = x.chunk(2, dim=2)\n",
    "\n",
    "        # Concatenate forward and backward outputs\n",
    "        outputs = torch.cat([fw_outputs, bw_outputs], dim=-1)\n",
    "\n",
    "        # Apply Multi-Head Attention\n",
    "        attn_output, _ = self.multihead_attn(outputs, outputs, outputs)\n",
    "        #print(\"Post-Attention Shape:\", attn_output.shape)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_weights = F.softmax(self.attention(attn_output), dim=1)\n",
    "        x = torch.sum(attn_output * attention_weights, dim=1)\n",
    "        #print(\"Post-Weighted Attention Shape:\", x.shape)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        outputs = self.fc(x)\n",
    "        #print(\"Post-Fully Connected Shape:\", outputs.shape)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already defined your AdvancedLSTM class with appropriate changes\n",
    "\n",
    "# Define the number of features and prediction length\n",
    "num_features = 24  # Number of features in the dataset\n",
    "prediction_length = 30  # Number of days you want to predict\n",
    "\n",
    "# Instantiate the model\n",
    "model = HybridLSTMGRUAttentionWithCNN(input_size=24, \n",
    "                     hidden_layer_size=600, \n",
    "                     num_layers=26, \n",
    "                     output_size=prediction_length * num_features,\n",
    "                     dropout_rate= 0.0,\n",
    "                     num_heads= 8, kernel_size= 8).to(device)  # output_size is 30 days * 9 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Decay L2 Reg\n",
    "\n",
    "The weight_decay parameter in neural network training, which applies L2 regularization, has a significant impact on how the model learns. Here's how changing it affects the training process:\n",
    "\n",
    "    Increasing Weight Decay:\n",
    "        This means you're increasing the strength of L2 regularization.\n",
    "        It penalizes larger weights more heavily, encouraging the model to keep the weights smaller and more uniform.\n",
    "        This can help prevent overfitting, especially in complex models or when you have limited training data, as it encourages the model to be simpler.\n",
    "        However, if the weight decay is set too high, it might lead to underfitting, where the model becomes overly simple and fails to capture the underlying patterns in the data.\n",
    "\n",
    "    Decreasing Weight Decay:\n",
    "        Lowering the weight decay reduces the strength of L2 regularization.\n",
    "        It allows the model to have larger weights, which can be beneficial if the model is too simple and underfitting.\n",
    "        This gives the model more flexibility to learn from the data, which can be useful if your model is not capturing complex patterns in the training data.\n",
    "        On the flip side, too little regularization might lead to overfitting, where the model learns the noise in the training data rather than the actual signal.\n",
    "\n",
    "In summary, adjusting the weight decay is a balancing act:\n",
    "\n",
    "    Increase it if your model is overfitting (i.e., it performs well on training data but poorly on validation/test data).\n",
    "    Decrease it if your model is underfitting (i.e., it's not performing well even on the training data).\n",
    "\n",
    "Finding the right level of regularization is crucial for training effective neural network models. It often requires some experimentation and tuning, alongside monitoring the models performance on both training and validation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate\n",
    "\n",
    "Adjusting the learning rate, which is a key hyperparameter in neural network training, significantly impacts the learning dynamics. Here's how changing it affects the training process:\n",
    "\n",
    "    Increasing the Learning Rate:\n",
    "        Faster Learning: A higher learning rate can speed up the learning process. The model's weights are updated more significantly with each iteration, potentially leading to faster convergence.\n",
    "        Risk of Instability: However, a too high learning rate can cause the training to become unstable. The model might overshoot the optimal points in the loss landscape, leading to erratic behavior in loss and other metrics.\n",
    "        Potential to Skip Optimal Solutions: If the learning rate is excessively high, the optimizer might skip over minima, failing to converge to the best solution.\n",
    "\n",
    "    Decreasing the Learning Rate:\n",
    "        More Stable, Gradual Learning: A lower learning rate results in smaller, more precise updates to the weights. This can lead to more stable convergence and finer adjustments to the model parameters.\n",
    "        Risk of Slow Convergence: While stability is increased, the downside is that learning might be slower. The model might take a longer time to converge to an optimal solution.\n",
    "        Potential for Getting Stuck in Local Minima: With a very low learning rate, there's a risk that the model might get stuck in local minima or take an excessively long time to escape flat regions in the loss landscape.\n",
    "\n",
    "In summary, the learning rate needs to be carefully chosen:\n",
    "\n",
    "    Increase it if the training process is too slow, and there are no signs of convergence.\n",
    "    Decrease it if the training process is unstable, with erratic loss values or failure to improve the loss over epochs.\n",
    "\n",
    "Typically, it's common to start with a higher learning rate and then reduce it as training progresses (using learning rate schedulers). This approach allows for rapid learning initially and more refined adjustments later in the training. Experimenting with different learning rates and monitoring the training/validation performance is essential to find the best setting for your specific model and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 Regularization techniques\n",
    "\n",
    "L1 and L2 regularization are powerful techniques to prevent overfitting in machine learning models, including neural networks. Here's how to discern when and how to manipulate these regularization parameters:\n",
    "L1 Regularization (Lasso):\n",
    "\n",
    "    Characteristics: L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients. It can lead to sparse models where some weights become exactly zero, effectively performing feature selection.\n",
    "    When to Use:\n",
    "        If you suspect or know that only a subset of features is important, L1 can help in identifying them.\n",
    "        Useful in models with high dimensionality (more features than samples).\n",
    "    Adjustment Strategy:\n",
    "        Increase the L1 penalty (lambda) if the model is overfitting (i.e., performing well on training data but poorly on validation data).\n",
    "        Decrease if the model is underfitting or too many features are being zeroed out, causing loss of important information.\n",
    "\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "    Characteristics: L2 regularization adds a penalty equal to the square of the magnitude of coefficients. It doesnt encourage sparse models but distributes the error among all the features.\n",
    "    When to Use:\n",
    "        Generally useful in most scenarios where overfitting is a concern.\n",
    "        Particularly effective in neural networks to prevent weights from becoming too large, leading to overfitting.\n",
    "    Adjustment Strategy:\n",
    "        Increase the L2 penalty (weight decay) if the model is overfitting.\n",
    "        Decrease if the model is underfitting or if training doesnt converge.\n",
    "\n",
    "Practical Tips for Manipulating L1/L2:\n",
    "\n",
    "    Start with L2: L2 regularization is usually the first go-to method as it's less aggressive in terms of feature reduction and generally improves generalization.\n",
    "    Combining L1 and L2: Elastic Net regularization combines both L1 and L2 penalties. It can be a good middle ground if youre unsure which to choose.\n",
    "    Scale of Regularization Parameters: Start with smaller values (e.g., 0.01, 0.001) and increase gradually. Very high values can overly constrain the model, leading to underfitting.\n",
    "    Monitoring Performance: Regularly monitor training and validation loss. If validation loss decreases but training loss increases, it could indicate over-regularization.\n",
    "    Automated Hyperparameter Tuning: Consider using techniques like grid search, random search, or Bayesian optimization for systematic tuning of these parameters.\n",
    "    Regularization and Learning Rate: When adjusting regularization parameters, keep an eye on the learning rate. A higher learning rate may require stronger regularization to combat overfitting.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "The decision to adjust L1 or L2 regularization depends largely on the specific characteristics of your problem and dataset. Regularization is a balancing act  too little might lead to overfitting, too much to underfitting. Careful experimentation and monitoring are key to finding the right levels of L1 and L2 regularization for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 REG more details\n",
    "\n",
    "L1 regularization, also known as Lasso regularization, encourages sparsity in the parameter weights of the model. This means it not only penalizes the magnitude of the weights to prevent overfitting but also can drive some of those weights to zero, effectively \"unlearning\" or removing some features from the model.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "    The L1 penalty is the sum of the absolute values of the weights.\n",
    "    When applied during training, it can cause some weights to shrink to zero, especially for less important or redundant features.\n",
    "    This can be particularly useful when you have a high-dimensional dataset with many features that may not all be relevant to the prediction task.\n",
    "\n",
    "The consequence of this \"unlearning\" is that L1 regularization can be seen as a form of automatic feature selection. By driving certain weights to zero, the model ends up using fewer features, which can make the model simpler, faster, and more interpretable. However, this also means that L1 regularization can lead to a biased model if the penalty is too strong, as it might remove features that are actually important.\n",
    "\n",
    "The key with L1 (and all forms of regularization) is to find the right balance between fitting the training data and maintaining a model that can generalize well to unseen data. This balance is usually found through hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(r'C:\\Machine Learning\\SPY VIX 9 Input Output Model\\Saved\\MegaNumMuncher2_Checkpoint-current.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddDataToTensorboard(avg_tpb, avg_loss, avg_mse, avg_mae, avg_rmse, avg_r2, model, global_step):\n",
    "    \"\"\"\n",
    "    This method will populate the tensorboard with variables we want to watch.\n",
    "    :param writer: The SummaryWriter object from TensorBoard.\n",
    "    :param avg_tpb: Average time per batch.\n",
    "    :param avg_loss: Average loss.\n",
    "    :param avg_mse: Average mean squared error.\n",
    "    :param avg_mae: Average mean absolute error.\n",
    "    :param avg_rmse: Average root mean squared error.\n",
    "    :param avg_r2: Average R-squared value.\n",
    "    :param model: The PyTorch model from which to log parameters and gradients.\n",
    "    :param global_step: The global step value to track progress in the logs.\n",
    "    \"\"\"\n",
    "    writer.add_scalar('Average Time Per Batch', avg_tpb, global_step)\n",
    "    writer.add_scalar('Average Loss', avg_loss, global_step)\n",
    "    writer.add_scalar('Average MSE', avg_mse, global_step)\n",
    "    writer.add_scalar('Average MAE', avg_mae, global_step)\n",
    "    writer.add_scalar('Average RMSE', avg_rmse, global_step)\n",
    "    writer.add_scalar('Average R2', avg_r2, global_step)\n",
    "    \n",
    "    # Make sure the model's parameters have gradients before trying to log them\n",
    "    for tag, value in model.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        if value.grad is not None:\n",
    "            writer.add_histogram(tag, value.data.cpu().numpy(), global_step)\n",
    "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), global_step)\n",
    "\n",
    "# Function to calculate and log feature metrics\n",
    "def calculate_and_log_feature_metrics(y_true, y_pred, feature_names, global_step):\n",
    "    \"\"\"\n",
    "    Calculate and log metrics for individual features.\n",
    "    \"\"\"\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_true = y_true[:, i]\n",
    "        feature_pred = y_pred[:, i]\n",
    "        \n",
    "        mse = mean_squared_error(feature_true, feature_pred)\n",
    "        mae = mean_absolute_error(feature_true, feature_pred)\n",
    "        r2 = r2_score(feature_true, feature_pred)\n",
    "        rmse = np.sqrt(mse)  # Calculate RMSE\n",
    "\n",
    "        # Log the metrics for the current feature\n",
    "        writer.add_scalar(f'MSE/{feature_name}', mse, global_step)\n",
    "        writer.add_scalar(f'MAE/{feature_name}', mae, global_step)\n",
    "        writer.add_scalar(f'R2/{feature_name}', r2, global_step)\n",
    "        writer.add_scalar(f'RMSE/{feature_name}', rmse, global_step)\n",
    "\n",
    "def linear_interpolate(x, x_min, x_max, y_min, y_max):\n",
    "    \"\"\" Linearly interpolates a value 'x' in range [x_min, x_max] to range [y_min, y_max] \"\"\"\n",
    "    return y_min + (y_max - y_min) * ((x - x_min) / (x_max - x_min))\n",
    "\n",
    "def FindOptimalTraining(x_train_tensor):\n",
    "    \"\"\"\n",
    "    Takes the x_train_tensor and checks the row count to modify batch size and learning rate based on the iteration to optimize work.\n",
    "    \"\"\"\n",
    "    row_count = x_train_tensor.size()[0]\n",
    "\n",
    "    # Ensure row_count is within expected bounds\n",
    "    row_count = max(100, min(5000, row_count))\n",
    "\n",
    "    # # Linearly interpolate values TURNED OFF TO TEST\n",
    "    # batch_size = int(linear_interpolate(row_count, 100, 5000, 16, 64))\n",
    "    # learning_rate = linear_interpolate(row_count, 100, 5000, 0.0015, 0.00270)\n",
    "    # #weight_decay = 0.0000003333 # Was 0.0000005806 changed 9:58 am 2024-01-07 #It's not uncommon to try values like 0.01, 0.001, or 0.0001 for weight decay          \n",
    "    # weight_decay = linear_interpolate(row_count, 100, 5000, 0.000000450, 0.000000975)\n",
    "    \n",
    "    \n",
    "        # Linearly interpolate values\n",
    "    batch_size = 32 #int(linear_interpolate(row_count, 100, 5000, 16, 64))\n",
    "    learning_rate = 0.00150 #linear_interpolate(row_count, 100, 5000, 0.0015, 0.00270)\n",
    "    #weight_decay = 0.0000003333 # Was 0.0000005806 changed 9:58 am 2024-01-07 #It's not uncommon to try values like 0.01, 0.001, or 0.0001 for weight decay          \n",
    "    weight_decay = 0.0000005000 #linear_interpolate(row_count, 100, 5000, 0.000000450, 0.000000975)\n",
    "    \n",
    "#WEIGHT DECAY , 0.0000005000 , learning rate 00.0025 to stabalize bath 32\n",
    "    # Ensure batch size does not exceed the dataset size\n",
    "    batch_size = min(batch_size, row_count)\n",
    "    \n",
    "    print(f\"Learning Rate: {learning_rate:.6f}, Batch Size: {batch_size}, Weight Decay: {weight_decay:.10f} , X-Tensor Shape: {x_train_tensor.shape}\")\n",
    "    return (learning_rate, batch_size, weight_decay)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filepath='model_checkpoint.pth'):\n",
    "    \"\"\"\n",
    "    Save a model checkpoint.\n",
    "\n",
    "    :param model: PyTorch model to save.\n",
    "    :param optimizer: Optimizer whose state we want to save.\n",
    "    :param epoch: Current epoch number.\n",
    "    :param loss: Current loss value.\n",
    "    :param filepath: Path to save the checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss if loss is not None else 'N/A',  # Optionally store the loss value\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved to {filepath}\")\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    Load a model checkpoint.\n",
    "\n",
    "    :param filepath: Path to the checkpoint file.\n",
    "    :param model: PyTorch model to load the parameters into.\n",
    "    :param optimizer: Optimizer to load the state into.\n",
    "    :param device: The device to load the model onto.\n",
    "    :return: The checkpoint epoch and loss.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint.get('loss', None)  # Retrieve the loss if available, else None\n",
    "    print(f\"Checkpoint loaded from {filepath} at epoch {epoch} with loss {loss}\")\n",
    "    return epoch, loss\n",
    "\n",
    "def save_model(model, filepath='model.pth'):\n",
    "    \"\"\"\n",
    "    Save a model's state_dict.\n",
    "\n",
    "    :param model: PyTorch model to save.\n",
    "    :param filepath: Path to save the model.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath):\n",
    "    \"\"\"\n",
    "    Load a model's state_dict from a given filepath.\n",
    "\n",
    "    :param filepath: Path to the model file.\n",
    "    :return: Loaded PyTorch model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define the number of features and prediction length\n",
    "    num_features = 24  # Number of features in the dataset\n",
    "    prediction_length = 30  # Number of days you want to predict\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = HybridLSTMGRUAttentionWithCNN(input_size=24, \n",
    "                        hidden_layer_size=600, \n",
    "                        num_layers=26, \n",
    "                        output_size=prediction_length * num_features,\n",
    "                        dropout_rate= 0.0,\n",
    "                        num_heads= 8, kernel_size= 8).to(device)  # output_size is 30 days * 9 features\n",
    "    \n",
    "    try:\n",
    "        # Load the saved model state into the model\n",
    "        model_state = torch.load(filepath, map_location=device)\n",
    "        model.load_state_dict(model_state['model_state_dict'])  # Ensure this matches the key used when saving\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "    except KeyError:\n",
    "        print(\"Error: Incompatible state dict keys. Please check the model architecture and the state dict.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_data_for_prediction(normalized_data, look_back):\n",
    "    \"\"\"\n",
    "    Prepare data for prediction based on the look_back period.\n",
    "    :param normalized_data: Normalized data (2D array where rows are timesteps and columns are features).\n",
    "    :param look_back: Number of timesteps to look back for making the prediction.\n",
    "    :return: Data prepared for prediction.\n",
    "    \"\"\"\n",
    "    # Use the last 'look_back' timesteps to make the prediction\n",
    "    return normalized_data[-look_back:]\n",
    "\n",
    "def preprocess_input(input_data, fitted_standard_scaler, fitted_min_max_scaler):\n",
    "    \"\"\"\n",
    "    Standardize and normalize new input data using the already fitted scalers from the training data.\n",
    "    :param input_data: New data to be preprocessed (2D array where rows are timesteps and columns are features).\n",
    "    :param fitted_standard_scaler: The StandardScaler instance fitted on the training data.\n",
    "    :param fitted_min_max_scaler: The MinMaxScaler instance fitted on the training data.\n",
    "    :return: Preprocessed data.\n",
    "    \"\"\"\n",
    "    standardized_data = fitted_standard_scaler.transform(input_data)\n",
    "    normalized_data = fitted_min_max_scaler.transform(standardized_data)\n",
    "    return normalized_data\n",
    "\n",
    "# Example usage:\n",
    "# new_raw_data = ...  # New data to be predicted (should be a 2D array)\n",
    "# preprocessed_data = preprocess_input(new_raw_data, standard_scaler, min_max_scaler)\n",
    "\n",
    "def predict(model, prepared_data, device):\n",
    "    \"\"\"\n",
    "    Make a prediction based on prepared input data.\n",
    "    :param model: Trained LSTM model.\n",
    "    :param prepared_data: Prepared data for prediction (should be a 2D array of shape (look_back, number_of_features)).\n",
    "    :param device: The device (CPU or CUDA) for running the prediction.\n",
    "    :return: Predicted values for future sequence.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        if not isinstance(prepared_data, np.ndarray):\n",
    "            raise ValueError(\"prepared_data should be a numpy array\")\n",
    "        if prepared_data.shape[1] != 30:  # Ensure the second dimension matches the number of features\n",
    "            raise ValueError(\"prepared_data shape second dimension must be 9\")\n",
    "        \n",
    "        # Convert prepared data to a PyTorch tensor and add a batch dimension\n",
    "        input_tensor = torch.from_numpy(prepared_data).float().to(device)\n",
    "        input_tensor = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "        # Get the prediction from the model\n",
    "        prediction = model(input_tensor)\n",
    "\n",
    "        # Move the prediction back to the CPU if it was on CUDA\n",
    "        prediction = prediction.cpu()\n",
    "\n",
    "        # Reshape the output to remove batch dimension and get the actual prediction values\n",
    "        prediction = prediction.view(-1, 30)  # Reshape to match your output format (e.g., (30, 9))\n",
    "        return prediction.numpy()  # Convert to numpy array for easier handling\n",
    "    \n",
    "def revert_scaling(predictions, min_max_scaler, standard_scaler, feature_columns):\n",
    "    \"\"\"\n",
    "    Revert the scaling of the predictions to their original scale.\n",
    "\n",
    "    :param predictions: The predictions from the model, assumed to be a numpy array.\n",
    "    :param min_max_scaler: The MinMaxScaler instance used for scaling the data.\n",
    "    :param standard_scaler: The StandardScaler instance used for scaling the data.\n",
    "    :param feature_columns: List of feature names corresponding to the columns.\n",
    "    :return: DataFrame with predictions reverted to original scale.\n",
    "    \"\"\"\n",
    "    # Inverse MinMax Scaling\n",
    "    predictions = min_max_scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Inverse Standard Scaling\n",
    "    predictions = standard_scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions, columns=feature_columns)\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have the fitted min_max_scaler, standard_scaler, and the list of feature columns\n",
    "# predictions = ... # Output from the predict function\n",
    "# reverted_predictions_df = revert_scaling(predictions, min_max_scaler, standard_scaler, feature_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   l1_lambda = 0.0000875 AS OF 5:40 PM\n",
    "#    batch_size = int(linear_interpolate(row_count, 100, 5000, 12, 64))\n",
    "#    learning_rate = linear_interpolate(row_count, 100, 5000, 0.00135, 0.00350)\n",
    "#   #weight_decay = 0.0000003333 # Was 0.0000005806 changed 9:58 am 2024-01-07 #It's not uncommon to try values like 0.01, 0.001, or 0.0001 for weight decay          \n",
    "#   weight_decay = linear_interpolate(row_count, 100, 5000, 0.000000175, 0.0000022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2\n",
    "\n",
    "While it's true that Lasso regression utilizing L1 penalty tends to produce sparse coefficient vectors, encouraging feature selection and promoting simpler models, I wouldn't necessarily associate L1 with \"flattening out\" the weights. Rather, L1 encourages shrinkage of coefficients toward zero, effectively eliminating irrelevant or weak features altogether.\n",
    "\n",
    "On the other hand, Ridge Regression with L2 penalty preserves nonzero coefficients but distributes strength uniformly across them, thereby discouraging extreme values and promoting cooperation amongst correlated features. Although L2 does not enforce explicit feature removal like L1, it indirectly leads to a degree of generalization by attenuating the influence of less important features.\n",
    "\n",
    "Summarizing:\n",
    "\n",
    "    # L1 Penalty (Lasso Regression): Encourages sparsity by driving some coefficients to exact zeros, hence contributing to feature selection and producing simple models.\n",
    "    # L2 Penalty (Ridge Regression): Promotes equal distribution of importance among correlated features, preventing dominance by any single feature and ultimately facilitating moderate generalization.\n",
    "\n",
    "Both penalties play vital roles in controlling model complexity, addressing collinearity concerns, and enhancing interpretability. Leveraging either or a blend of both depending on the context propels us toward our ultimate goal of acquiring meaningful insights and generating accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is PFG\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([3928, 60, 24])\n",
      "Iteration: 0, Epoch: 0, Avg Time per Batch: 0.2401 sec, Loss: 39.01580957, MSE: 0.0981, MAE: 0.2308, RMSE: 0.3132, R2: -1.9055\n",
      "Iteration: 0, Epoch: 1, Avg Time per Batch: 0.2373 sec, Loss: 21.31200872, MSE: 0.0716, MAE: 0.1925, RMSE: 0.2677, R2: -1.1044\n",
      "Iteration: 0, Epoch: 2, Avg Time per Batch: 0.2381 sec, Loss: 15.39796447, MSE: 0.0611, MAE: 0.1776, RMSE: 0.2472, R2: -0.7826\n",
      "Iteration: 0, Epoch: 3, Avg Time per Batch: 0.2347 sec, Loss: 12.43427461, MSE: 0.0556, MAE: 0.1699, RMSE: 0.2358, R2: -0.6151\n",
      "Iteration: 0, Epoch: 4, Avg Time per Batch: 0.2380 sec, Loss: 10.65137999, MSE: 0.0523, MAE: 0.1652, RMSE: 0.2286, R2: -0.5112\n",
      "Iteration: 0, Epoch: 5, Avg Time per Batch: 0.2376 sec, Loss: 9.45892057, MSE: 0.0500, MAE: 0.1621, RMSE: 0.2236, R2: -0.4402\n",
      "Iteration: 0, Epoch: 6, Avg Time per Batch: 0.2376 sec, Loss: 8.60643928, MSE: 0.0483, MAE: 0.1599, RMSE: 0.2199, R2: -0.3875\n",
      "Iteration: 0, Epoch: 7, Avg Time per Batch: 0.2387 sec, Loss: 7.96697979, MSE: 0.0471, MAE: 0.1582, RMSE: 0.2170, R2: -0.3470\n",
      "Iteration: 0, Epoch: 8, Avg Time per Batch: 0.2356 sec, Loss: 7.46913731, MSE: 0.0461, MAE: 0.1570, RMSE: 0.2147, R2: -0.3149\n",
      "Iteration: 0, Epoch: 9, Avg Time per Batch: 0.2540 sec, Loss: 7.07066264, MSE: 0.0453, MAE: 0.1559, RMSE: 0.2129, R2: -0.2889\n",
      "Iteration: 0, Epoch: 10, Avg Time per Batch: 0.2491 sec, Loss: 6.74473184, MSE: 0.0446, MAE: 0.1551, RMSE: 0.2113, R2: -0.2675\n",
      "Iteration: 0, Epoch: 11, Avg Time per Batch: 0.2403 sec, Loss: 6.47287242, MSE: 0.0441, MAE: 0.1544, RMSE: 0.2100, R2: -0.2494\n",
      "Iteration: 0, Epoch: 12, Avg Time per Batch: 0.2686 sec, Loss: 6.24259556, MSE: 0.0436, MAE: 0.1538, RMSE: 0.2089, R2: -0.2341\n",
      "Iteration: 0, Epoch: 13, Avg Time per Batch: 0.2368 sec, Loss: 6.04533673, MSE: 0.0432, MAE: 0.1534, RMSE: 0.2079, R2: -0.2208\n",
      "Iteration: 0, Epoch: 14, Avg Time per Batch: 0.2644 sec, Loss: 5.87421208, MSE: 0.0429, MAE: 0.1530, RMSE: 0.2070, R2: -0.2091\n",
      "Iteration: 0, Epoch: 15, Avg Time per Batch: 0.2704 sec, Loss: 5.72440159, MSE: 0.0426, MAE: 0.1526, RMSE: 0.2063, R2: -0.1986\n",
      "Iteration: 0, Epoch: 16, Avg Time per Batch: 0.2699 sec, Loss: 5.59218735, MSE: 0.0423, MAE: 0.1523, RMSE: 0.2056, R2: -0.1894\n",
      "Iteration: 0, Epoch: 17, Avg Time per Batch: 0.2736 sec, Loss: 5.47463003, MSE: 0.0420, MAE: 0.1520, RMSE: 0.2050, R2: -0.1812\n",
      "Iteration: 0, Epoch: 18, Avg Time per Batch: 0.2712 sec, Loss: 5.36948254, MSE: 0.0418, MAE: 0.1517, RMSE: 0.2045, R2: -0.1740\n",
      "Iteration: 0, Epoch: 19, Avg Time per Batch: 0.2709 sec, Loss: 5.27486519, MSE: 0.0416, MAE: 0.1515, RMSE: 0.2040, R2: -0.1674\n",
      "Epoch 19 completed in 938.96 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 0, Epoch: 19, Loss: 5.2748651920\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is JLL\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5001, 60, 24])\n",
      "Iteration: 1, Epoch: 0, Avg Time per Batch: 0.2686 sec, Loss: 3.48201380, MSE: 0.0450, MAE: 0.1616, RMSE: 0.2122, R2: -0.0443\n",
      "Iteration: 1, Epoch: 1, Avg Time per Batch: 0.2695 sec, Loss: 3.48161691, MSE: 0.0450, MAE: 0.1615, RMSE: 0.2120, R2: -0.0446\n",
      "Iteration: 1, Epoch: 2, Avg Time per Batch: 0.2697 sec, Loss: 3.48153075, MSE: 0.0450, MAE: 0.1615, RMSE: 0.2121, R2: -0.0444\n",
      "Iteration: 1, Epoch: 3, Avg Time per Batch: 0.2708 sec, Loss: 3.48190100, MSE: 0.0449, MAE: 0.1614, RMSE: 0.2120, R2: -0.0443\n",
      "Iteration: 1, Epoch: 4, Avg Time per Batch: 0.2689 sec, Loss: 3.48165576, MSE: 0.0449, MAE: 0.1614, RMSE: 0.2120, R2: -0.0432\n",
      "Iteration: 1, Epoch: 5, Avg Time per Batch: 0.2690 sec, Loss: 3.48147034, MSE: 0.0449, MAE: 0.1614, RMSE: 0.2120, R2: -0.0429\n",
      "Iteration: 1, Epoch: 6, Avg Time per Batch: 0.2727 sec, Loss: 3.48130580, MSE: 0.0449, MAE: 0.1614, RMSE: 0.2120, R2: -0.0424\n",
      "Iteration: 1, Epoch: 7, Avg Time per Batch: 0.2697 sec, Loss: 3.48116647, MSE: 0.0449, MAE: 0.1614, RMSE: 0.2120, R2: -0.0421\n",
      "Iteration: 1, Epoch: 8, Avg Time per Batch: 0.2703 sec, Loss: 3.48097206, MSE: 0.0449, MAE: 0.1614, RMSE: 0.2120, R2: -0.0427\n",
      "Iteration: 1, Epoch: 9, Avg Time per Batch: 0.2693 sec, Loss: 3.48089792, MSE: 0.0449, MAE: 0.1614, RMSE: 0.2120, R2: -0.0425\n",
      "Iteration: 1, Epoch: 10, Avg Time per Batch: 0.2710 sec, Loss: 3.48076976, MSE: 0.0449, MAE: 0.1614, RMSE: 0.2120, R2: -0.0422\n",
      "Iteration: 1, Epoch: 11, Avg Time per Batch: 0.2696 sec, Loss: 3.48068643, MSE: 0.0449, MAE: 0.1614, RMSE: 0.2119, R2: -0.0419\n",
      "Iteration: 1, Epoch: 12, Avg Time per Batch: 0.2689 sec, Loss: 3.48049352, MSE: 0.0449, MAE: 0.1614, RMSE: 0.2120, R2: -0.0420\n",
      "Iteration: 1, Epoch: 13, Avg Time per Batch: 0.2713 sec, Loss: 3.48031502, MSE: 0.0449, MAE: 0.1614, RMSE: 0.2119, R2: -0.0415\n",
      "Iteration: 1, Epoch: 14, Avg Time per Batch: 0.2704 sec, Loss: 3.48018458, MSE: 0.0449, MAE: 0.1615, RMSE: 0.2119, R2: -0.0415\n",
      "Iteration: 1, Epoch: 15, Avg Time per Batch: 0.2680 sec, Loss: 3.48007561, MSE: 0.0449, MAE: 0.1615, RMSE: 0.2119, R2: -0.0416\n",
      "Iteration: 1, Epoch: 16, Avg Time per Batch: 0.2687 sec, Loss: 3.48002646, MSE: 0.0449, MAE: 0.1615, RMSE: 0.2119, R2: -0.0415\n",
      "Iteration: 1, Epoch: 17, Avg Time per Batch: 0.2680 sec, Loss: 3.47983009, MSE: 0.0449, MAE: 0.1615, RMSE: 0.2119, R2: -0.0415\n",
      "Iteration: 1, Epoch: 18, Avg Time per Batch: 0.2695 sec, Loss: 3.47970973, MSE: 0.0449, MAE: 0.1615, RMSE: 0.2119, R2: -0.0414\n",
      "Iteration: 1, Epoch: 19, Avg Time per Batch: 0.2671 sec, Loss: 3.47958805, MSE: 0.0449, MAE: 0.1615, RMSE: 0.2119, R2: -0.0416\n",
      "Epoch 19 completed in 1278.25 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 1, Epoch: 19, Loss: 3.4795880544\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is ARAY\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([2596, 60, 24])\n",
      "Iteration: 2, Epoch: 0, Avg Time per Batch: 0.2707 sec, Loss: 3.46435369, MSE: 0.0312, MAE: 0.1225, RMSE: 0.1767, R2: -0.1255\n",
      "Iteration: 2, Epoch: 1, Avg Time per Batch: 0.2693 sec, Loss: 3.46304610, MSE: 0.0308, MAE: 0.1207, RMSE: 0.1756, R2: -0.0834\n",
      "Iteration: 2, Epoch: 2, Avg Time per Batch: 0.2718 sec, Loss: 3.46250445, MSE: 0.0307, MAE: 0.1201, RMSE: 0.1753, R2: -0.0692\n",
      "Iteration: 2, Epoch: 3, Avg Time per Batch: 0.2702 sec, Loss: 3.46235750, MSE: 0.0306, MAE: 0.1198, RMSE: 0.1751, R2: -0.0627\n",
      "Iteration: 2, Epoch: 4, Avg Time per Batch: 0.2689 sec, Loss: 3.46166731, MSE: 0.0306, MAE: 0.1196, RMSE: 0.1749, R2: -0.0593\n",
      "Iteration: 2, Epoch: 5, Avg Time per Batch: 0.2713 sec, Loss: 3.46127094, MSE: 0.0306, MAE: 0.1195, RMSE: 0.1748, R2: -0.0561\n",
      "Iteration: 2, Epoch: 6, Avg Time per Batch: 0.2712 sec, Loss: 3.46158716, MSE: 0.0306, MAE: 0.1194, RMSE: 0.1748, R2: -0.0540\n",
      "Iteration: 2, Epoch: 7, Avg Time per Batch: 0.2706 sec, Loss: 3.46138089, MSE: 0.0306, MAE: 0.1194, RMSE: 0.1748, R2: -0.0531\n",
      "Iteration: 2, Epoch: 8, Avg Time per Batch: 0.2700 sec, Loss: 3.46141712, MSE: 0.0306, MAE: 0.1194, RMSE: 0.1748, R2: -0.0525\n",
      "Iteration: 2, Epoch: 9, Avg Time per Batch: 0.2718 sec, Loss: 3.46150414, MSE: 0.0306, MAE: 0.1194, RMSE: 0.1748, R2: -0.0522\n",
      "Iteration: 2, Epoch: 10, Avg Time per Batch: 0.2690 sec, Loss: 3.46134171, MSE: 0.0305, MAE: 0.1193, RMSE: 0.1748, R2: -0.0519\n",
      "Iteration: 2, Epoch: 11, Avg Time per Batch: 0.2695 sec, Loss: 3.46135078, MSE: 0.0305, MAE: 0.1193, RMSE: 0.1748, R2: -0.0514\n",
      "Iteration: 2, Epoch: 12, Avg Time per Batch: 0.2718 sec, Loss: 3.46127302, MSE: 0.0305, MAE: 0.1193, RMSE: 0.1747, R2: -0.0505\n",
      "Iteration: 2, Epoch: 13, Avg Time per Batch: 0.2688 sec, Loss: 3.46108867, MSE: 0.0305, MAE: 0.1193, RMSE: 0.1747, R2: -0.0502\n",
      "Iteration: 2, Epoch: 14, Avg Time per Batch: 0.2709 sec, Loss: 3.46091881, MSE: 0.0305, MAE: 0.1193, RMSE: 0.1747, R2: -0.0497\n",
      "Iteration: 2, Epoch: 15, Avg Time per Batch: 0.2706 sec, Loss: 3.46103022, MSE: 0.0305, MAE: 0.1192, RMSE: 0.1747, R2: -0.0496\n",
      "Iteration: 2, Epoch: 16, Avg Time per Batch: 0.2724 sec, Loss: 3.46094005, MSE: 0.0305, MAE: 0.1192, RMSE: 0.1747, R2: -0.0492\n",
      "Iteration: 2, Epoch: 17, Avg Time per Batch: 0.2700 sec, Loss: 3.46088973, MSE: 0.0305, MAE: 0.1192, RMSE: 0.1747, R2: -0.0491\n",
      "Iteration: 2, Epoch: 18, Avg Time per Batch: 0.2677 sec, Loss: 3.46081501, MSE: 0.0305, MAE: 0.1192, RMSE: 0.1747, R2: -0.0487\n",
      "Iteration: 2, Epoch: 19, Avg Time per Batch: 0.2700 sec, Loss: 3.46067320, MSE: 0.0305, MAE: 0.1192, RMSE: 0.1747, R2: -0.0491\n",
      "Epoch 19 completed in 816.39 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 2, Epoch: 19, Loss: 3.4606731969\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is AORT\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5895, 60, 24])\n",
      "Iteration: 3, Epoch: 0, Avg Time per Batch: 0.2695 sec, Loss: 3.46200167, MSE: 0.0317, MAE: 0.1238, RMSE: 0.1781, R2: -0.0639\n",
      "Iteration: 3, Epoch: 1, Avg Time per Batch: 0.2684 sec, Loss: 3.46170228, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1780, R2: -0.0575\n",
      "Iteration: 3, Epoch: 2, Avg Time per Batch: 0.2686 sec, Loss: 3.46100220, MSE: 0.0317, MAE: 0.1236, RMSE: 0.1779, R2: -0.0535\n",
      "Iteration: 3, Epoch: 3, Avg Time per Batch: 0.2690 sec, Loss: 3.46088970, MSE: 0.0317, MAE: 0.1236, RMSE: 0.1779, R2: -0.0522\n",
      "Iteration: 3, Epoch: 4, Avg Time per Batch: 0.2673 sec, Loss: 3.46088009, MSE: 0.0317, MAE: 0.1236, RMSE: 0.1779, R2: -0.0510\n",
      "Iteration: 3, Epoch: 5, Avg Time per Batch: 0.2687 sec, Loss: 3.46052263, MSE: 0.0317, MAE: 0.1236, RMSE: 0.1779, R2: -0.0508\n",
      "Iteration: 3, Epoch: 6, Avg Time per Batch: 0.2681 sec, Loss: 3.46050804, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0502\n",
      "Iteration: 3, Epoch: 7, Avg Time per Batch: 0.2694 sec, Loss: 3.46038018, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0498\n",
      "Iteration: 3, Epoch: 8, Avg Time per Batch: 0.2681 sec, Loss: 3.46019403, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0497\n",
      "Iteration: 3, Epoch: 9, Avg Time per Batch: 0.2692 sec, Loss: 3.46012666, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0498\n",
      "Iteration: 3, Epoch: 10, Avg Time per Batch: 0.2681 sec, Loss: 3.45997499, MSE: 0.0316, MAE: 0.1237, RMSE: 0.1779, R2: -0.0500\n",
      "Iteration: 3, Epoch: 11, Avg Time per Batch: 0.2674 sec, Loss: 3.45991742, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0502\n",
      "Iteration: 3, Epoch: 12, Avg Time per Batch: 0.2675 sec, Loss: 3.45978910, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0496\n",
      "Iteration: 3, Epoch: 13, Avg Time per Batch: 0.2669 sec, Loss: 3.45963714, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0497\n",
      "Iteration: 3, Epoch: 14, Avg Time per Batch: 0.2680 sec, Loss: 3.45950740, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0494\n",
      "Iteration: 3, Epoch: 15, Avg Time per Batch: 0.2678 sec, Loss: 3.45939038, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0497\n",
      "Iteration: 3, Epoch: 16, Avg Time per Batch: 0.2693 sec, Loss: 3.45926914, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0495\n",
      "Iteration: 3, Epoch: 17, Avg Time per Batch: 0.2680 sec, Loss: 3.45913305, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0494\n",
      "Iteration: 3, Epoch: 18, Avg Time per Batch: 0.2682 sec, Loss: 3.45905521, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0496\n",
      "Iteration: 3, Epoch: 19, Avg Time per Batch: 0.2666 sec, Loss: 3.45889822, MSE: 0.0317, MAE: 0.1237, RMSE: 0.1779, R2: -0.0495\n",
      "Epoch 19 completed in 1447.77 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 3, Epoch: 19, Loss: 3.4588982211\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is SLRX\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([589, 60, 24])\n",
      "Iteration: 4, Epoch: 0, Avg Time per Batch: 0.2816 sec, Loss: 3.48628737, MSE: 0.0500, MAE: 0.1766, RMSE: 0.2235, R2: -0.1402\n",
      "Iteration: 4, Epoch: 1, Avg Time per Batch: 0.2830 sec, Loss: 3.47448946, MSE: 0.0484, MAE: 0.1741, RMSE: 0.2200, R2: -0.0986\n",
      "Iteration: 4, Epoch: 2, Avg Time per Batch: 0.2865 sec, Loss: 3.47410602, MSE: 0.0480, MAE: 0.1735, RMSE: 0.2190, R2: -0.0830\n",
      "Iteration: 4, Epoch: 3, Avg Time per Batch: 0.2831 sec, Loss: 3.47542543, MSE: 0.0476, MAE: 0.1731, RMSE: 0.2183, R2: -0.0768\n",
      "Iteration: 4, Epoch: 4, Avg Time per Batch: 0.2842 sec, Loss: 3.47099317, MSE: 0.0475, MAE: 0.1730, RMSE: 0.2179, R2: -0.0725\n",
      "Iteration: 4, Epoch: 5, Avg Time per Batch: 0.2863 sec, Loss: 3.47404956, MSE: 0.0474, MAE: 0.1729, RMSE: 0.2176, R2: -0.0702\n",
      "Iteration: 4, Epoch: 6, Avg Time per Batch: 0.2870 sec, Loss: 3.47252108, MSE: 0.0472, MAE: 0.1727, RMSE: 0.2173, R2: -0.0668\n",
      "Iteration: 4, Epoch: 7, Avg Time per Batch: 0.2794 sec, Loss: 3.47343160, MSE: 0.0471, MAE: 0.1725, RMSE: 0.2171, R2: -0.0647\n",
      "Iteration: 4, Epoch: 8, Avg Time per Batch: 0.2756 sec, Loss: 3.47276476, MSE: 0.0471, MAE: 0.1725, RMSE: 0.2171, R2: -0.0637\n",
      "Iteration: 4, Epoch: 9, Avg Time per Batch: 0.2752 sec, Loss: 3.47134002, MSE: 0.0471, MAE: 0.1724, RMSE: 0.2169, R2: -0.0631\n",
      "Iteration: 4, Epoch: 10, Avg Time per Batch: 0.2910 sec, Loss: 3.47305758, MSE: 0.0471, MAE: 0.1725, RMSE: 0.2170, R2: -0.0624\n",
      "Iteration: 4, Epoch: 11, Avg Time per Batch: 0.2850 sec, Loss: 3.47169099, MSE: 0.0471, MAE: 0.1724, RMSE: 0.2169, R2: -0.0615\n",
      "Iteration: 4, Epoch: 12, Avg Time per Batch: 0.2825 sec, Loss: 3.47265926, MSE: 0.0470, MAE: 0.1725, RMSE: 0.2169, R2: -0.0603\n",
      "Iteration: 4, Epoch: 13, Avg Time per Batch: 0.2854 sec, Loss: 3.47284329, MSE: 0.0470, MAE: 0.1725, RMSE: 0.2169, R2: -0.0590\n",
      "Iteration: 4, Epoch: 14, Avg Time per Batch: 0.2770 sec, Loss: 3.47163939, MSE: 0.0470, MAE: 0.1724, RMSE: 0.2168, R2: -0.0591\n",
      "Iteration: 4, Epoch: 15, Avg Time per Batch: 0.2708 sec, Loss: 3.47266809, MSE: 0.0470, MAE: 0.1724, RMSE: 0.2168, R2: -0.0588\n",
      "Iteration: 4, Epoch: 16, Avg Time per Batch: 0.2783 sec, Loss: 3.47140115, MSE: 0.0470, MAE: 0.1724, RMSE: 0.2168, R2: -0.0587\n",
      "Iteration: 4, Epoch: 17, Avg Time per Batch: 0.2799 sec, Loss: 3.47222607, MSE: 0.0470, MAE: 0.1724, RMSE: 0.2168, R2: -0.0582\n",
      "Iteration: 4, Epoch: 18, Avg Time per Batch: 0.2826 sec, Loss: 3.47207580, MSE: 0.0470, MAE: 0.1724, RMSE: 0.2168, R2: -0.0578\n",
      "Iteration: 4, Epoch: 19, Avg Time per Batch: 0.2767 sec, Loss: 3.47206523, MSE: 0.0470, MAE: 0.1724, RMSE: 0.2168, R2: -0.0570\n",
      "Epoch 19 completed in 440.62 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 4, Epoch: 19, Loss: 3.4720652328\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is HLX\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5012, 60, 24])\n",
      "Iteration: 5, Epoch: 0, Avg Time per Batch: 0.2691 sec, Loss: 3.46717281, MSE: 0.0413, MAE: 0.1511, RMSE: 0.2032, R2: -0.0554\n",
      "Iteration: 5, Epoch: 1, Avg Time per Batch: 0.2649 sec, Loss: 3.46527913, MSE: 0.0412, MAE: 0.1508, RMSE: 0.2029, R2: -0.0491\n",
      "Iteration: 5, Epoch: 2, Avg Time per Batch: 0.2674 sec, Loss: 3.46550611, MSE: 0.0411, MAE: 0.1507, RMSE: 0.2028, R2: -0.0469\n",
      "Iteration: 5, Epoch: 3, Avg Time per Batch: 0.2687 sec, Loss: 3.46537544, MSE: 0.0411, MAE: 0.1506, RMSE: 0.2028, R2: -0.0457\n",
      "Iteration: 5, Epoch: 4, Avg Time per Batch: 0.2657 sec, Loss: 3.46557398, MSE: 0.0411, MAE: 0.1507, RMSE: 0.2028, R2: -0.0458\n",
      "Iteration: 5, Epoch: 5, Avg Time per Batch: 0.2691 sec, Loss: 3.46521838, MSE: 0.0411, MAE: 0.1506, RMSE: 0.2028, R2: -0.0448\n",
      "Iteration: 5, Epoch: 6, Avg Time per Batch: 0.2642 sec, Loss: 3.46507245, MSE: 0.0411, MAE: 0.1506, RMSE: 0.2027, R2: -0.0451\n",
      "Iteration: 5, Epoch: 7, Avg Time per Batch: 0.2679 sec, Loss: 3.46508050, MSE: 0.0411, MAE: 0.1506, RMSE: 0.2027, R2: -0.0443\n",
      "Iteration: 5, Epoch: 8, Avg Time per Batch: 0.2701 sec, Loss: 3.46482122, MSE: 0.0411, MAE: 0.1506, RMSE: 0.2027, R2: -0.0444\n",
      "Iteration: 5, Epoch: 9, Avg Time per Batch: 0.2689 sec, Loss: 3.46478624, MSE: 0.0411, MAE: 0.1506, RMSE: 0.2028, R2: -0.0444\n",
      "Iteration: 5, Epoch: 10, Avg Time per Batch: 0.2681 sec, Loss: 3.46472890, MSE: 0.0411, MAE: 0.1506, RMSE: 0.2028, R2: -0.0442\n",
      "Iteration: 5, Epoch: 11, Avg Time per Batch: 0.2667 sec, Loss: 3.46467077, MSE: 0.0411, MAE: 0.1506, RMSE: 0.2028, R2: -0.0444\n",
      "Iteration: 5, Epoch: 12, Avg Time per Batch: 0.2682 sec, Loss: 3.46443656, MSE: 0.0411, MAE: 0.1507, RMSE: 0.2028, R2: -0.0443\n",
      "Iteration: 5, Epoch: 13, Avg Time per Batch: 0.2669 sec, Loss: 3.46438711, MSE: 0.0411, MAE: 0.1507, RMSE: 0.2028, R2: -0.0444\n",
      "Iteration: 5, Epoch: 14, Avg Time per Batch: 0.2703 sec, Loss: 3.46421060, MSE: 0.0411, MAE: 0.1507, RMSE: 0.2028, R2: -0.0443\n",
      "Iteration: 5, Epoch: 15, Avg Time per Batch: 0.2685 sec, Loss: 3.46418327, MSE: 0.0411, MAE: 0.1507, RMSE: 0.2028, R2: -0.0442\n",
      "Iteration: 5, Epoch: 16, Avg Time per Batch: 0.2654 sec, Loss: 3.46406102, MSE: 0.0411, MAE: 0.1507, RMSE: 0.2028, R2: -0.0444\n",
      "Iteration: 5, Epoch: 17, Avg Time per Batch: 0.2635 sec, Loss: 3.46396508, MSE: 0.0411, MAE: 0.1507, RMSE: 0.2028, R2: -0.0444\n",
      "Iteration: 5, Epoch: 18, Avg Time per Batch: 0.2662 sec, Loss: 3.46380413, MSE: 0.0411, MAE: 0.1507, RMSE: 0.2028, R2: -0.0446\n",
      "Iteration: 5, Epoch: 19, Avg Time per Batch: 0.2700 sec, Loss: 3.46369743, MSE: 0.0411, MAE: 0.1507, RMSE: 0.2028, R2: -0.0444\n",
      "Epoch 19 completed in 1287.37 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 5, Epoch: 19, Loss: 3.4636974269\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is HOFT\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([3758, 60, 24])\n",
      "Iteration: 6, Epoch: 0, Avg Time per Batch: 0.2691 sec, Loss: 3.45588468, MSE: 0.0345, MAE: 0.1291, RMSE: 0.1857, R2: -0.0516\n",
      "Iteration: 6, Epoch: 1, Avg Time per Batch: 0.2701 sec, Loss: 3.45457295, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1857, R2: -0.0483\n",
      "Iteration: 6, Epoch: 2, Avg Time per Batch: 0.2672 sec, Loss: 3.45434063, MSE: 0.0345, MAE: 0.1288, RMSE: 0.1857, R2: -0.0487\n",
      "Iteration: 6, Epoch: 3, Avg Time per Batch: 0.2693 sec, Loss: 3.45459413, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1858, R2: -0.0477\n",
      "Iteration: 6, Epoch: 4, Avg Time per Batch: 0.2706 sec, Loss: 3.45459954, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1858, R2: -0.0488\n",
      "Iteration: 6, Epoch: 5, Avg Time per Batch: 0.2684 sec, Loss: 3.45447069, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1858, R2: -0.0489\n",
      "Iteration: 6, Epoch: 6, Avg Time per Batch: 0.2674 sec, Loss: 3.45414898, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1858, R2: -0.0489\n",
      "Iteration: 6, Epoch: 7, Avg Time per Batch: 0.2681 sec, Loss: 3.45390419, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1858, R2: -0.0493\n",
      "Iteration: 6, Epoch: 8, Avg Time per Batch: 0.2696 sec, Loss: 3.45409079, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1858, R2: -0.0501\n",
      "Iteration: 6, Epoch: 9, Avg Time per Batch: 0.2684 sec, Loss: 3.45404720, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1858, R2: -0.0494\n",
      "Iteration: 6, Epoch: 10, Avg Time per Batch: 0.2708 sec, Loss: 3.45396031, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1858, R2: -0.0495\n",
      "Iteration: 6, Epoch: 11, Avg Time per Batch: 0.2664 sec, Loss: 3.45372893, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1858, R2: -0.0495\n",
      "Iteration: 6, Epoch: 12, Avg Time per Batch: 0.2690 sec, Loss: 3.45377362, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1858, R2: -0.0497\n",
      "Iteration: 6, Epoch: 13, Avg Time per Batch: 0.2724 sec, Loss: 3.45369955, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1858, R2: -0.0503\n",
      "Iteration: 6, Epoch: 14, Avg Time per Batch: 0.2723 sec, Loss: 3.45352714, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1859, R2: -0.0504\n",
      "Iteration: 6, Epoch: 15, Avg Time per Batch: 0.2699 sec, Loss: 3.45348710, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1859, R2: -0.0507\n",
      "Iteration: 6, Epoch: 16, Avg Time per Batch: 0.2670 sec, Loss: 3.45338827, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1859, R2: -0.0507\n",
      "Iteration: 6, Epoch: 17, Avg Time per Batch: 0.2674 sec, Loss: 3.45333877, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1859, R2: -0.0505\n",
      "Iteration: 6, Epoch: 18, Avg Time per Batch: 0.2701 sec, Loss: 3.45336294, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1859, R2: -0.0501\n",
      "Iteration: 6, Epoch: 19, Avg Time per Batch: 0.2688 sec, Loss: 3.45319639, MSE: 0.0345, MAE: 0.1289, RMSE: 0.1859, R2: -0.0500\n",
      "Epoch 19 completed in 1051.08 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 6, Epoch: 19, Loss: 3.4531963931\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is ESBA\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([922, 60, 24])\n",
      "Iteration: 7, Epoch: 0, Avg Time per Batch: 0.2750 sec, Loss: 3.48133085, MSE: 0.0587, MAE: 0.1843, RMSE: 0.2423, R2: -0.2311\n",
      "Iteration: 7, Epoch: 1, Avg Time per Batch: 0.2652 sec, Loss: 3.47276515, MSE: 0.0538, MAE: 0.1774, RMSE: 0.2320, R2: -0.1432\n",
      "Iteration: 7, Epoch: 2, Avg Time per Batch: 0.2697 sec, Loss: 3.46930120, MSE: 0.0519, MAE: 0.1750, RMSE: 0.2279, R2: -0.1100\n",
      "Iteration: 7, Epoch: 3, Avg Time per Batch: 0.2767 sec, Loss: 3.46894660, MSE: 0.0510, MAE: 0.1739, RMSE: 0.2259, R2: -0.0932\n",
      "Iteration: 7, Epoch: 4, Avg Time per Batch: 0.2719 sec, Loss: 3.46964793, MSE: 0.0505, MAE: 0.1734, RMSE: 0.2248, R2: -0.0852\n",
      "Iteration: 7, Epoch: 5, Avg Time per Batch: 0.2829 sec, Loss: 3.46803413, MSE: 0.0502, MAE: 0.1729, RMSE: 0.2240, R2: -0.0768\n",
      "Iteration: 7, Epoch: 6, Avg Time per Batch: 0.2740 sec, Loss: 3.46867794, MSE: 0.0499, MAE: 0.1724, RMSE: 0.2233, R2: -0.0720\n",
      "Iteration: 7, Epoch: 7, Avg Time per Batch: 0.2675 sec, Loss: 3.46810850, MSE: 0.0496, MAE: 0.1722, RMSE: 0.2228, R2: -0.0687\n",
      "Iteration: 7, Epoch: 8, Avg Time per Batch: 0.2715 sec, Loss: 3.46780967, MSE: 0.0495, MAE: 0.1720, RMSE: 0.2225, R2: -0.0664\n",
      "Iteration: 7, Epoch: 9, Avg Time per Batch: 0.2696 sec, Loss: 3.46760628, MSE: 0.0494, MAE: 0.1718, RMSE: 0.2222, R2: -0.0649\n",
      "Iteration: 7, Epoch: 10, Avg Time per Batch: 0.2716 sec, Loss: 3.46694300, MSE: 0.0493, MAE: 0.1717, RMSE: 0.2220, R2: -0.0629\n",
      "Iteration: 7, Epoch: 11, Avg Time per Batch: 0.2759 sec, Loss: 3.46698924, MSE: 0.0492, MAE: 0.1716, RMSE: 0.2219, R2: -0.0609\n",
      "Iteration: 7, Epoch: 12, Avg Time per Batch: 0.2783 sec, Loss: 3.46714408, MSE: 0.0492, MAE: 0.1715, RMSE: 0.2217, R2: -0.0599\n",
      "Iteration: 7, Epoch: 13, Avg Time per Batch: 0.2732 sec, Loss: 3.46687911, MSE: 0.0491, MAE: 0.1714, RMSE: 0.2216, R2: -0.0593\n",
      "Iteration: 7, Epoch: 14, Avg Time per Batch: 0.2821 sec, Loss: 3.46704574, MSE: 0.0491, MAE: 0.1714, RMSE: 0.2216, R2: -0.0585\n",
      "Iteration: 7, Epoch: 15, Avg Time per Batch: 0.2746 sec, Loss: 3.46684059, MSE: 0.0490, MAE: 0.1713, RMSE: 0.2214, R2: -0.0573\n",
      "Iteration: 7, Epoch: 16, Avg Time per Batch: 0.2730 sec, Loss: 3.46689888, MSE: 0.0490, MAE: 0.1713, RMSE: 0.2213, R2: -0.0565\n",
      "Iteration: 7, Epoch: 17, Avg Time per Batch: 0.2731 sec, Loss: 3.46690569, MSE: 0.0489, MAE: 0.1712, RMSE: 0.2212, R2: -0.0558\n",
      "Iteration: 7, Epoch: 18, Avg Time per Batch: 0.2739 sec, Loss: 3.46656807, MSE: 0.0489, MAE: 0.1712, RMSE: 0.2211, R2: -0.0548\n",
      "Iteration: 7, Epoch: 19, Avg Time per Batch: 0.2756 sec, Loss: 3.46659073, MSE: 0.0489, MAE: 0.1711, RMSE: 0.2210, R2: -0.0540\n",
      "Epoch 19 completed in 498.23 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 7, Epoch: 19, Loss: 3.4665907255\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is TFIN\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([644, 60, 24])\n",
      "Iteration: 8, Epoch: 0, Avg Time per Batch: 0.2764 sec, Loss: 3.48219796, MSE: 0.0599, MAE: 0.1888, RMSE: 0.2447, R2: -0.0998\n",
      "Iteration: 8, Epoch: 1, Avg Time per Batch: 0.2766 sec, Loss: 3.47713989, MSE: 0.0585, MAE: 0.1863, RMSE: 0.2419, R2: -0.0761\n",
      "Iteration: 8, Epoch: 2, Avg Time per Batch: 0.2799 sec, Loss: 3.47757482, MSE: 0.0577, MAE: 0.1861, RMSE: 0.2403, R2: -0.0699\n",
      "Iteration: 8, Epoch: 3, Avg Time per Batch: 0.2818 sec, Loss: 3.47691016, MSE: 0.0573, MAE: 0.1860, RMSE: 0.2394, R2: -0.0614\n",
      "Iteration: 8, Epoch: 4, Avg Time per Batch: 0.2825 sec, Loss: 3.47582307, MSE: 0.0570, MAE: 0.1859, RMSE: 0.2388, R2: -0.0576\n",
      "Iteration: 8, Epoch: 5, Avg Time per Batch: 0.2789 sec, Loss: 3.47316837, MSE: 0.0568, MAE: 0.1855, RMSE: 0.2382, R2: -0.0565\n",
      "Iteration: 8, Epoch: 6, Avg Time per Batch: 0.2750 sec, Loss: 3.47384069, MSE: 0.0566, MAE: 0.1853, RMSE: 0.2379, R2: -0.0532\n",
      "Iteration: 8, Epoch: 7, Avg Time per Batch: 0.2845 sec, Loss: 3.47450270, MSE: 0.0564, MAE: 0.1852, RMSE: 0.2376, R2: -0.0523\n",
      "Iteration: 8, Epoch: 8, Avg Time per Batch: 0.2760 sec, Loss: 3.47317273, MSE: 0.0563, MAE: 0.1852, RMSE: 0.2374, R2: -0.0516\n",
      "Iteration: 8, Epoch: 9, Avg Time per Batch: 0.2863 sec, Loss: 3.47390001, MSE: 0.0563, MAE: 0.1851, RMSE: 0.2372, R2: -0.0499\n",
      "Iteration: 8, Epoch: 10, Avg Time per Batch: 0.2796 sec, Loss: 3.47333206, MSE: 0.0562, MAE: 0.1850, RMSE: 0.2370, R2: -0.0482\n",
      "Iteration: 8, Epoch: 11, Avg Time per Batch: 0.2781 sec, Loss: 3.47282310, MSE: 0.0561, MAE: 0.1850, RMSE: 0.2369, R2: -0.0471\n",
      "Iteration: 8, Epoch: 12, Avg Time per Batch: 0.2879 sec, Loss: 3.47345489, MSE: 0.0561, MAE: 0.1849, RMSE: 0.2368, R2: -0.0469\n",
      "Iteration: 8, Epoch: 13, Avg Time per Batch: 0.2821 sec, Loss: 3.47305698, MSE: 0.0560, MAE: 0.1849, RMSE: 0.2367, R2: -0.0462\n",
      "Iteration: 8, Epoch: 14, Avg Time per Batch: 0.2844 sec, Loss: 3.47277534, MSE: 0.0560, MAE: 0.1849, RMSE: 0.2367, R2: -0.0458\n",
      "Iteration: 8, Epoch: 15, Avg Time per Batch: 0.2832 sec, Loss: 3.47349142, MSE: 0.0560, MAE: 0.1849, RMSE: 0.2366, R2: -0.0459\n",
      "Iteration: 8, Epoch: 16, Avg Time per Batch: 0.2843 sec, Loss: 3.47306638, MSE: 0.0559, MAE: 0.1848, RMSE: 0.2365, R2: -0.0451\n",
      "Iteration: 8, Epoch: 17, Avg Time per Batch: 0.2837 sec, Loss: 3.47271778, MSE: 0.0559, MAE: 0.1848, RMSE: 0.2365, R2: -0.0449\n",
      "Iteration: 8, Epoch: 18, Avg Time per Batch: 0.2846 sec, Loss: 3.47314459, MSE: 0.0559, MAE: 0.1848, RMSE: 0.2365, R2: -0.0445\n",
      "Iteration: 8, Epoch: 19, Avg Time per Batch: 0.2799 sec, Loss: 3.47255841, MSE: 0.0559, MAE: 0.1848, RMSE: 0.2364, R2: -0.0444\n",
      "Epoch 19 completed in 447.83 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 8, Epoch: 19, Loss: 3.4725584105\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is MLSS\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5430, 60, 24])\n",
      "Iteration: 9, Epoch: 0, Avg Time per Batch: 0.2683 sec, Loss: 3.44568640, MSE: 0.0302, MAE: 0.1133, RMSE: 0.1738, R2: -0.5130\n",
      "Iteration: 9, Epoch: 1, Avg Time per Batch: 0.2683 sec, Loss: 3.44520513, MSE: 0.0294, MAE: 0.1108, RMSE: 0.1716, R2: -0.3128\n",
      "Iteration: 9, Epoch: 2, Avg Time per Batch: 0.2676 sec, Loss: 3.44464534, MSE: 0.0292, MAE: 0.1100, RMSE: 0.1708, R2: -0.2457\n",
      "Iteration: 9, Epoch: 3, Avg Time per Batch: 0.2689 sec, Loss: 3.44443541, MSE: 0.0290, MAE: 0.1096, RMSE: 0.1704, R2: -0.2117\n",
      "Iteration: 9, Epoch: 4, Avg Time per Batch: 0.2680 sec, Loss: 3.44418196, MSE: 0.0290, MAE: 0.1094, RMSE: 0.1702, R2: -0.1895\n",
      "Iteration: 9, Epoch: 5, Avg Time per Batch: 0.2670 sec, Loss: 3.44402314, MSE: 0.0289, MAE: 0.1093, RMSE: 0.1701, R2: -0.1744\n",
      "Iteration: 9, Epoch: 6, Avg Time per Batch: 0.2668 sec, Loss: 3.44390650, MSE: 0.0289, MAE: 0.1091, RMSE: 0.1700, R2: -0.1675\n",
      "Iteration: 9, Epoch: 7, Avg Time per Batch: 0.2668 sec, Loss: 3.44379875, MSE: 0.0289, MAE: 0.1091, RMSE: 0.1699, R2: -0.1627\n",
      "Iteration: 9, Epoch: 8, Avg Time per Batch: 0.2668 sec, Loss: 3.44374672, MSE: 0.0289, MAE: 0.1090, RMSE: 0.1699, R2: -0.1540\n",
      "Iteration: 9, Epoch: 9, Avg Time per Batch: 0.2655 sec, Loss: 3.44358704, MSE: 0.0289, MAE: 0.1090, RMSE: 0.1699, R2: -0.1455\n",
      "Iteration: 9, Epoch: 10, Avg Time per Batch: 0.2680 sec, Loss: 3.44350258, MSE: 0.0288, MAE: 0.1089, RMSE: 0.1698, R2: -0.1406\n",
      "Iteration: 9, Epoch: 11, Avg Time per Batch: 0.2685 sec, Loss: 3.44342259, MSE: 0.0288, MAE: 0.1089, RMSE: 0.1698, R2: -0.1377\n",
      "Iteration: 9, Epoch: 12, Avg Time per Batch: 0.2680 sec, Loss: 3.44329531, MSE: 0.0288, MAE: 0.1089, RMSE: 0.1698, R2: -0.1326\n",
      "Iteration: 9, Epoch: 13, Avg Time per Batch: 0.2673 sec, Loss: 3.44323320, MSE: 0.0288, MAE: 0.1089, RMSE: 0.1698, R2: -0.1286\n",
      "Iteration: 9, Epoch: 14, Avg Time per Batch: 0.2671 sec, Loss: 3.44311040, MSE: 0.0288, MAE: 0.1089, RMSE: 0.1698, R2: -0.1264\n",
      "Iteration: 9, Epoch: 15, Avg Time per Batch: 0.2672 sec, Loss: 3.44298458, MSE: 0.0288, MAE: 0.1088, RMSE: 0.1698, R2: -0.1227\n",
      "Iteration: 9, Epoch: 16, Avg Time per Batch: 0.2683 sec, Loss: 3.44290808, MSE: 0.0288, MAE: 0.1088, RMSE: 0.1697, R2: -0.1217\n",
      "Iteration: 9, Epoch: 17, Avg Time per Batch: 0.2674 sec, Loss: 3.44279374, MSE: 0.0288, MAE: 0.1088, RMSE: 0.1697, R2: -0.1208\n",
      "Iteration: 9, Epoch: 18, Avg Time per Batch: 0.2678 sec, Loss: 3.44271482, MSE: 0.0288, MAE: 0.1088, RMSE: 0.1697, R2: -0.1202\n",
      "Iteration: 9, Epoch: 19, Avg Time per Batch: 0.2676 sec, Loss: 3.44262787, MSE: 0.0288, MAE: 0.1088, RMSE: 0.1697, R2: -0.1187\n",
      "Epoch 19 completed in 1357.74 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 9, Epoch: 19, Loss: 3.4426278714\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is QMCO\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([4487, 60, 24])\n",
      "Iteration: 10, Epoch: 0, Avg Time per Batch: 0.2670 sec, Loss: 3.44850009, MSE: 0.0350, MAE: 0.1305, RMSE: 0.1871, R2: -0.0611\n",
      "Iteration: 10, Epoch: 1, Avg Time per Batch: 0.2663 sec, Loss: 3.44787698, MSE: 0.0350, MAE: 0.1303, RMSE: 0.1871, R2: -0.0510\n",
      "Iteration: 10, Epoch: 2, Avg Time per Batch: 0.2667 sec, Loss: 3.44723390, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1871, R2: -0.0519\n",
      "Iteration: 10, Epoch: 3, Avg Time per Batch: 0.2673 sec, Loss: 3.44733331, MSE: 0.0350, MAE: 0.1303, RMSE: 0.1872, R2: -0.0521\n",
      "Iteration: 10, Epoch: 4, Avg Time per Batch: 0.2670 sec, Loss: 3.44707771, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1871, R2: -0.0519\n",
      "Iteration: 10, Epoch: 5, Avg Time per Batch: 0.2676 sec, Loss: 3.44687306, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1871, R2: -0.0517\n",
      "Iteration: 10, Epoch: 6, Avg Time per Batch: 0.2679 sec, Loss: 3.44691071, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1871, R2: -0.0512\n",
      "Iteration: 10, Epoch: 7, Avg Time per Batch: 0.2677 sec, Loss: 3.44675602, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1870, R2: -0.0500\n",
      "Iteration: 10, Epoch: 8, Avg Time per Batch: 0.2685 sec, Loss: 3.44659328, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1870, R2: -0.0510\n",
      "Iteration: 10, Epoch: 9, Avg Time per Batch: 0.2688 sec, Loss: 3.44646019, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1870, R2: -0.0512\n",
      "Iteration: 10, Epoch: 10, Avg Time per Batch: 0.2687 sec, Loss: 3.44638967, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1870, R2: -0.0516\n",
      "Iteration: 10, Epoch: 11, Avg Time per Batch: 0.2681 sec, Loss: 3.44638879, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1870, R2: -0.0514\n",
      "Iteration: 10, Epoch: 12, Avg Time per Batch: 0.2675 sec, Loss: 3.44624136, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1870, R2: -0.0510\n",
      "Iteration: 10, Epoch: 13, Avg Time per Batch: 0.2677 sec, Loss: 3.44616232, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1870, R2: -0.0507\n",
      "Iteration: 10, Epoch: 14, Avg Time per Batch: 0.2683 sec, Loss: 3.44615699, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1870, R2: -0.0504\n",
      "Iteration: 10, Epoch: 15, Avg Time per Batch: 0.2682 sec, Loss: 3.44596300, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1870, R2: -0.0508\n",
      "Iteration: 10, Epoch: 16, Avg Time per Batch: 0.2676 sec, Loss: 3.44596139, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1870, R2: -0.0505\n",
      "Iteration: 10, Epoch: 17, Avg Time per Batch: 0.2681 sec, Loss: 3.44592845, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1871, R2: -0.0511\n",
      "Iteration: 10, Epoch: 18, Avg Time per Batch: 0.2663 sec, Loss: 3.44577669, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1871, R2: -0.0510\n",
      "Iteration: 10, Epoch: 19, Avg Time per Batch: 0.2663 sec, Loss: 3.44574724, MSE: 0.0350, MAE: 0.1302, RMSE: 0.1871, R2: -0.0509\n",
      "Epoch 19 completed in 1179.69 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 10, Epoch: 19, Loss: 3.4457472430\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is PRTH\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([121, 60, 24])\n",
      "Iteration: 11, Epoch: 0, Avg Time per Batch: 0.4251 sec, Loss: 3.40342927, MSE: 0.0783, MAE: 0.2148, RMSE: 0.2798, R2: -2.7010\n",
      "Iteration: 11, Epoch: 1, Avg Time per Batch: 0.3838 sec, Loss: 3.41122997, MSE: 0.0721, MAE: 0.2044, RMSE: 0.2685, R2: -1.9351\n",
      "Iteration: 11, Epoch: 2, Avg Time per Batch: 0.4141 sec, Loss: 3.43940711, MSE: 0.0657, MAE: 0.1943, RMSE: 0.2562, R2: -1.4389\n",
      "Iteration: 11, Epoch: 3, Avg Time per Batch: 0.3747 sec, Loss: 3.44182390, MSE: 0.0610, MAE: 0.1886, RMSE: 0.2471, R2: -1.2213\n",
      "Iteration: 11, Epoch: 4, Avg Time per Batch: 0.3421 sec, Loss: 3.44898415, MSE: 0.0574, MAE: 0.1838, RMSE: 0.2396, R2: -1.0308\n",
      "Iteration: 11, Epoch: 5, Avg Time per Batch: 0.3887 sec, Loss: 3.45338645, MSE: 0.0552, MAE: 0.1814, RMSE: 0.2350, R2: -0.8775\n",
      "Iteration: 11, Epoch: 6, Avg Time per Batch: 0.3951 sec, Loss: 3.45584614, MSE: 0.0535, MAE: 0.1789, RMSE: 0.2312, R2: -0.7771\n",
      "Iteration: 11, Epoch: 7, Avg Time per Batch: 0.4256 sec, Loss: 3.46259990, MSE: 0.0524, MAE: 0.1773, RMSE: 0.2288, R2: -0.7140\n",
      "Iteration: 11, Epoch: 8, Avg Time per Batch: 0.4101 sec, Loss: 3.46084525, MSE: 0.0516, MAE: 0.1762, RMSE: 0.2272, R2: -0.6754\n",
      "Iteration: 11, Epoch: 9, Avg Time per Batch: 0.3894 sec, Loss: 3.46079972, MSE: 0.0509, MAE: 0.1751, RMSE: 0.2257, R2: -0.6347\n",
      "Iteration: 11, Epoch: 10, Avg Time per Batch: 0.3918 sec, Loss: 3.45941936, MSE: 0.0503, MAE: 0.1741, RMSE: 0.2242, R2: -0.5849\n",
      "Iteration: 11, Epoch: 11, Avg Time per Batch: 0.4105 sec, Loss: 3.46409589, MSE: 0.0496, MAE: 0.1730, RMSE: 0.2227, R2: -0.5438\n",
      "Iteration: 11, Epoch: 12, Avg Time per Batch: 0.3665 sec, Loss: 3.45721846, MSE: 0.0489, MAE: 0.1719, RMSE: 0.2212, R2: -0.5069\n",
      "Iteration: 11, Epoch: 13, Avg Time per Batch: 0.3482 sec, Loss: 3.45493942, MSE: 0.0485, MAE: 0.1713, RMSE: 0.2201, R2: -0.4750\n",
      "Iteration: 11, Epoch: 14, Avg Time per Batch: 0.3429 sec, Loss: 3.45345569, MSE: 0.0480, MAE: 0.1707, RMSE: 0.2191, R2: -0.4470\n",
      "Iteration: 11, Epoch: 15, Avg Time per Batch: 0.4103 sec, Loss: 3.45207443, MSE: 0.0477, MAE: 0.1702, RMSE: 0.2183, R2: -0.4242\n",
      "Iteration: 11, Epoch: 16, Avg Time per Batch: 0.4315 sec, Loss: 3.45137374, MSE: 0.0473, MAE: 0.1697, RMSE: 0.2175, R2: -0.4082\n",
      "Iteration: 11, Epoch: 17, Avg Time per Batch: 0.3673 sec, Loss: 3.45449513, MSE: 0.0470, MAE: 0.1692, RMSE: 0.2168, R2: -0.3917\n",
      "Iteration: 11, Epoch: 18, Avg Time per Batch: 0.4233 sec, Loss: 3.45156739, MSE: 0.0467, MAE: 0.1688, RMSE: 0.2162, R2: -0.3748\n",
      "Iteration: 11, Epoch: 19, Avg Time per Batch: 0.3728 sec, Loss: 3.45206157, MSE: 0.0464, MAE: 0.1684, RMSE: 0.2155, R2: -0.3582\n",
      "Epoch 19 completed in 342.24 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 11, Epoch: 19, Loss: 3.4520615697\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is MET\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([4315, 60, 24])\n",
      "Iteration: 12, Epoch: 0, Avg Time per Batch: 0.2716 sec, Loss: 3.44963396, MSE: 0.0397, MAE: 0.1527, RMSE: 0.1993, R2: -0.0598\n",
      "Iteration: 12, Epoch: 1, Avg Time per Batch: 0.2694 sec, Loss: 3.44966788, MSE: 0.0396, MAE: 0.1526, RMSE: 0.1989, R2: -0.0529\n",
      "Iteration: 12, Epoch: 2, Avg Time per Batch: 0.2672 sec, Loss: 3.44935634, MSE: 0.0395, MAE: 0.1525, RMSE: 0.1987, R2: -0.0485\n",
      "Iteration: 12, Epoch: 3, Avg Time per Batch: 0.2679 sec, Loss: 3.44889065, MSE: 0.0395, MAE: 0.1525, RMSE: 0.1987, R2: -0.0499\n",
      "Iteration: 12, Epoch: 4, Avg Time per Batch: 0.2680 sec, Loss: 3.44865431, MSE: 0.0395, MAE: 0.1525, RMSE: 0.1987, R2: -0.0499\n",
      "Iteration: 12, Epoch: 5, Avg Time per Batch: 0.2673 sec, Loss: 3.44879316, MSE: 0.0395, MAE: 0.1525, RMSE: 0.1987, R2: -0.0491\n",
      "Iteration: 12, Epoch: 6, Avg Time per Batch: 0.2683 sec, Loss: 3.44867287, MSE: 0.0395, MAE: 0.1525, RMSE: 0.1987, R2: -0.0479\n",
      "Iteration: 12, Epoch: 7, Avg Time per Batch: 0.2690 sec, Loss: 3.44865877, MSE: 0.0395, MAE: 0.1525, RMSE: 0.1987, R2: -0.0479\n",
      "Iteration: 12, Epoch: 8, Avg Time per Batch: 0.2670 sec, Loss: 3.44845986, MSE: 0.0395, MAE: 0.1525, RMSE: 0.1986, R2: -0.0474\n",
      "Iteration: 12, Epoch: 9, Avg Time per Batch: 0.2642 sec, Loss: 3.44831640, MSE: 0.0395, MAE: 0.1524, RMSE: 0.1986, R2: -0.0475\n",
      "Iteration: 12, Epoch: 10, Avg Time per Batch: 0.2693 sec, Loss: 3.44828166, MSE: 0.0395, MAE: 0.1525, RMSE: 0.1986, R2: -0.0475\n",
      "Iteration: 12, Epoch: 11, Avg Time per Batch: 0.2658 sec, Loss: 3.44811797, MSE: 0.0394, MAE: 0.1524, RMSE: 0.1986, R2: -0.0469\n",
      "Iteration: 12, Epoch: 12, Avg Time per Batch: 0.2666 sec, Loss: 3.44809574, MSE: 0.0395, MAE: 0.1524, RMSE: 0.1986, R2: -0.0472\n",
      "Iteration: 12, Epoch: 13, Avg Time per Batch: 0.2651 sec, Loss: 3.44802911, MSE: 0.0394, MAE: 0.1524, RMSE: 0.1986, R2: -0.0468\n",
      "Iteration: 12, Epoch: 14, Avg Time per Batch: 0.2666 sec, Loss: 3.44802714, MSE: 0.0394, MAE: 0.1524, RMSE: 0.1986, R2: -0.0471\n",
      "Iteration: 12, Epoch: 15, Avg Time per Batch: 0.2686 sec, Loss: 3.44778023, MSE: 0.0394, MAE: 0.1524, RMSE: 0.1986, R2: -0.0470\n",
      "Iteration: 12, Epoch: 16, Avg Time per Batch: 0.2680 sec, Loss: 3.44772399, MSE: 0.0394, MAE: 0.1524, RMSE: 0.1986, R2: -0.0469\n",
      "Iteration: 12, Epoch: 17, Avg Time per Batch: 0.2663 sec, Loss: 3.44768060, MSE: 0.0394, MAE: 0.1524, RMSE: 0.1986, R2: -0.0470\n",
      "Iteration: 12, Epoch: 18, Avg Time per Batch: 0.2627 sec, Loss: 3.44762208, MSE: 0.0394, MAE: 0.1524, RMSE: 0.1986, R2: -0.0470\n",
      "Iteration: 12, Epoch: 19, Avg Time per Batch: 0.2677 sec, Loss: 3.44754949, MSE: 0.0394, MAE: 0.1524, RMSE: 0.1986, R2: -0.0470\n",
      "Epoch 19 completed in 1143.50 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 12, Epoch: 19, Loss: 3.4475494949\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is CNSL\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([2985, 60, 24])\n",
      "Iteration: 13, Epoch: 0, Avg Time per Batch: 0.2659 sec, Loss: 3.45079820, MSE: 0.0446, MAE: 0.1577, RMSE: 0.2112, R2: -0.0528\n",
      "Iteration: 13, Epoch: 1, Avg Time per Batch: 0.2651 sec, Loss: 3.45017440, MSE: 0.0445, MAE: 0.1573, RMSE: 0.2110, R2: -0.0513\n",
      "Iteration: 13, Epoch: 2, Avg Time per Batch: 0.2640 sec, Loss: 3.45057216, MSE: 0.0444, MAE: 0.1571, RMSE: 0.2108, R2: -0.0511\n",
      "Iteration: 13, Epoch: 3, Avg Time per Batch: 0.2681 sec, Loss: 3.45012034, MSE: 0.0444, MAE: 0.1571, RMSE: 0.2108, R2: -0.0514\n",
      "Iteration: 13, Epoch: 4, Avg Time per Batch: 0.2657 sec, Loss: 3.45068568, MSE: 0.0444, MAE: 0.1570, RMSE: 0.2108, R2: -0.0519\n",
      "Iteration: 13, Epoch: 5, Avg Time per Batch: 0.2636 sec, Loss: 3.45030029, MSE: 0.0444, MAE: 0.1570, RMSE: 0.2108, R2: -0.0516\n",
      "Iteration: 13, Epoch: 6, Avg Time per Batch: 0.2677 sec, Loss: 3.45005151, MSE: 0.0444, MAE: 0.1570, RMSE: 0.2108, R2: -0.0515\n",
      "Iteration: 13, Epoch: 7, Avg Time per Batch: 0.2670 sec, Loss: 3.45015360, MSE: 0.0444, MAE: 0.1570, RMSE: 0.2107, R2: -0.0520\n",
      "Iteration: 13, Epoch: 8, Avg Time per Batch: 0.2650 sec, Loss: 3.45019880, MSE: 0.0444, MAE: 0.1570, RMSE: 0.2107, R2: -0.0515\n",
      "Iteration: 13, Epoch: 9, Avg Time per Batch: 0.2678 sec, Loss: 3.45008899, MSE: 0.0444, MAE: 0.1570, RMSE: 0.2107, R2: -0.0509\n",
      "Iteration: 13, Epoch: 10, Avg Time per Batch: 0.2669 sec, Loss: 3.45011627, MSE: 0.0444, MAE: 0.1570, RMSE: 0.2107, R2: -0.0504\n",
      "Iteration: 13, Epoch: 11, Avg Time per Batch: 0.2659 sec, Loss: 3.45015519, MSE: 0.0444, MAE: 0.1570, RMSE: 0.2107, R2: -0.0499\n",
      "Iteration: 13, Epoch: 12, Avg Time per Batch: 0.2642 sec, Loss: 3.44996496, MSE: 0.0444, MAE: 0.1569, RMSE: 0.2107, R2: -0.0499\n",
      "Iteration: 13, Epoch: 13, Avg Time per Batch: 0.2674 sec, Loss: 3.44980587, MSE: 0.0444, MAE: 0.1569, RMSE: 0.2107, R2: -0.0496\n",
      "Iteration: 13, Epoch: 14, Avg Time per Batch: 0.2686 sec, Loss: 3.44996523, MSE: 0.0444, MAE: 0.1569, RMSE: 0.2107, R2: -0.0498\n",
      "Iteration: 13, Epoch: 15, Avg Time per Batch: 0.2675 sec, Loss: 3.44985018, MSE: 0.0444, MAE: 0.1569, RMSE: 0.2107, R2: -0.0497\n",
      "Iteration: 13, Epoch: 16, Avg Time per Batch: 0.2668 sec, Loss: 3.44972438, MSE: 0.0444, MAE: 0.1569, RMSE: 0.2107, R2: -0.0494\n",
      "Iteration: 13, Epoch: 17, Avg Time per Batch: 0.2631 sec, Loss: 3.44968023, MSE: 0.0444, MAE: 0.1569, RMSE: 0.2107, R2: -0.0492\n",
      "Iteration: 13, Epoch: 18, Avg Time per Batch: 0.2626 sec, Loss: 3.44959933, MSE: 0.0444, MAE: 0.1569, RMSE: 0.2107, R2: -0.0492\n",
      "Iteration: 13, Epoch: 19, Avg Time per Batch: 0.2667 sec, Loss: 3.44946605, MSE: 0.0444, MAE: 0.1569, RMSE: 0.2107, R2: -0.0491\n",
      "Epoch 19 completed in 891.74 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 13, Epoch: 19, Loss: 3.4494660517\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is BERY\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([1170, 60, 24])\n",
      "Iteration: 14, Epoch: 0, Avg Time per Batch: 0.2651 sec, Loss: 3.45892863, MSE: 0.0514, MAE: 0.1714, RMSE: 0.2268, R2: -0.0919\n",
      "Iteration: 14, Epoch: 1, Avg Time per Batch: 0.2707 sec, Loss: 3.45649861, MSE: 0.0508, MAE: 0.1701, RMSE: 0.2253, R2: -0.0693\n",
      "Iteration: 14, Epoch: 2, Avg Time per Batch: 0.2658 sec, Loss: 3.45758051, MSE: 0.0505, MAE: 0.1693, RMSE: 0.2247, R2: -0.0586\n",
      "Iteration: 14, Epoch: 3, Avg Time per Batch: 0.2797 sec, Loss: 3.45635207, MSE: 0.0503, MAE: 0.1690, RMSE: 0.2244, R2: -0.0550\n",
      "Iteration: 14, Epoch: 4, Avg Time per Batch: 0.2753 sec, Loss: 3.45570101, MSE: 0.0502, MAE: 0.1688, RMSE: 0.2241, R2: -0.0527\n",
      "Iteration: 14, Epoch: 5, Avg Time per Batch: 0.2705 sec, Loss: 3.45486051, MSE: 0.0502, MAE: 0.1686, RMSE: 0.2240, R2: -0.0522\n",
      "Iteration: 14, Epoch: 6, Avg Time per Batch: 0.2717 sec, Loss: 3.45495633, MSE: 0.0501, MAE: 0.1686, RMSE: 0.2239, R2: -0.0515\n",
      "Iteration: 14, Epoch: 7, Avg Time per Batch: 0.2764 sec, Loss: 3.45480062, MSE: 0.0501, MAE: 0.1686, RMSE: 0.2238, R2: -0.0502\n",
      "Iteration: 14, Epoch: 8, Avg Time per Batch: 0.2705 sec, Loss: 3.45496359, MSE: 0.0501, MAE: 0.1684, RMSE: 0.2237, R2: -0.0494\n",
      "Iteration: 14, Epoch: 9, Avg Time per Batch: 0.2721 sec, Loss: 3.45491827, MSE: 0.0500, MAE: 0.1685, RMSE: 0.2237, R2: -0.0503\n",
      "Iteration: 14, Epoch: 10, Avg Time per Batch: 0.2731 sec, Loss: 3.45474899, MSE: 0.0500, MAE: 0.1684, RMSE: 0.2236, R2: -0.0489\n",
      "Iteration: 14, Epoch: 11, Avg Time per Batch: 0.2715 sec, Loss: 3.45463456, MSE: 0.0500, MAE: 0.1683, RMSE: 0.2236, R2: -0.0488\n",
      "Iteration: 14, Epoch: 12, Avg Time per Batch: 0.2698 sec, Loss: 3.45417511, MSE: 0.0500, MAE: 0.1683, RMSE: 0.2236, R2: -0.0479\n",
      "Iteration: 14, Epoch: 13, Avg Time per Batch: 0.2656 sec, Loss: 3.45433893, MSE: 0.0500, MAE: 0.1683, RMSE: 0.2235, R2: -0.0477\n",
      "Iteration: 14, Epoch: 14, Avg Time per Batch: 0.2735 sec, Loss: 3.45463006, MSE: 0.0500, MAE: 0.1683, RMSE: 0.2236, R2: -0.0475\n",
      "Iteration: 14, Epoch: 15, Avg Time per Batch: 0.2741 sec, Loss: 3.45451954, MSE: 0.0500, MAE: 0.1683, RMSE: 0.2235, R2: -0.0471\n",
      "Iteration: 14, Epoch: 16, Avg Time per Batch: 0.2773 sec, Loss: 3.45437354, MSE: 0.0500, MAE: 0.1683, RMSE: 0.2235, R2: -0.0466\n",
      "Iteration: 14, Epoch: 17, Avg Time per Batch: 0.2774 sec, Loss: 3.45409389, MSE: 0.0500, MAE: 0.1682, RMSE: 0.2235, R2: -0.0465\n",
      "Iteration: 14, Epoch: 18, Avg Time per Batch: 0.2707 sec, Loss: 3.45408597, MSE: 0.0499, MAE: 0.1682, RMSE: 0.2235, R2: -0.0464\n",
      "Iteration: 14, Epoch: 19, Avg Time per Batch: 0.2673 sec, Loss: 3.45405602, MSE: 0.0499, MAE: 0.1682, RMSE: 0.2235, R2: -0.0461\n",
      "Epoch 19 completed in 546.68 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 14, Epoch: 19, Loss: 3.4540560184\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is TXRH\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([3186, 60, 24])\n",
      "Iteration: 15, Epoch: 0, Avg Time per Batch: 0.2634 sec, Loss: 3.45051255, MSE: 0.0459, MAE: 0.1615, RMSE: 0.2143, R2: -0.0686\n",
      "Iteration: 15, Epoch: 1, Avg Time per Batch: 0.2644 sec, Loss: 3.44963950, MSE: 0.0457, MAE: 0.1609, RMSE: 0.2138, R2: -0.0581\n",
      "Iteration: 15, Epoch: 2, Avg Time per Batch: 0.2667 sec, Loss: 3.44895935, MSE: 0.0457, MAE: 0.1607, RMSE: 0.2137, R2: -0.0573\n",
      "Iteration: 15, Epoch: 3, Avg Time per Batch: 0.2677 sec, Loss: 3.44892593, MSE: 0.0456, MAE: 0.1605, RMSE: 0.2135, R2: -0.0541\n",
      "Iteration: 15, Epoch: 4, Avg Time per Batch: 0.2685 sec, Loss: 3.44886386, MSE: 0.0455, MAE: 0.1603, RMSE: 0.2134, R2: -0.0524\n",
      "Iteration: 15, Epoch: 5, Avg Time per Batch: 0.2643 sec, Loss: 3.44821683, MSE: 0.0455, MAE: 0.1603, RMSE: 0.2134, R2: -0.0501\n",
      "Iteration: 15, Epoch: 6, Avg Time per Batch: 0.2645 sec, Loss: 3.44838599, MSE: 0.0455, MAE: 0.1602, RMSE: 0.2133, R2: -0.0489\n",
      "Iteration: 15, Epoch: 7, Avg Time per Batch: 0.2630 sec, Loss: 3.44850558, MSE: 0.0455, MAE: 0.1602, RMSE: 0.2133, R2: -0.0482\n",
      "Iteration: 15, Epoch: 8, Avg Time per Batch: 0.2696 sec, Loss: 3.44824352, MSE: 0.0455, MAE: 0.1601, RMSE: 0.2132, R2: -0.0483\n",
      "Iteration: 15, Epoch: 9, Avg Time per Batch: 0.2707 sec, Loss: 3.44811690, MSE: 0.0455, MAE: 0.1601, RMSE: 0.2132, R2: -0.0483\n",
      "Iteration: 15, Epoch: 10, Avg Time per Batch: 0.2659 sec, Loss: 3.44826983, MSE: 0.0455, MAE: 0.1601, RMSE: 0.2132, R2: -0.0480\n",
      "Iteration: 15, Epoch: 11, Avg Time per Batch: 0.2661 sec, Loss: 3.44819567, MSE: 0.0455, MAE: 0.1601, RMSE: 0.2132, R2: -0.0476\n",
      "Iteration: 15, Epoch: 12, Avg Time per Batch: 0.2683 sec, Loss: 3.44806845, MSE: 0.0455, MAE: 0.1601, RMSE: 0.2132, R2: -0.0475\n",
      "Iteration: 15, Epoch: 13, Avg Time per Batch: 0.2663 sec, Loss: 3.44801985, MSE: 0.0455, MAE: 0.1601, RMSE: 0.2132, R2: -0.0480\n",
      "Iteration: 15, Epoch: 14, Avg Time per Batch: 0.2674 sec, Loss: 3.44794396, MSE: 0.0455, MAE: 0.1601, RMSE: 0.2132, R2: -0.0477\n",
      "Iteration: 15, Epoch: 15, Avg Time per Batch: 0.2668 sec, Loss: 3.44788477, MSE: 0.0455, MAE: 0.1601, RMSE: 0.2132, R2: -0.0477\n",
      "Iteration: 15, Epoch: 16, Avg Time per Batch: 0.2682 sec, Loss: 3.44785564, MSE: 0.0454, MAE: 0.1601, RMSE: 0.2132, R2: -0.0474\n",
      "Iteration: 15, Epoch: 17, Avg Time per Batch: 0.2679 sec, Loss: 3.44771857, MSE: 0.0455, MAE: 0.1601, RMSE: 0.2132, R2: -0.0473\n",
      "Iteration: 15, Epoch: 18, Avg Time per Batch: 0.2699 sec, Loss: 3.44773655, MSE: 0.0454, MAE: 0.1601, RMSE: 0.2132, R2: -0.0468\n",
      "Iteration: 15, Epoch: 19, Avg Time per Batch: 0.2673 sec, Loss: 3.44772256, MSE: 0.0454, MAE: 0.1601, RMSE: 0.2132, R2: -0.0465\n",
      "Epoch 19 completed in 934.73 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 15, Epoch: 19, Loss: 3.4477225602\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is DVAX\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([3344, 60, 24])\n",
      "Iteration: 16, Epoch: 0, Avg Time per Batch: 0.2664 sec, Loss: 3.43769907, MSE: 0.0380, MAE: 0.1456, RMSE: 0.1949, R2: -0.0814\n",
      "Iteration: 16, Epoch: 1, Avg Time per Batch: 0.2656 sec, Loss: 3.43891709, MSE: 0.0380, MAE: 0.1452, RMSE: 0.1948, R2: -0.0671\n",
      "Iteration: 16, Epoch: 2, Avg Time per Batch: 0.2679 sec, Loss: 3.43787060, MSE: 0.0380, MAE: 0.1452, RMSE: 0.1948, R2: -0.0607\n",
      "Iteration: 16, Epoch: 3, Avg Time per Batch: 0.2699 sec, Loss: 3.43860727, MSE: 0.0379, MAE: 0.1452, RMSE: 0.1948, R2: -0.0587\n",
      "Iteration: 16, Epoch: 4, Avg Time per Batch: 0.2678 sec, Loss: 3.43853658, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1948, R2: -0.0569\n",
      "Iteration: 16, Epoch: 5, Avg Time per Batch: 0.2670 sec, Loss: 3.43848261, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1947, R2: -0.0568\n",
      "Iteration: 16, Epoch: 6, Avg Time per Batch: 0.2687 sec, Loss: 3.43845798, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1947, R2: -0.0560\n",
      "Iteration: 16, Epoch: 7, Avg Time per Batch: 0.2695 sec, Loss: 3.43823143, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1947, R2: -0.0558\n",
      "Iteration: 16, Epoch: 8, Avg Time per Batch: 0.2694 sec, Loss: 3.43834620, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1947, R2: -0.0561\n",
      "Iteration: 16, Epoch: 9, Avg Time per Batch: 0.2694 sec, Loss: 3.43814193, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1947, R2: -0.0554\n",
      "Iteration: 16, Epoch: 10, Avg Time per Batch: 0.2693 sec, Loss: 3.43823263, MSE: 0.0379, MAE: 0.1450, RMSE: 0.1947, R2: -0.0548\n",
      "Iteration: 16, Epoch: 11, Avg Time per Batch: 0.2685 sec, Loss: 3.43818958, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1947, R2: -0.0554\n",
      "Iteration: 16, Epoch: 12, Avg Time per Batch: 0.2702 sec, Loss: 3.43820208, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1948, R2: -0.0549\n",
      "Iteration: 16, Epoch: 13, Avg Time per Batch: 0.2692 sec, Loss: 3.43802217, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1948, R2: -0.0548\n",
      "Iteration: 16, Epoch: 14, Avg Time per Batch: 0.2679 sec, Loss: 3.43802360, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1948, R2: -0.0547\n",
      "Iteration: 16, Epoch: 15, Avg Time per Batch: 0.2671 sec, Loss: 3.43803951, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1948, R2: -0.0545\n",
      "Iteration: 16, Epoch: 16, Avg Time per Batch: 0.2700 sec, Loss: 3.43792727, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1947, R2: -0.0545\n",
      "Iteration: 16, Epoch: 17, Avg Time per Batch: 0.2683 sec, Loss: 3.43791157, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1947, R2: -0.0542\n",
      "Iteration: 16, Epoch: 18, Avg Time per Batch: 0.2696 sec, Loss: 3.43789063, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1947, R2: -0.0541\n",
      "Iteration: 16, Epoch: 19, Avg Time per Batch: 0.2679 sec, Loss: 3.43784655, MSE: 0.0379, MAE: 0.1451, RMSE: 0.1947, R2: -0.0540\n",
      "Epoch 19 completed in 961.03 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 16, Epoch: 19, Loss: 3.4378465471\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is CNQ\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([4235, 60, 24])\n",
      "Iteration: 17, Epoch: 0, Avg Time per Batch: 0.2679 sec, Loss: 3.44658117, MSE: 0.0483, MAE: 0.1668, RMSE: 0.2198, R2: -0.0607\n",
      "Iteration: 17, Epoch: 1, Avg Time per Batch: 0.2668 sec, Loss: 3.44692341, MSE: 0.0480, MAE: 0.1663, RMSE: 0.2191, R2: -0.0504\n",
      "Iteration: 17, Epoch: 2, Avg Time per Batch: 0.2675 sec, Loss: 3.44705628, MSE: 0.0480, MAE: 0.1662, RMSE: 0.2190, R2: -0.0474\n",
      "Iteration: 17, Epoch: 3, Avg Time per Batch: 0.2674 sec, Loss: 3.44703371, MSE: 0.0479, MAE: 0.1661, RMSE: 0.2189, R2: -0.0482\n",
      "Iteration: 17, Epoch: 4, Avg Time per Batch: 0.2694 sec, Loss: 3.44686240, MSE: 0.0479, MAE: 0.1661, RMSE: 0.2188, R2: -0.0475\n",
      "Iteration: 17, Epoch: 5, Avg Time per Batch: 0.2671 sec, Loss: 3.44699219, MSE: 0.0479, MAE: 0.1660, RMSE: 0.2188, R2: -0.0469\n",
      "Iteration: 17, Epoch: 6, Avg Time per Batch: 0.2673 sec, Loss: 3.44686145, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0462\n",
      "Iteration: 17, Epoch: 7, Avg Time per Batch: 0.2675 sec, Loss: 3.44676040, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0459\n",
      "Iteration: 17, Epoch: 8, Avg Time per Batch: 0.2676 sec, Loss: 3.44682414, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0462\n",
      "Iteration: 17, Epoch: 9, Avg Time per Batch: 0.2684 sec, Loss: 3.44659660, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0460\n",
      "Iteration: 17, Epoch: 10, Avg Time per Batch: 0.2697 sec, Loss: 3.44654847, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0460\n",
      "Iteration: 17, Epoch: 11, Avg Time per Batch: 0.2684 sec, Loss: 3.44657398, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0460\n",
      "Iteration: 17, Epoch: 12, Avg Time per Batch: 0.2690 sec, Loss: 3.44645123, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0461\n",
      "Iteration: 17, Epoch: 13, Avg Time per Batch: 0.2676 sec, Loss: 3.44640029, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0457\n",
      "Iteration: 17, Epoch: 14, Avg Time per Batch: 0.2685 sec, Loss: 3.44639568, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0455\n",
      "Iteration: 17, Epoch: 15, Avg Time per Batch: 0.2677 sec, Loss: 3.44630274, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0456\n",
      "Iteration: 17, Epoch: 16, Avg Time per Batch: 0.2683 sec, Loss: 3.44622462, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0455\n",
      "Iteration: 17, Epoch: 17, Avg Time per Batch: 0.2678 sec, Loss: 3.44612720, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0455\n",
      "Iteration: 17, Epoch: 18, Avg Time per Batch: 0.2684 sec, Loss: 3.44608273, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0455\n",
      "Iteration: 17, Epoch: 19, Avg Time per Batch: 0.2686 sec, Loss: 3.44602538, MSE: 0.0478, MAE: 0.1660, RMSE: 0.2187, R2: -0.0455\n",
      "Epoch 19 completed in 1132.73 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 17, Epoch: 19, Loss: 3.4460253765\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is EBAY\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([4701, 60, 24])\n",
      "Iteration: 18, Epoch: 0, Avg Time per Batch: 0.2675 sec, Loss: 3.43626766, MSE: 0.0393, MAE: 0.1486, RMSE: 0.1982, R2: -0.0529\n",
      "Iteration: 18, Epoch: 1, Avg Time per Batch: 0.2664 sec, Loss: 3.43543797, MSE: 0.0392, MAE: 0.1485, RMSE: 0.1980, R2: -0.0481\n",
      "Iteration: 18, Epoch: 2, Avg Time per Batch: 0.2686 sec, Loss: 3.43555000, MSE: 0.0391, MAE: 0.1484, RMSE: 0.1978, R2: -0.0474\n",
      "Iteration: 18, Epoch: 3, Avg Time per Batch: 0.2692 sec, Loss: 3.43534664, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0463\n",
      "Iteration: 18, Epoch: 4, Avg Time per Batch: 0.2675 sec, Loss: 3.43527355, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0456\n",
      "Iteration: 18, Epoch: 5, Avg Time per Batch: 0.2680 sec, Loss: 3.43534071, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0452\n",
      "Iteration: 18, Epoch: 6, Avg Time per Batch: 0.2676 sec, Loss: 3.43537794, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0449\n",
      "Iteration: 18, Epoch: 7, Avg Time per Batch: 0.2689 sec, Loss: 3.43531029, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0448\n",
      "Iteration: 18, Epoch: 8, Avg Time per Batch: 0.2680 sec, Loss: 3.43517465, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0449\n",
      "Iteration: 18, Epoch: 9, Avg Time per Batch: 0.2689 sec, Loss: 3.43511275, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0442\n",
      "Iteration: 18, Epoch: 10, Avg Time per Batch: 0.2680 sec, Loss: 3.43505987, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0446\n",
      "Iteration: 18, Epoch: 11, Avg Time per Batch: 0.2684 sec, Loss: 3.43509016, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0448\n",
      "Iteration: 18, Epoch: 12, Avg Time per Batch: 0.2678 sec, Loss: 3.43500412, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0449\n",
      "Iteration: 18, Epoch: 13, Avg Time per Batch: 0.2693 sec, Loss: 3.43491943, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0445\n",
      "Iteration: 18, Epoch: 14, Avg Time per Batch: 0.2679 sec, Loss: 3.43482437, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0443\n",
      "Iteration: 18, Epoch: 15, Avg Time per Batch: 0.2687 sec, Loss: 3.43478317, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0445\n",
      "Iteration: 18, Epoch: 16, Avg Time per Batch: 0.2699 sec, Loss: 3.43472042, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0441\n",
      "Iteration: 18, Epoch: 17, Avg Time per Batch: 0.2676 sec, Loss: 3.43466055, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0443\n",
      "Iteration: 18, Epoch: 18, Avg Time per Batch: 0.2680 sec, Loss: 3.43465544, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0444\n",
      "Iteration: 18, Epoch: 19, Avg Time per Batch: 0.2691 sec, Loss: 3.43457915, MSE: 0.0391, MAE: 0.1483, RMSE: 0.1977, R2: -0.0444\n",
      "Epoch 19 completed in 1218.97 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 18, Epoch: 19, Loss: 3.4345791468\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is FTK\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([2982, 60, 24])\n",
      "Iteration: 19, Epoch: 0, Avg Time per Batch: 0.2706 sec, Loss: 3.43003848, MSE: 0.0372, MAE: 0.1412, RMSE: 0.1929, R2: -0.0645\n",
      "Iteration: 19, Epoch: 1, Avg Time per Batch: 0.2707 sec, Loss: 3.43122927, MSE: 0.0372, MAE: 0.1410, RMSE: 0.1928, R2: -0.0559\n",
      "Iteration: 19, Epoch: 2, Avg Time per Batch: 0.2697 sec, Loss: 3.43150704, MSE: 0.0371, MAE: 0.1408, RMSE: 0.1927, R2: -0.0533\n",
      "Iteration: 19, Epoch: 3, Avg Time per Batch: 0.2679 sec, Loss: 3.43082188, MSE: 0.0372, MAE: 0.1408, RMSE: 0.1928, R2: -0.0546\n",
      "Iteration: 19, Epoch: 4, Avg Time per Batch: 0.2711 sec, Loss: 3.43099372, MSE: 0.0372, MAE: 0.1408, RMSE: 0.1928, R2: -0.0543\n",
      "Iteration: 19, Epoch: 5, Avg Time per Batch: 0.2689 sec, Loss: 3.43094132, MSE: 0.0372, MAE: 0.1408, RMSE: 0.1928, R2: -0.0539\n",
      "Iteration: 19, Epoch: 6, Avg Time per Batch: 0.2676 sec, Loss: 3.43117203, MSE: 0.0372, MAE: 0.1408, RMSE: 0.1928, R2: -0.0534\n",
      "Iteration: 19, Epoch: 7, Avg Time per Batch: 0.2695 sec, Loss: 3.43099596, MSE: 0.0372, MAE: 0.1408, RMSE: 0.1927, R2: -0.0520\n",
      "Iteration: 19, Epoch: 8, Avg Time per Batch: 0.2691 sec, Loss: 3.43074729, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0509\n",
      "Iteration: 19, Epoch: 9, Avg Time per Batch: 0.2675 sec, Loss: 3.43083358, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0505\n",
      "Iteration: 19, Epoch: 10, Avg Time per Batch: 0.2662 sec, Loss: 3.43086329, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0506\n",
      "Iteration: 19, Epoch: 11, Avg Time per Batch: 0.2679 sec, Loss: 3.43074403, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0506\n",
      "Iteration: 19, Epoch: 12, Avg Time per Batch: 0.2693 sec, Loss: 3.43092536, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0506\n",
      "Iteration: 19, Epoch: 13, Avg Time per Batch: 0.2640 sec, Loss: 3.43082619, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0502\n",
      "Iteration: 19, Epoch: 14, Avg Time per Batch: 0.2683 sec, Loss: 3.43069569, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0500\n",
      "Iteration: 19, Epoch: 15, Avg Time per Batch: 0.2687 sec, Loss: 3.43063893, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0505\n",
      "Iteration: 19, Epoch: 16, Avg Time per Batch: 0.2647 sec, Loss: 3.43079286, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0501\n",
      "Iteration: 19, Epoch: 17, Avg Time per Batch: 0.2679 sec, Loss: 3.43076024, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0500\n",
      "Iteration: 19, Epoch: 18, Avg Time per Batch: 0.2684 sec, Loss: 3.43065120, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0500\n",
      "Iteration: 19, Epoch: 19, Avg Time per Batch: 0.2648 sec, Loss: 3.43068926, MSE: 0.0371, MAE: 0.1407, RMSE: 0.1927, R2: -0.0501\n",
      "Epoch 19 completed in 894.51 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 19, Epoch: 19, Loss: 3.4306892626\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is GOL\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([3257, 60, 24])\n",
      "Iteration: 20, Epoch: 0, Avg Time per Batch: 0.2722 sec, Loss: 3.43619965, MSE: 0.0446, MAE: 0.1571, RMSE: 0.2113, R2: -0.0523\n",
      "Iteration: 20, Epoch: 1, Avg Time per Batch: 0.2644 sec, Loss: 3.43729095, MSE: 0.0445, MAE: 0.1569, RMSE: 0.2110, R2: -0.0467\n",
      "Iteration: 20, Epoch: 2, Avg Time per Batch: 0.2674 sec, Loss: 3.43718045, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0469\n",
      "Iteration: 20, Epoch: 3, Avg Time per Batch: 0.2661 sec, Loss: 3.43714774, MSE: 0.0445, MAE: 0.1569, RMSE: 0.2108, R2: -0.0476\n",
      "Iteration: 20, Epoch: 4, Avg Time per Batch: 0.2659 sec, Loss: 3.43704429, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0482\n",
      "Iteration: 20, Epoch: 5, Avg Time per Batch: 0.2700 sec, Loss: 3.43729627, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0487\n",
      "Iteration: 20, Epoch: 6, Avg Time per Batch: 0.2717 sec, Loss: 3.43722327, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0484\n",
      "Iteration: 20, Epoch: 7, Avg Time per Batch: 0.2664 sec, Loss: 3.43700127, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0480\n",
      "Iteration: 20, Epoch: 8, Avg Time per Batch: 0.2656 sec, Loss: 3.43723509, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0477\n",
      "Iteration: 20, Epoch: 9, Avg Time per Batch: 0.2677 sec, Loss: 3.43694864, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0472\n",
      "Iteration: 20, Epoch: 10, Avg Time per Batch: 0.2659 sec, Loss: 3.43708182, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0474\n",
      "Iteration: 20, Epoch: 11, Avg Time per Batch: 0.2683 sec, Loss: 3.43706145, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0472\n",
      "Iteration: 20, Epoch: 12, Avg Time per Batch: 0.2720 sec, Loss: 3.43702375, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0466\n",
      "Iteration: 20, Epoch: 13, Avg Time per Batch: 0.2642 sec, Loss: 3.43698318, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0463\n",
      "Iteration: 20, Epoch: 14, Avg Time per Batch: 0.2673 sec, Loss: 3.43697729, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0466\n",
      "Iteration: 20, Epoch: 15, Avg Time per Batch: 0.2687 sec, Loss: 3.43684110, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0466\n",
      "Iteration: 20, Epoch: 16, Avg Time per Batch: 0.2676 sec, Loss: 3.43687104, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0465\n",
      "Iteration: 20, Epoch: 17, Avg Time per Batch: 0.2640 sec, Loss: 3.43694538, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0466\n",
      "Iteration: 20, Epoch: 18, Avg Time per Batch: 0.2660 sec, Loss: 3.43679178, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0466\n",
      "Iteration: 20, Epoch: 19, Avg Time per Batch: 0.2666 sec, Loss: 3.43683260, MSE: 0.0445, MAE: 0.1570, RMSE: 0.2109, R2: -0.0470\n",
      "Epoch 19 completed in 943.33 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 20, Epoch: 19, Loss: 3.4368326005\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is FENG\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([1521, 60, 24])\n",
      "Iteration: 21, Epoch: 0, Avg Time per Batch: 0.2723 sec, Loss: 3.43469862, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2112, R2: -0.0692\n",
      "Iteration: 21, Epoch: 1, Avg Time per Batch: 0.2717 sec, Loss: 3.43564191, MSE: 0.0445, MAE: 0.1581, RMSE: 0.2110, R2: -0.0539\n",
      "Iteration: 21, Epoch: 2, Avg Time per Batch: 0.2709 sec, Loss: 3.43539533, MSE: 0.0446, MAE: 0.1583, RMSE: 0.2112, R2: -0.0543\n",
      "Iteration: 21, Epoch: 3, Avg Time per Batch: 0.2706 sec, Loss: 3.43604386, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2112, R2: -0.0513\n",
      "Iteration: 21, Epoch: 4, Avg Time per Batch: 0.2709 sec, Loss: 3.43495867, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2112, R2: -0.0523\n",
      "Iteration: 21, Epoch: 5, Avg Time per Batch: 0.2672 sec, Loss: 3.43641483, MSE: 0.0446, MAE: 0.1581, RMSE: 0.2112, R2: -0.0504\n",
      "Iteration: 21, Epoch: 6, Avg Time per Batch: 0.2727 sec, Loss: 3.43565928, MSE: 0.0446, MAE: 0.1581, RMSE: 0.2111, R2: -0.0496\n",
      "Iteration: 21, Epoch: 7, Avg Time per Batch: 0.2703 sec, Loss: 3.43562206, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2111, R2: -0.0488\n",
      "Iteration: 21, Epoch: 8, Avg Time per Batch: 0.2729 sec, Loss: 3.43596329, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2111, R2: -0.0486\n",
      "Iteration: 21, Epoch: 9, Avg Time per Batch: 0.2713 sec, Loss: 3.43615767, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2111, R2: -0.0476\n",
      "Iteration: 21, Epoch: 10, Avg Time per Batch: 0.2744 sec, Loss: 3.43602141, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2111, R2: -0.0481\n",
      "Iteration: 21, Epoch: 11, Avg Time per Batch: 0.2677 sec, Loss: 3.43611389, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2111, R2: -0.0475\n",
      "Iteration: 21, Epoch: 12, Avg Time per Batch: 0.2669 sec, Loss: 3.43600299, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2111, R2: -0.0483\n",
      "Iteration: 21, Epoch: 13, Avg Time per Batch: 0.2743 sec, Loss: 3.43605604, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2112, R2: -0.0489\n",
      "Iteration: 21, Epoch: 14, Avg Time per Batch: 0.2702 sec, Loss: 3.43594805, MSE: 0.0446, MAE: 0.1583, RMSE: 0.2112, R2: -0.0484\n",
      "Iteration: 21, Epoch: 15, Avg Time per Batch: 0.2652 sec, Loss: 3.43601316, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2112, R2: -0.0483\n",
      "Iteration: 21, Epoch: 16, Avg Time per Batch: 0.2694 sec, Loss: 3.43598645, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2112, R2: -0.0480\n",
      "Iteration: 21, Epoch: 17, Avg Time per Batch: 0.2686 sec, Loss: 3.43582969, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2112, R2: -0.0480\n",
      "Iteration: 21, Epoch: 18, Avg Time per Batch: 0.2706 sec, Loss: 3.43590618, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2112, R2: -0.0478\n",
      "Iteration: 21, Epoch: 19, Avg Time per Batch: 0.2614 sec, Loss: 3.43599719, MSE: 0.0446, MAE: 0.1582, RMSE: 0.2111, R2: -0.0477\n",
      "Epoch 19 completed in 614.78 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 21, Epoch: 19, Loss: 3.4359971868\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is KWR\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5895, 60, 24])\n",
      "Iteration: 22, Epoch: 0, Avg Time per Batch: 0.2627 sec, Loss: 3.42700492, MSE: 0.0371, MAE: 0.1384, RMSE: 0.1925, R2: -2.1967\n",
      "Iteration: 22, Epoch: 1, Avg Time per Batch: 0.2670 sec, Loss: 3.42715905, MSE: 0.0368, MAE: 0.1379, RMSE: 0.1918, R2: -1.1581\n",
      "Iteration: 22, Epoch: 2, Avg Time per Batch: 0.2655 sec, Loss: 3.42731972, MSE: 0.0367, MAE: 0.1377, RMSE: 0.1916, R2: -0.8162\n",
      "Iteration: 22, Epoch: 3, Avg Time per Batch: 0.2620 sec, Loss: 3.42732216, MSE: 0.0367, MAE: 0.1376, RMSE: 0.1915, R2: -0.6464\n",
      "Iteration: 22, Epoch: 4, Avg Time per Batch: 0.2632 sec, Loss: 3.42731200, MSE: 0.0367, MAE: 0.1375, RMSE: 0.1915, R2: -0.5464\n",
      "Iteration: 22, Epoch: 5, Avg Time per Batch: 0.2664 sec, Loss: 3.42719479, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.4811\n",
      "Iteration: 22, Epoch: 6, Avg Time per Batch: 0.2656 sec, Loss: 3.42708764, MSE: 0.0367, MAE: 0.1375, RMSE: 0.1914, R2: -0.4366\n",
      "Iteration: 22, Epoch: 7, Avg Time per Batch: 0.2634 sec, Loss: 3.42691918, MSE: 0.0367, MAE: 0.1375, RMSE: 0.1915, R2: -0.4056\n",
      "Iteration: 22, Epoch: 8, Avg Time per Batch: 0.2668 sec, Loss: 3.42691579, MSE: 0.0367, MAE: 0.1375, RMSE: 0.1915, R2: -0.3794\n",
      "Iteration: 22, Epoch: 9, Avg Time per Batch: 0.2639 sec, Loss: 3.42677245, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.3615\n",
      "Iteration: 22, Epoch: 10, Avg Time per Batch: 0.2635 sec, Loss: 3.42680947, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.3479\n",
      "Iteration: 22, Epoch: 11, Avg Time per Batch: 0.2681 sec, Loss: 3.42675346, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.3383\n",
      "Iteration: 22, Epoch: 12, Avg Time per Batch: 0.2651 sec, Loss: 3.42668893, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.3305\n",
      "Iteration: 22, Epoch: 13, Avg Time per Batch: 0.2641 sec, Loss: 3.42657349, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.3242\n",
      "Iteration: 22, Epoch: 14, Avg Time per Batch: 0.2650 sec, Loss: 3.42653650, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.3202\n",
      "Iteration: 22, Epoch: 15, Avg Time per Batch: 0.2660 sec, Loss: 3.42640356, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.3157\n",
      "Iteration: 22, Epoch: 16, Avg Time per Batch: 0.2664 sec, Loss: 3.42638603, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.3138\n",
      "Iteration: 22, Epoch: 17, Avg Time per Batch: 0.2635 sec, Loss: 3.42629735, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.3119\n",
      "Iteration: 22, Epoch: 18, Avg Time per Batch: 0.2678 sec, Loss: 3.42623446, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.3109\n",
      "Iteration: 22, Epoch: 19, Avg Time per Batch: 0.2643 sec, Loss: 3.42615108, MSE: 0.0366, MAE: 0.1375, RMSE: 0.1914, R2: -0.3106\n",
      "Epoch 19 completed in 1451.60 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 22, Epoch: 19, Loss: 3.4261510816\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is LNW\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5895, 60, 24])\n",
      "Iteration: 23, Epoch: 0, Avg Time per Batch: 0.2641 sec, Loss: 3.42254745, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1885, R2: -0.0522\n",
      "Iteration: 23, Epoch: 1, Avg Time per Batch: 0.2664 sec, Loss: 3.42294789, MSE: 0.0355, MAE: 0.1390, RMSE: 0.1884, R2: -0.0514\n",
      "Iteration: 23, Epoch: 2, Avg Time per Batch: 0.2644 sec, Loss: 3.42308356, MSE: 0.0355, MAE: 0.1390, RMSE: 0.1883, R2: -0.0506\n",
      "Iteration: 23, Epoch: 3, Avg Time per Batch: 0.2663 sec, Loss: 3.42307909, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0509\n",
      "Iteration: 23, Epoch: 4, Avg Time per Batch: 0.2677 sec, Loss: 3.42310753, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0511\n",
      "Iteration: 23, Epoch: 5, Avg Time per Batch: 0.2672 sec, Loss: 3.42304745, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0513\n",
      "Iteration: 23, Epoch: 6, Avg Time per Batch: 0.2649 sec, Loss: 3.42285475, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0507\n",
      "Iteration: 23, Epoch: 7, Avg Time per Batch: 0.2649 sec, Loss: 3.42286026, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0512\n",
      "Iteration: 23, Epoch: 8, Avg Time per Batch: 0.2666 sec, Loss: 3.42266095, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0506\n",
      "Iteration: 23, Epoch: 9, Avg Time per Batch: 0.2668 sec, Loss: 3.42256541, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0499\n",
      "Iteration: 23, Epoch: 10, Avg Time per Batch: 0.2670 sec, Loss: 3.42252328, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0496\n",
      "Iteration: 23, Epoch: 11, Avg Time per Batch: 0.2678 sec, Loss: 3.42243198, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0495\n",
      "Iteration: 23, Epoch: 12, Avg Time per Batch: 0.2697 sec, Loss: 3.42234967, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0495\n",
      "Iteration: 23, Epoch: 13, Avg Time per Batch: 0.2683 sec, Loss: 3.42227295, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0494\n",
      "Iteration: 23, Epoch: 14, Avg Time per Batch: 0.2675 sec, Loss: 3.42217739, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0492\n",
      "Iteration: 23, Epoch: 15, Avg Time per Batch: 0.2691 sec, Loss: 3.42206767, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0488\n",
      "Iteration: 23, Epoch: 16, Avg Time per Batch: 0.2691 sec, Loss: 3.42198224, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0485\n",
      "Iteration: 23, Epoch: 17, Avg Time per Batch: 0.2677 sec, Loss: 3.42183539, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0484\n",
      "Iteration: 23, Epoch: 18, Avg Time per Batch: 0.2676 sec, Loss: 3.42181152, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0484\n",
      "Iteration: 23, Epoch: 19, Avg Time per Batch: 0.2658 sec, Loss: 3.42165575, MSE: 0.0355, MAE: 0.1391, RMSE: 0.1884, R2: -0.0483\n",
      "Epoch 19 completed in 1453.00 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 23, Epoch: 19, Loss: 3.4216557481\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is VMI\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5895, 60, 24])\n",
      "Iteration: 24, Epoch: 0, Avg Time per Batch: 0.2681 sec, Loss: 3.43461641, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2232, R2: -0.0442\n",
      "Iteration: 24, Epoch: 1, Avg Time per Batch: 0.2673 sec, Loss: 3.43434757, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2232, R2: -0.0437\n",
      "Iteration: 24, Epoch: 2, Avg Time per Batch: 0.2674 sec, Loss: 3.43415457, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2232, R2: -0.0453\n",
      "Iteration: 24, Epoch: 3, Avg Time per Batch: 0.2658 sec, Loss: 3.43399392, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0452\n",
      "Iteration: 24, Epoch: 4, Avg Time per Batch: 0.2676 sec, Loss: 3.43399424, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0457\n",
      "Iteration: 24, Epoch: 5, Avg Time per Batch: 0.2677 sec, Loss: 3.43370216, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0457\n",
      "Iteration: 24, Epoch: 6, Avg Time per Batch: 0.2676 sec, Loss: 3.43367007, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0458\n",
      "Iteration: 24, Epoch: 7, Avg Time per Batch: 0.2677 sec, Loss: 3.43348634, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0460\n",
      "Iteration: 24, Epoch: 8, Avg Time per Batch: 0.2672 sec, Loss: 3.43332424, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2232, R2: -0.0456\n",
      "Iteration: 24, Epoch: 9, Avg Time per Batch: 0.2666 sec, Loss: 3.43322073, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0457\n",
      "Iteration: 24, Epoch: 10, Avg Time per Batch: 0.2684 sec, Loss: 3.43309216, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0457\n",
      "Iteration: 24, Epoch: 11, Avg Time per Batch: 0.2675 sec, Loss: 3.43303588, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0456\n",
      "Iteration: 24, Epoch: 12, Avg Time per Batch: 0.2691 sec, Loss: 3.43288159, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0455\n",
      "Iteration: 24, Epoch: 13, Avg Time per Batch: 0.2661 sec, Loss: 3.43279148, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0452\n",
      "Iteration: 24, Epoch: 14, Avg Time per Batch: 0.2672 sec, Loss: 3.43266657, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0451\n",
      "Iteration: 24, Epoch: 15, Avg Time per Batch: 0.2680 sec, Loss: 3.43253213, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0452\n",
      "Iteration: 24, Epoch: 16, Avg Time per Batch: 0.2671 sec, Loss: 3.43241208, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0451\n",
      "Iteration: 24, Epoch: 17, Avg Time per Batch: 0.2677 sec, Loss: 3.43229931, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0450\n",
      "Iteration: 24, Epoch: 18, Avg Time per Batch: 0.2661 sec, Loss: 3.43219901, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0448\n",
      "Iteration: 24, Epoch: 19, Avg Time per Batch: 0.2666 sec, Loss: 3.43210361, MSE: 0.0498, MAE: 0.1710, RMSE: 0.2231, R2: -0.0449\n",
      "Epoch 19 completed in 1450.39 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 24, Epoch: 19, Loss: 3.4321036085\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is RRX\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5895, 60, 24])\n",
      "Iteration: 25, Epoch: 0, Avg Time per Batch: 0.2674 sec, Loss: 3.42761023, MSE: 0.0485, MAE: 0.1692, RMSE: 0.2203, R2: -0.0457\n",
      "Iteration: 25, Epoch: 1, Avg Time per Batch: 0.2667 sec, Loss: 3.42785953, MSE: 0.0485, MAE: 0.1692, RMSE: 0.2202, R2: -0.0451\n",
      "Iteration: 25, Epoch: 2, Avg Time per Batch: 0.2684 sec, Loss: 3.42798491, MSE: 0.0485, MAE: 0.1692, RMSE: 0.2202, R2: -0.0453\n",
      "Iteration: 25, Epoch: 3, Avg Time per Batch: 0.2674 sec, Loss: 3.42776035, MSE: 0.0485, MAE: 0.1692, RMSE: 0.2202, R2: -0.0461\n",
      "Iteration: 25, Epoch: 4, Avg Time per Batch: 0.2655 sec, Loss: 3.42795834, MSE: 0.0485, MAE: 0.1692, RMSE: 0.2202, R2: -0.0461\n",
      "Iteration: 25, Epoch: 5, Avg Time per Batch: 0.2666 sec, Loss: 3.42771792, MSE: 0.0485, MAE: 0.1692, RMSE: 0.2202, R2: -0.0468\n",
      "Iteration: 25, Epoch: 6, Avg Time per Batch: 0.2674 sec, Loss: 3.42770474, MSE: 0.0485, MAE: 0.1692, RMSE: 0.2202, R2: -0.0472\n",
      "Iteration: 25, Epoch: 7, Avg Time per Batch: 0.2669 sec, Loss: 3.42753191, MSE: 0.0485, MAE: 0.1692, RMSE: 0.2201, R2: -0.0465\n",
      "Iteration: 25, Epoch: 8, Avg Time per Batch: 0.2667 sec, Loss: 3.42736603, MSE: 0.0485, MAE: 0.1692, RMSE: 0.2201, R2: -0.0463\n",
      "Iteration: 25, Epoch: 9, Avg Time per Batch: 0.2659 sec, Loss: 3.42723631, MSE: 0.0485, MAE: 0.1692, RMSE: 0.2201, R2: -0.0465\n",
      "Iteration: 25, Epoch: 10, Avg Time per Batch: 0.2680 sec, Loss: 3.42716085, MSE: 0.0484, MAE: 0.1692, RMSE: 0.2201, R2: -0.0460\n",
      "Iteration: 25, Epoch: 11, Avg Time per Batch: 0.2677 sec, Loss: 3.42700818, MSE: 0.0484, MAE: 0.1692, RMSE: 0.2201, R2: -0.0457\n",
      "Iteration: 25, Epoch: 12, Avg Time per Batch: 0.2685 sec, Loss: 3.42697767, MSE: 0.0484, MAE: 0.1692, RMSE: 0.2201, R2: -0.0453\n",
      "Iteration: 25, Epoch: 13, Avg Time per Batch: 0.2674 sec, Loss: 3.42681195, MSE: 0.0484, MAE: 0.1692, RMSE: 0.2201, R2: -0.0452\n",
      "Iteration: 25, Epoch: 14, Avg Time per Batch: 0.2669 sec, Loss: 3.42671742, MSE: 0.0484, MAE: 0.1692, RMSE: 0.2201, R2: -0.0455\n",
      "Iteration: 25, Epoch: 15, Avg Time per Batch: 0.2672 sec, Loss: 3.42661799, MSE: 0.0484, MAE: 0.1692, RMSE: 0.2201, R2: -0.0455\n",
      "Iteration: 25, Epoch: 16, Avg Time per Batch: 0.2674 sec, Loss: 3.42645255, MSE: 0.0484, MAE: 0.1692, RMSE: 0.2201, R2: -0.0456\n",
      "Iteration: 25, Epoch: 17, Avg Time per Batch: 0.2680 sec, Loss: 3.42634970, MSE: 0.0484, MAE: 0.1692, RMSE: 0.2201, R2: -0.0455\n",
      "Iteration: 25, Epoch: 18, Avg Time per Batch: 0.2675 sec, Loss: 3.42625143, MSE: 0.0484, MAE: 0.1692, RMSE: 0.2201, R2: -0.0455\n",
      "Iteration: 25, Epoch: 19, Avg Time per Batch: 0.2645 sec, Loss: 3.42610195, MSE: 0.0484, MAE: 0.1692, RMSE: 0.2201, R2: -0.0456\n",
      "Epoch 19 completed in 1448.32 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 25, Epoch: 19, Loss: 3.4261019486\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is OMQS\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([1221, 60, 24])\n",
      "Iteration: 26, Epoch: 0, Avg Time per Batch: 0.2722 sec, Loss: 3.42823699, MSE: 0.0486, MAE: 0.1701, RMSE: 0.2204, R2: -0.2054\n",
      "Iteration: 26, Epoch: 1, Avg Time per Batch: 0.2746 sec, Loss: 3.42414662, MSE: 0.0477, MAE: 0.1673, RMSE: 0.2185, R2: -0.1259\n",
      "Iteration: 26, Epoch: 2, Avg Time per Batch: 0.2772 sec, Loss: 3.42379395, MSE: 0.0475, MAE: 0.1665, RMSE: 0.2179, R2: -0.0990\n",
      "Iteration: 26, Epoch: 3, Avg Time per Batch: 0.2761 sec, Loss: 3.42327767, MSE: 0.0473, MAE: 0.1660, RMSE: 0.2175, R2: -0.0849\n",
      "Iteration: 26, Epoch: 4, Avg Time per Batch: 0.2755 sec, Loss: 3.42268578, MSE: 0.0472, MAE: 0.1657, RMSE: 0.2172, R2: -0.0770\n",
      "Iteration: 26, Epoch: 5, Avg Time per Batch: 0.2714 sec, Loss: 3.42283745, MSE: 0.0471, MAE: 0.1655, RMSE: 0.2170, R2: -0.0703\n",
      "Iteration: 26, Epoch: 6, Avg Time per Batch: 0.2734 sec, Loss: 3.42299570, MSE: 0.0471, MAE: 0.1654, RMSE: 0.2169, R2: -0.0670\n",
      "Iteration: 26, Epoch: 7, Avg Time per Batch: 0.2686 sec, Loss: 3.42264995, MSE: 0.0470, MAE: 0.1652, RMSE: 0.2168, R2: -0.0648\n",
      "Iteration: 26, Epoch: 8, Avg Time per Batch: 0.2726 sec, Loss: 3.42226038, MSE: 0.0470, MAE: 0.1651, RMSE: 0.2168, R2: -0.0627\n",
      "Iteration: 26, Epoch: 9, Avg Time per Batch: 0.2773 sec, Loss: 3.42235824, MSE: 0.0470, MAE: 0.1652, RMSE: 0.2168, R2: -0.0613\n",
      "Iteration: 26, Epoch: 10, Avg Time per Batch: 0.2732 sec, Loss: 3.42219541, MSE: 0.0470, MAE: 0.1651, RMSE: 0.2168, R2: -0.0602\n",
      "Iteration: 26, Epoch: 11, Avg Time per Batch: 0.2755 sec, Loss: 3.42249364, MSE: 0.0470, MAE: 0.1650, RMSE: 0.2167, R2: -0.0591\n",
      "Iteration: 26, Epoch: 12, Avg Time per Batch: 0.2782 sec, Loss: 3.42228902, MSE: 0.0469, MAE: 0.1650, RMSE: 0.2167, R2: -0.0575\n",
      "Iteration: 26, Epoch: 13, Avg Time per Batch: 0.2745 sec, Loss: 3.42219002, MSE: 0.0469, MAE: 0.1649, RMSE: 0.2166, R2: -0.0566\n",
      "Iteration: 26, Epoch: 14, Avg Time per Batch: 0.2682 sec, Loss: 3.42227741, MSE: 0.0469, MAE: 0.1649, RMSE: 0.2166, R2: -0.0557\n",
      "Iteration: 26, Epoch: 15, Avg Time per Batch: 0.2762 sec, Loss: 3.42215639, MSE: 0.0469, MAE: 0.1649, RMSE: 0.2166, R2: -0.0555\n",
      "Iteration: 26, Epoch: 16, Avg Time per Batch: 0.2671 sec, Loss: 3.42178568, MSE: 0.0469, MAE: 0.1648, RMSE: 0.2165, R2: -0.0552\n",
      "Iteration: 26, Epoch: 17, Avg Time per Batch: 0.2702 sec, Loss: 3.42187424, MSE: 0.0469, MAE: 0.1648, RMSE: 0.2165, R2: -0.0545\n",
      "Iteration: 26, Epoch: 18, Avg Time per Batch: 0.2759 sec, Loss: 3.42211153, MSE: 0.0469, MAE: 0.1648, RMSE: 0.2165, R2: -0.0541\n",
      "Iteration: 26, Epoch: 19, Avg Time per Batch: 0.2685 sec, Loss: 3.42204528, MSE: 0.0469, MAE: 0.1648, RMSE: 0.2165, R2: -0.0536\n",
      "Epoch 19 completed in 555.54 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 26, Epoch: 19, Loss: 3.4220452773\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is ASX\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([4191, 60, 24])\n",
      "Iteration: 27, Epoch: 0, Avg Time per Batch: 0.2636 sec, Loss: 3.41811728, MSE: 0.0445, MAE: 0.1582, RMSE: 0.2109, R2: -0.0710\n",
      "Iteration: 27, Epoch: 1, Avg Time per Batch: 0.2642 sec, Loss: 3.41861068, MSE: 0.0443, MAE: 0.1577, RMSE: 0.2104, R2: -0.0588\n",
      "Iteration: 27, Epoch: 2, Avg Time per Batch: 0.2667 sec, Loss: 3.41833378, MSE: 0.0442, MAE: 0.1576, RMSE: 0.2102, R2: -0.0533\n",
      "Iteration: 27, Epoch: 3, Avg Time per Batch: 0.2658 sec, Loss: 3.41851069, MSE: 0.0442, MAE: 0.1575, RMSE: 0.2102, R2: -0.0517\n",
      "Iteration: 27, Epoch: 4, Avg Time per Batch: 0.2706 sec, Loss: 3.41811278, MSE: 0.0442, MAE: 0.1575, RMSE: 0.2102, R2: -0.0499\n",
      "Iteration: 27, Epoch: 5, Avg Time per Batch: 0.2642 sec, Loss: 3.41834972, MSE: 0.0441, MAE: 0.1574, RMSE: 0.2101, R2: -0.0492\n",
      "Iteration: 27, Epoch: 6, Avg Time per Batch: 0.2631 sec, Loss: 3.41821690, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0491\n",
      "Iteration: 27, Epoch: 7, Avg Time per Batch: 0.2622 sec, Loss: 3.41821513, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0488\n",
      "Iteration: 27, Epoch: 8, Avg Time per Batch: 0.2721 sec, Loss: 3.41816629, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0485\n",
      "Iteration: 27, Epoch: 9, Avg Time per Batch: 0.2667 sec, Loss: 3.41809125, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0480\n",
      "Iteration: 27, Epoch: 10, Avg Time per Batch: 0.2682 sec, Loss: 3.41805086, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0476\n",
      "Iteration: 27, Epoch: 11, Avg Time per Batch: 0.2685 sec, Loss: 3.41799243, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0475\n",
      "Iteration: 27, Epoch: 12, Avg Time per Batch: 0.2676 sec, Loss: 3.41798286, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0472\n",
      "Iteration: 27, Epoch: 13, Avg Time per Batch: 0.2663 sec, Loss: 3.41786445, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0470\n",
      "Iteration: 27, Epoch: 14, Avg Time per Batch: 0.2672 sec, Loss: 3.41783590, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0465\n",
      "Iteration: 27, Epoch: 15, Avg Time per Batch: 0.2623 sec, Loss: 3.41785077, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0462\n",
      "Iteration: 27, Epoch: 16, Avg Time per Batch: 0.2696 sec, Loss: 3.41769251, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0460\n",
      "Iteration: 27, Epoch: 17, Avg Time per Batch: 0.2684 sec, Loss: 3.41764230, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0457\n",
      "Iteration: 27, Epoch: 18, Avg Time per Batch: 0.2632 sec, Loss: 3.41755879, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0455\n",
      "Iteration: 27, Epoch: 19, Avg Time per Batch: 0.2663 sec, Loss: 3.41757851, MSE: 0.0441, MAE: 0.1573, RMSE: 0.2100, R2: -0.0455\n",
      "Epoch 19 completed in 1123.04 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 27, Epoch: 19, Loss: 3.4175785073\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is CUK\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([4176, 60, 24])\n",
      "Iteration: 28, Epoch: 0, Avg Time per Batch: 0.2684 sec, Loss: 3.40747004, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1903, R2: -0.0470\n",
      "Iteration: 28, Epoch: 1, Avg Time per Batch: 0.2665 sec, Loss: 3.40816965, MSE: 0.0361, MAE: 0.1390, RMSE: 0.1901, R2: -0.0464\n",
      "Iteration: 28, Epoch: 2, Avg Time per Batch: 0.2632 sec, Loss: 3.40817829, MSE: 0.0361, MAE: 0.1390, RMSE: 0.1901, R2: -0.0468\n",
      "Iteration: 28, Epoch: 3, Avg Time per Batch: 0.2674 sec, Loss: 3.40827693, MSE: 0.0362, MAE: 0.1391, RMSE: 0.1901, R2: -0.0475\n",
      "Iteration: 28, Epoch: 4, Avg Time per Batch: 0.2669 sec, Loss: 3.40799467, MSE: 0.0361, MAE: 0.1391, RMSE: 0.1901, R2: -0.0463\n",
      "Iteration: 28, Epoch: 5, Avg Time per Batch: 0.2677 sec, Loss: 3.40819279, MSE: 0.0362, MAE: 0.1391, RMSE: 0.1901, R2: -0.0462\n",
      "Iteration: 28, Epoch: 6, Avg Time per Batch: 0.2633 sec, Loss: 3.40798152, MSE: 0.0362, MAE: 0.1391, RMSE: 0.1902, R2: -0.0467\n",
      "Iteration: 28, Epoch: 7, Avg Time per Batch: 0.2717 sec, Loss: 3.40793495, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0486\n",
      "Iteration: 28, Epoch: 8, Avg Time per Batch: 0.2622 sec, Loss: 3.40794422, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0490\n",
      "Iteration: 28, Epoch: 9, Avg Time per Batch: 0.2701 sec, Loss: 3.40781759, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0486\n",
      "Iteration: 28, Epoch: 10, Avg Time per Batch: 0.2338 sec, Loss: 3.40778854, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0481\n",
      "Iteration: 28, Epoch: 11, Avg Time per Batch: 0.2368 sec, Loss: 3.40769005, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0478\n",
      "Iteration: 28, Epoch: 12, Avg Time per Batch: 0.2387 sec, Loss: 3.40762432, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0474\n",
      "Iteration: 28, Epoch: 13, Avg Time per Batch: 0.2388 sec, Loss: 3.40759028, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0474\n",
      "Iteration: 28, Epoch: 14, Avg Time per Batch: 0.2401 sec, Loss: 3.40747987, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0473\n",
      "Iteration: 28, Epoch: 15, Avg Time per Batch: 0.2392 sec, Loss: 3.40750519, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0472\n",
      "Iteration: 28, Epoch: 16, Avg Time per Batch: 0.2399 sec, Loss: 3.40737398, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0474\n",
      "Iteration: 28, Epoch: 17, Avg Time per Batch: 0.2400 sec, Loss: 3.40729761, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0479\n",
      "Iteration: 28, Epoch: 18, Avg Time per Batch: 0.2392 sec, Loss: 3.40729894, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0476\n",
      "Iteration: 28, Epoch: 19, Avg Time per Batch: 0.2395 sec, Loss: 3.40724314, MSE: 0.0362, MAE: 0.1392, RMSE: 0.1902, R2: -0.0475\n",
      "Epoch 19 completed in 1013.70 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 28, Epoch: 19, Loss: 3.4072431435\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is MOFG\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([2352, 60, 24])\n",
      "Iteration: 29, Epoch: 0, Avg Time per Batch: 0.2434 sec, Loss: 3.41877668, MSE: 0.0504, MAE: 0.1697, RMSE: 0.2244, R2: -0.1285\n",
      "Iteration: 29, Epoch: 1, Avg Time per Batch: 0.2402 sec, Loss: 3.41953366, MSE: 0.0502, MAE: 0.1691, RMSE: 0.2241, R2: -0.0909\n",
      "Iteration: 29, Epoch: 2, Avg Time per Batch: 0.2413 sec, Loss: 3.41957313, MSE: 0.0503, MAE: 0.1692, RMSE: 0.2242, R2: -0.0785\n",
      "Iteration: 29, Epoch: 3, Avg Time per Batch: 0.2401 sec, Loss: 3.41942894, MSE: 0.0503, MAE: 0.1692, RMSE: 0.2243, R2: -0.0723\n",
      "Iteration: 29, Epoch: 4, Avg Time per Batch: 0.2413 sec, Loss: 3.41954686, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2242, R2: -0.0691\n",
      "Iteration: 29, Epoch: 5, Avg Time per Batch: 0.2401 sec, Loss: 3.41950884, MSE: 0.0502, MAE: 0.1691, RMSE: 0.2241, R2: -0.0653\n",
      "Iteration: 29, Epoch: 6, Avg Time per Batch: 0.2398 sec, Loss: 3.41957198, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2242, R2: -0.0638\n",
      "Iteration: 29, Epoch: 7, Avg Time per Batch: 0.2411 sec, Loss: 3.41956687, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2242, R2: -0.0619\n",
      "Iteration: 29, Epoch: 8, Avg Time per Batch: 0.2405 sec, Loss: 3.41939845, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2242, R2: -0.0613\n",
      "Iteration: 29, Epoch: 9, Avg Time per Batch: 0.2397 sec, Loss: 3.41949391, MSE: 0.0503, MAE: 0.1690, RMSE: 0.2242, R2: -0.0614\n",
      "Iteration: 29, Epoch: 10, Avg Time per Batch: 0.2393 sec, Loss: 3.41937985, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2243, R2: -0.0608\n",
      "Iteration: 29, Epoch: 11, Avg Time per Batch: 0.2329 sec, Loss: 3.41934634, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2243, R2: -0.0608\n",
      "Iteration: 29, Epoch: 12, Avg Time per Batch: 0.2396 sec, Loss: 3.41942351, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2243, R2: -0.0615\n",
      "Iteration: 29, Epoch: 13, Avg Time per Batch: 0.2356 sec, Loss: 3.41925995, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2243, R2: -0.0610\n",
      "Iteration: 29, Epoch: 14, Avg Time per Batch: 0.2356 sec, Loss: 3.41935208, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2243, R2: -0.0606\n",
      "Iteration: 29, Epoch: 15, Avg Time per Batch: 0.2369 sec, Loss: 3.41923254, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2243, R2: -0.0604\n",
      "Iteration: 29, Epoch: 16, Avg Time per Batch: 0.2377 sec, Loss: 3.41925947, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2243, R2: -0.0601\n",
      "Iteration: 29, Epoch: 17, Avg Time per Batch: 0.2361 sec, Loss: 3.41923951, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2243, R2: -0.0606\n",
      "Iteration: 29, Epoch: 18, Avg Time per Batch: 0.2370 sec, Loss: 3.41908006, MSE: 0.0503, MAE: 0.1691, RMSE: 0.2243, R2: -0.0602\n",
      "Iteration: 29, Epoch: 19, Avg Time per Batch: 0.2393 sec, Loss: 3.41921100, MSE: 0.0503, MAE: 0.1692, RMSE: 0.2243, R2: -0.0599\n",
      "Epoch 19 completed in 608.37 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 29, Epoch: 19, Loss: 3.4192110029\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is ATGE\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5895, 60, 24])\n",
      "Iteration: 30, Epoch: 0, Avg Time per Batch: 0.2355 sec, Loss: 3.40457534, MSE: 0.0377, MAE: 0.1449, RMSE: 0.1941, R2: -0.0504\n",
      "Iteration: 30, Epoch: 1, Avg Time per Batch: 0.2331 sec, Loss: 3.40500858, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0474\n",
      "Iteration: 30, Epoch: 2, Avg Time per Batch: 0.2341 sec, Loss: 3.40483092, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0480\n",
      "Iteration: 30, Epoch: 3, Avg Time per Batch: 0.2339 sec, Loss: 3.40512101, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0489\n",
      "Iteration: 30, Epoch: 4, Avg Time per Batch: 0.2337 sec, Loss: 3.40500960, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0487\n",
      "Iteration: 30, Epoch: 5, Avg Time per Batch: 0.2348 sec, Loss: 3.40495696, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0485\n",
      "Iteration: 30, Epoch: 6, Avg Time per Batch: 0.2346 sec, Loss: 3.40487973, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0480\n",
      "Iteration: 30, Epoch: 7, Avg Time per Batch: 0.2341 sec, Loss: 3.40476766, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0480\n",
      "Iteration: 30, Epoch: 8, Avg Time per Batch: 0.2350 sec, Loss: 3.40465948, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0481\n",
      "Iteration: 30, Epoch: 9, Avg Time per Batch: 0.2348 sec, Loss: 3.40452529, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0485\n",
      "Iteration: 30, Epoch: 10, Avg Time per Batch: 0.2354 sec, Loss: 3.40444995, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0483\n",
      "Iteration: 30, Epoch: 11, Avg Time per Batch: 0.2344 sec, Loss: 3.40437725, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0479\n",
      "Iteration: 30, Epoch: 12, Avg Time per Batch: 0.2345 sec, Loss: 3.40424635, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0481\n",
      "Iteration: 30, Epoch: 13, Avg Time per Batch: 0.2341 sec, Loss: 3.40411839, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0478\n",
      "Iteration: 30, Epoch: 14, Avg Time per Batch: 0.2352 sec, Loss: 3.40400020, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0476\n",
      "Iteration: 30, Epoch: 15, Avg Time per Batch: 0.2343 sec, Loss: 3.40391712, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0478\n",
      "Iteration: 30, Epoch: 16, Avg Time per Batch: 0.2342 sec, Loss: 3.40382720, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0478\n",
      "Iteration: 30, Epoch: 17, Avg Time per Batch: 0.2342 sec, Loss: 3.40372269, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0479\n",
      "Iteration: 30, Epoch: 18, Avg Time per Batch: 0.2353 sec, Loss: 3.40364250, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0479\n",
      "Iteration: 30, Epoch: 19, Avg Time per Batch: 0.2350 sec, Loss: 3.40354368, MSE: 0.0376, MAE: 0.1448, RMSE: 0.1940, R2: -0.0478\n",
      "Epoch 19 completed in 1165.08 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 30, Epoch: 19, Loss: 3.4035436796\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is KFY\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([4605, 60, 24])\n",
      "Iteration: 31, Epoch: 0, Avg Time per Batch: 0.2364 sec, Loss: 3.39980775, MSE: 0.0369, MAE: 0.1424, RMSE: 0.1920, R2: -0.0515\n",
      "Iteration: 31, Epoch: 1, Avg Time per Batch: 0.2360 sec, Loss: 3.39999261, MSE: 0.0369, MAE: 0.1424, RMSE: 0.1920, R2: -0.0515\n",
      "Iteration: 31, Epoch: 2, Avg Time per Batch: 0.2356 sec, Loss: 3.39991291, MSE: 0.0368, MAE: 0.1423, RMSE: 0.1919, R2: -0.0501\n",
      "Iteration: 31, Epoch: 3, Avg Time per Batch: 0.2364 sec, Loss: 3.40012665, MSE: 0.0368, MAE: 0.1423, RMSE: 0.1920, R2: -0.0493\n",
      "Iteration: 31, Epoch: 4, Avg Time per Batch: 0.2353 sec, Loss: 3.39991860, MSE: 0.0368, MAE: 0.1423, RMSE: 0.1919, R2: -0.0482\n",
      "Iteration: 31, Epoch: 5, Avg Time per Batch: 0.2361 sec, Loss: 3.40001509, MSE: 0.0368, MAE: 0.1423, RMSE: 0.1919, R2: -0.0482\n",
      "Iteration: 31, Epoch: 6, Avg Time per Batch: 0.2350 sec, Loss: 3.39989825, MSE: 0.0368, MAE: 0.1423, RMSE: 0.1919, R2: -0.0474\n",
      "Iteration: 31, Epoch: 7, Avg Time per Batch: 0.2339 sec, Loss: 3.39987899, MSE: 0.0368, MAE: 0.1423, RMSE: 0.1920, R2: -0.0480\n",
      "Iteration: 31, Epoch: 8, Avg Time per Batch: 0.2355 sec, Loss: 3.39979203, MSE: 0.0368, MAE: 0.1423, RMSE: 0.1919, R2: -0.0478\n",
      "Iteration: 31, Epoch: 9, Avg Time per Batch: 0.2355 sec, Loss: 3.39978026, MSE: 0.0368, MAE: 0.1423, RMSE: 0.1920, R2: -0.0475\n",
      "Iteration: 31, Epoch: 10, Avg Time per Batch: 0.2353 sec, Loss: 3.39967763, MSE: 0.0369, MAE: 0.1423, RMSE: 0.1920, R2: -0.0474\n",
      "Iteration: 31, Epoch: 11, Avg Time per Batch: 0.2355 sec, Loss: 3.39967653, MSE: 0.0369, MAE: 0.1423, RMSE: 0.1920, R2: -0.0480\n",
      "Iteration: 31, Epoch: 12, Avg Time per Batch: 0.2352 sec, Loss: 3.39953764, MSE: 0.0369, MAE: 0.1423, RMSE: 0.1920, R2: -0.0478\n",
      "Iteration: 31, Epoch: 13, Avg Time per Batch: 0.2356 sec, Loss: 3.39944339, MSE: 0.0369, MAE: 0.1423, RMSE: 0.1920, R2: -0.0478\n",
      "Iteration: 31, Epoch: 14, Avg Time per Batch: 0.2346 sec, Loss: 3.39942375, MSE: 0.0369, MAE: 0.1423, RMSE: 0.1920, R2: -0.0480\n",
      "Iteration: 31, Epoch: 15, Avg Time per Batch: 0.2352 sec, Loss: 3.39931093, MSE: 0.0369, MAE: 0.1423, RMSE: 0.1920, R2: -0.0478\n",
      "Iteration: 31, Epoch: 16, Avg Time per Batch: 0.2346 sec, Loss: 3.39927018, MSE: 0.0369, MAE: 0.1423, RMSE: 0.1920, R2: -0.0477\n",
      "Iteration: 31, Epoch: 17, Avg Time per Batch: 0.2348 sec, Loss: 3.39924234, MSE: 0.0369, MAE: 0.1423, RMSE: 0.1920, R2: -0.0479\n",
      "Iteration: 31, Epoch: 18, Avg Time per Batch: 0.2354 sec, Loss: 3.39912322, MSE: 0.0369, MAE: 0.1423, RMSE: 0.1920, R2: -0.0476\n",
      "Iteration: 31, Epoch: 19, Avg Time per Batch: 0.2352 sec, Loss: 3.39908299, MSE: 0.0369, MAE: 0.1423, RMSE: 0.1920, R2: -0.0473\n",
      "Epoch 19 completed in 955.26 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 31, Epoch: 19, Loss: 3.3990829935\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is FBIZ\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([2908, 60, 24])\n",
      "Iteration: 32, Epoch: 0, Avg Time per Batch: 0.2381 sec, Loss: 3.41351483, MSE: 0.0540, MAE: 0.1771, RMSE: 0.2324, R2: -0.0747\n",
      "Iteration: 32, Epoch: 1, Avg Time per Batch: 0.2381 sec, Loss: 3.41430317, MSE: 0.0541, MAE: 0.1771, RMSE: 0.2325, R2: -0.0630\n",
      "Iteration: 32, Epoch: 2, Avg Time per Batch: 0.2363 sec, Loss: 3.41444874, MSE: 0.0541, MAE: 0.1772, RMSE: 0.2326, R2: -0.0616\n",
      "Iteration: 32, Epoch: 3, Avg Time per Batch: 0.2373 sec, Loss: 3.41474137, MSE: 0.0541, MAE: 0.1772, RMSE: 0.2326, R2: -0.0628\n",
      "Iteration: 32, Epoch: 4, Avg Time per Batch: 0.2368 sec, Loss: 3.41434955, MSE: 0.0541, MAE: 0.1771, RMSE: 0.2325, R2: -0.0607\n",
      "Iteration: 32, Epoch: 5, Avg Time per Batch: 0.2350 sec, Loss: 3.41429911, MSE: 0.0540, MAE: 0.1770, RMSE: 0.2324, R2: -0.0591\n",
      "Iteration: 32, Epoch: 6, Avg Time per Batch: 0.2367 sec, Loss: 3.41435813, MSE: 0.0540, MAE: 0.1770, RMSE: 0.2324, R2: -0.0586\n",
      "Iteration: 32, Epoch: 7, Avg Time per Batch: 0.2351 sec, Loss: 3.41437754, MSE: 0.0540, MAE: 0.1770, RMSE: 0.2324, R2: -0.0575\n",
      "Iteration: 32, Epoch: 8, Avg Time per Batch: 0.2359 sec, Loss: 3.41436149, MSE: 0.0540, MAE: 0.1770, RMSE: 0.2324, R2: -0.0571\n",
      "Iteration: 32, Epoch: 9, Avg Time per Batch: 0.2354 sec, Loss: 3.41419283, MSE: 0.0540, MAE: 0.1769, RMSE: 0.2323, R2: -0.0566\n",
      "Iteration: 32, Epoch: 10, Avg Time per Batch: 0.2366 sec, Loss: 3.41418149, MSE: 0.0539, MAE: 0.1768, RMSE: 0.2322, R2: -0.0567\n",
      "Iteration: 32, Epoch: 11, Avg Time per Batch: 0.2374 sec, Loss: 3.41402725, MSE: 0.0539, MAE: 0.1769, RMSE: 0.2323, R2: -0.0573\n",
      "Iteration: 32, Epoch: 12, Avg Time per Batch: 0.2368 sec, Loss: 3.41409833, MSE: 0.0539, MAE: 0.1769, RMSE: 0.2323, R2: -0.0573\n",
      "Iteration: 32, Epoch: 13, Avg Time per Batch: 0.2358 sec, Loss: 3.41404839, MSE: 0.0539, MAE: 0.1768, RMSE: 0.2322, R2: -0.0570\n",
      "Iteration: 32, Epoch: 14, Avg Time per Batch: 0.2376 sec, Loss: 3.41406739, MSE: 0.0539, MAE: 0.1768, RMSE: 0.2322, R2: -0.0571\n",
      "Iteration: 32, Epoch: 15, Avg Time per Batch: 0.2351 sec, Loss: 3.41395989, MSE: 0.0539, MAE: 0.1768, RMSE: 0.2322, R2: -0.0566\n",
      "Iteration: 32, Epoch: 16, Avg Time per Batch: 0.2381 sec, Loss: 3.41391285, MSE: 0.0539, MAE: 0.1768, RMSE: 0.2322, R2: -0.0569\n",
      "Iteration: 32, Epoch: 17, Avg Time per Batch: 0.2348 sec, Loss: 3.41388894, MSE: 0.0539, MAE: 0.1769, RMSE: 0.2323, R2: -0.0574\n",
      "Iteration: 32, Epoch: 18, Avg Time per Batch: 0.2349 sec, Loss: 3.41381715, MSE: 0.0539, MAE: 0.1769, RMSE: 0.2323, R2: -0.0571\n",
      "Iteration: 32, Epoch: 19, Avg Time per Batch: 0.2353 sec, Loss: 3.41386145, MSE: 0.0539, MAE: 0.1768, RMSE: 0.2322, R2: -0.0573\n",
      "Epoch 19 completed in 688.05 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 32, Epoch: 19, Loss: 3.4138614546\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is TEAM\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([370, 60, 24])\n",
      "Iteration: 33, Epoch: 0, Avg Time per Batch: 0.2631 sec, Loss: 3.37907646, MSE: 0.0419, MAE: 0.1566, RMSE: 0.2047, R2: -0.0891\n",
      "Iteration: 33, Epoch: 1, Avg Time per Batch: 0.2555 sec, Loss: 3.39668436, MSE: 0.0425, MAE: 0.1573, RMSE: 0.2062, R2: -0.0753\n",
      "Iteration: 33, Epoch: 2, Avg Time per Batch: 0.2565 sec, Loss: 3.39806668, MSE: 0.0427, MAE: 0.1575, RMSE: 0.2065, R2: -0.0640\n",
      "Iteration: 33, Epoch: 3, Avg Time per Batch: 0.2543 sec, Loss: 3.40024112, MSE: 0.0427, MAE: 0.1575, RMSE: 0.2066, R2: -0.0578\n",
      "Iteration: 33, Epoch: 4, Avg Time per Batch: 0.2635 sec, Loss: 3.39665575, MSE: 0.0424, MAE: 0.1571, RMSE: 0.2060, R2: -0.0523\n",
      "Iteration: 33, Epoch: 5, Avg Time per Batch: 0.2581 sec, Loss: 3.40157481, MSE: 0.0424, MAE: 0.1568, RMSE: 0.2058, R2: -0.0505\n",
      "Iteration: 33, Epoch: 6, Avg Time per Batch: 0.2619 sec, Loss: 3.39933194, MSE: 0.0424, MAE: 0.1570, RMSE: 0.2060, R2: -0.0475\n",
      "Iteration: 33, Epoch: 7, Avg Time per Batch: 0.2597 sec, Loss: 3.39986006, MSE: 0.0425, MAE: 0.1571, RMSE: 0.2062, R2: -0.0476\n",
      "Iteration: 33, Epoch: 8, Avg Time per Batch: 0.2559 sec, Loss: 3.40020405, MSE: 0.0426, MAE: 0.1573, RMSE: 0.2064, R2: -0.0474\n",
      "Iteration: 33, Epoch: 9, Avg Time per Batch: 0.2591 sec, Loss: 3.39952952, MSE: 0.0425, MAE: 0.1572, RMSE: 0.2063, R2: -0.0467\n",
      "Iteration: 33, Epoch: 10, Avg Time per Batch: 0.2649 sec, Loss: 3.40044077, MSE: 0.0424, MAE: 0.1569, RMSE: 0.2059, R2: -0.0466\n",
      "Iteration: 33, Epoch: 11, Avg Time per Batch: 0.2579 sec, Loss: 3.40135480, MSE: 0.0423, MAE: 0.1568, RMSE: 0.2058, R2: -0.0457\n",
      "Iteration: 33, Epoch: 12, Avg Time per Batch: 0.2566 sec, Loss: 3.39918283, MSE: 0.0423, MAE: 0.1568, RMSE: 0.2057, R2: -0.0456\n",
      "Iteration: 33, Epoch: 13, Avg Time per Batch: 0.2605 sec, Loss: 3.40071195, MSE: 0.0423, MAE: 0.1567, RMSE: 0.2057, R2: -0.0451\n",
      "Iteration: 33, Epoch: 14, Avg Time per Batch: 0.2595 sec, Loss: 3.40028076, MSE: 0.0423, MAE: 0.1567, RMSE: 0.2056, R2: -0.0443\n",
      "Iteration: 33, Epoch: 15, Avg Time per Batch: 0.2602 sec, Loss: 3.40073370, MSE: 0.0423, MAE: 0.1568, RMSE: 0.2057, R2: -0.0439\n",
      "Iteration: 33, Epoch: 16, Avg Time per Batch: 0.2592 sec, Loss: 3.40002517, MSE: 0.0423, MAE: 0.1567, RMSE: 0.2057, R2: -0.0434\n",
      "Iteration: 33, Epoch: 17, Avg Time per Batch: 0.2534 sec, Loss: 3.40117560, MSE: 0.0423, MAE: 0.1568, RMSE: 0.2057, R2: -0.0423\n",
      "Iteration: 33, Epoch: 18, Avg Time per Batch: 0.2592 sec, Loss: 3.40013891, MSE: 0.0424, MAE: 0.1569, RMSE: 0.2059, R2: -0.0422\n",
      "Iteration: 33, Epoch: 19, Avg Time per Batch: 0.2569 sec, Loss: 3.40092831, MSE: 0.0424, MAE: 0.1570, RMSE: 0.2059, R2: -0.0419\n",
      "Epoch 19 completed in 288.37 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 33, Epoch: 19, Loss: 3.4009283113\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is INDV\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([610, 60, 24])\n",
      "Iteration: 34, Epoch: 0, Avg Time per Batch: 0.2541 sec, Loss: 3.40648635, MSE: 0.0553, MAE: 0.1830, RMSE: 0.2353, R2: -0.4358\n",
      "Iteration: 34, Epoch: 1, Avg Time per Batch: 0.2435 sec, Loss: 3.41606677, MSE: 0.0543, MAE: 0.1805, RMSE: 0.2330, R2: -0.2642\n",
      "Iteration: 34, Epoch: 2, Avg Time per Batch: 0.2541 sec, Loss: 3.41095740, MSE: 0.0540, MAE: 0.1800, RMSE: 0.2324, R2: -0.1992\n",
      "Iteration: 34, Epoch: 3, Avg Time per Batch: 0.2444 sec, Loss: 3.41205287, MSE: 0.0538, MAE: 0.1795, RMSE: 0.2320, R2: -0.1656\n",
      "Iteration: 34, Epoch: 4, Avg Time per Batch: 0.2450 sec, Loss: 3.41088660, MSE: 0.0537, MAE: 0.1792, RMSE: 0.2317, R2: -0.1439\n",
      "Iteration: 34, Epoch: 5, Avg Time per Batch: 0.2528 sec, Loss: 3.41157379, MSE: 0.0536, MAE: 0.1789, RMSE: 0.2314, R2: -0.1287\n",
      "Iteration: 34, Epoch: 6, Avg Time per Batch: 0.2462 sec, Loss: 3.41183792, MSE: 0.0534, MAE: 0.1786, RMSE: 0.2311, R2: -0.1205\n",
      "Iteration: 34, Epoch: 7, Avg Time per Batch: 0.2453 sec, Loss: 3.41168459, MSE: 0.0532, MAE: 0.1783, RMSE: 0.2307, R2: -0.1133\n",
      "Iteration: 34, Epoch: 8, Avg Time per Batch: 0.2450 sec, Loss: 3.41192336, MSE: 0.0532, MAE: 0.1782, RMSE: 0.2307, R2: -0.1069\n",
      "Iteration: 34, Epoch: 9, Avg Time per Batch: 0.2453 sec, Loss: 3.41148838, MSE: 0.0531, MAE: 0.1779, RMSE: 0.2305, R2: -0.1020\n",
      "Iteration: 34, Epoch: 10, Avg Time per Batch: 0.2450 sec, Loss: 3.41187521, MSE: 0.0532, MAE: 0.1780, RMSE: 0.2306, R2: -0.0971\n",
      "Iteration: 34, Epoch: 11, Avg Time per Batch: 0.2456 sec, Loss: 3.41193429, MSE: 0.0532, MAE: 0.1780, RMSE: 0.2306, R2: -0.0939\n",
      "Iteration: 34, Epoch: 12, Avg Time per Batch: 0.2458 sec, Loss: 3.41145438, MSE: 0.0531, MAE: 0.1779, RMSE: 0.2305, R2: -0.0918\n",
      "Iteration: 34, Epoch: 13, Avg Time per Batch: 0.2494 sec, Loss: 3.41183745, MSE: 0.0531, MAE: 0.1779, RMSE: 0.2304, R2: -0.0889\n",
      "Iteration: 34, Epoch: 14, Avg Time per Batch: 0.2523 sec, Loss: 3.41149347, MSE: 0.0531, MAE: 0.1780, RMSE: 0.2305, R2: -0.0871\n",
      "Iteration: 34, Epoch: 15, Avg Time per Batch: 0.2490 sec, Loss: 3.41159653, MSE: 0.0531, MAE: 0.1779, RMSE: 0.2305, R2: -0.0860\n",
      "Iteration: 34, Epoch: 16, Avg Time per Batch: 0.2498 sec, Loss: 3.41161744, MSE: 0.0531, MAE: 0.1780, RMSE: 0.2305, R2: -0.0842\n",
      "Iteration: 34, Epoch: 17, Avg Time per Batch: 0.2440 sec, Loss: 3.41161369, MSE: 0.0531, MAE: 0.1779, RMSE: 0.2305, R2: -0.0825\n",
      "Iteration: 34, Epoch: 18, Avg Time per Batch: 0.2466 sec, Loss: 3.41179195, MSE: 0.0531, MAE: 0.1780, RMSE: 0.2305, R2: -0.0809\n",
      "Iteration: 34, Epoch: 19, Avg Time per Batch: 0.2503 sec, Loss: 3.41185180, MSE: 0.0531, MAE: 0.1779, RMSE: 0.2305, R2: -0.0801\n",
      "Epoch 19 completed in 329.32 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 34, Epoch: 19, Loss: 3.4118517988\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is GRMN\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([4143, 60, 24])\n",
      "Iteration: 35, Epoch: 0, Avg Time per Batch: 0.2342 sec, Loss: 3.39610557, MSE: 0.0385, MAE: 0.1478, RMSE: 0.1962, R2: -0.0637\n",
      "Iteration: 35, Epoch: 1, Avg Time per Batch: 0.2344 sec, Loss: 3.39654125, MSE: 0.0383, MAE: 0.1470, RMSE: 0.1956, R2: -0.0559\n",
      "Iteration: 35, Epoch: 2, Avg Time per Batch: 0.2344 sec, Loss: 3.39589763, MSE: 0.0382, MAE: 0.1468, RMSE: 0.1953, R2: -0.0531\n",
      "Iteration: 35, Epoch: 3, Avg Time per Batch: 0.2360 sec, Loss: 3.39611839, MSE: 0.0381, MAE: 0.1467, RMSE: 0.1952, R2: -0.0520\n",
      "Iteration: 35, Epoch: 4, Avg Time per Batch: 0.2356 sec, Loss: 3.39597863, MSE: 0.0380, MAE: 0.1466, RMSE: 0.1950, R2: -0.0505\n",
      "Iteration: 35, Epoch: 5, Avg Time per Batch: 0.2352 sec, Loss: 3.39571854, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0490\n",
      "Iteration: 35, Epoch: 6, Avg Time per Batch: 0.2359 sec, Loss: 3.39586395, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0484\n",
      "Iteration: 35, Epoch: 7, Avg Time per Batch: 0.2370 sec, Loss: 3.39566144, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0481\n",
      "Iteration: 35, Epoch: 8, Avg Time per Batch: 0.2355 sec, Loss: 3.39557184, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0477\n",
      "Iteration: 35, Epoch: 9, Avg Time per Batch: 0.2364 sec, Loss: 3.39552103, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0480\n",
      "Iteration: 35, Epoch: 10, Avg Time per Batch: 0.2355 sec, Loss: 3.39542516, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0480\n",
      "Iteration: 35, Epoch: 11, Avg Time per Batch: 0.2366 sec, Loss: 3.39533758, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0476\n",
      "Iteration: 35, Epoch: 12, Avg Time per Batch: 0.2361 sec, Loss: 3.39524979, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0472\n",
      "Iteration: 35, Epoch: 13, Avg Time per Batch: 0.2362 sec, Loss: 3.39517779, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0471\n",
      "Iteration: 35, Epoch: 14, Avg Time per Batch: 0.2370 sec, Loss: 3.39509163, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0469\n",
      "Iteration: 35, Epoch: 15, Avg Time per Batch: 0.2356 sec, Loss: 3.39504118, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0467\n",
      "Iteration: 35, Epoch: 16, Avg Time per Batch: 0.2353 sec, Loss: 3.39497020, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0469\n",
      "Iteration: 35, Epoch: 17, Avg Time per Batch: 0.2366 sec, Loss: 3.39487985, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0470\n",
      "Iteration: 35, Epoch: 18, Avg Time per Batch: 0.2364 sec, Loss: 3.39481583, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0468\n",
      "Iteration: 35, Epoch: 19, Avg Time per Batch: 0.2368 sec, Loss: 3.39478732, MSE: 0.0380, MAE: 0.1465, RMSE: 0.1950, R2: -0.0469\n",
      "Epoch 19 completed in 891.33 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 35, Epoch: 19, Loss: 3.3947873233\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is ANIK\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5895, 60, 24])\n",
      "Iteration: 36, Epoch: 0, Avg Time per Batch: 0.2350 sec, Loss: 3.39629479, MSE: 0.0415, MAE: 0.1455, RMSE: 0.2038, R2: -0.0584\n",
      "Iteration: 36, Epoch: 1, Avg Time per Batch: 0.2360 sec, Loss: 3.39581151, MSE: 0.0415, MAE: 0.1452, RMSE: 0.2036, R2: -0.0532\n",
      "Iteration: 36, Epoch: 2, Avg Time per Batch: 0.2350 sec, Loss: 3.39587921, MSE: 0.0414, MAE: 0.1451, RMSE: 0.2036, R2: -0.0519\n",
      "Iteration: 36, Epoch: 3, Avg Time per Batch: 0.2358 sec, Loss: 3.39586360, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0509\n",
      "Iteration: 36, Epoch: 4, Avg Time per Batch: 0.2355 sec, Loss: 3.39570596, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0512\n",
      "Iteration: 36, Epoch: 5, Avg Time per Batch: 0.2366 sec, Loss: 3.39570467, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0516\n",
      "Iteration: 36, Epoch: 6, Avg Time per Batch: 0.2352 sec, Loss: 3.39564375, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0512\n",
      "Iteration: 36, Epoch: 7, Avg Time per Batch: 0.2365 sec, Loss: 3.39553273, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0508\n",
      "Iteration: 36, Epoch: 8, Avg Time per Batch: 0.2354 sec, Loss: 3.39547998, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0513\n",
      "Iteration: 36, Epoch: 9, Avg Time per Batch: 0.2346 sec, Loss: 3.39527132, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0512\n",
      "Iteration: 36, Epoch: 10, Avg Time per Batch: 0.2364 sec, Loss: 3.39515590, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0506\n",
      "Iteration: 36, Epoch: 11, Avg Time per Batch: 0.2353 sec, Loss: 3.39511843, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0506\n",
      "Iteration: 36, Epoch: 12, Avg Time per Batch: 0.2353 sec, Loss: 3.39493268, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0504\n",
      "Iteration: 36, Epoch: 13, Avg Time per Batch: 0.2358 sec, Loss: 3.39486275, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0506\n",
      "Iteration: 36, Epoch: 14, Avg Time per Batch: 0.2357 sec, Loss: 3.39474389, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0506\n",
      "Iteration: 36, Epoch: 15, Avg Time per Batch: 0.2353 sec, Loss: 3.39462833, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0509\n",
      "Iteration: 36, Epoch: 16, Avg Time per Batch: 0.2359 sec, Loss: 3.39450996, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0510\n",
      "Iteration: 36, Epoch: 17, Avg Time per Batch: 0.2363 sec, Loss: 3.39441163, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0512\n",
      "Iteration: 36, Epoch: 18, Avg Time per Batch: 0.2358 sec, Loss: 3.39427413, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0511\n",
      "Iteration: 36, Epoch: 19, Avg Time per Batch: 0.2353 sec, Loss: 3.39420974, MSE: 0.0414, MAE: 0.1450, RMSE: 0.2035, R2: -0.0512\n",
      "Epoch 19 completed in 1172.04 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 36, Epoch: 19, Loss: 3.3942097359\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is AXR\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5895, 60, 24])\n",
      "Iteration: 37, Epoch: 0, Avg Time per Batch: 0.2360 sec, Loss: 3.37892964, MSE: 0.0296, MAE: 0.1182, RMSE: 0.1722, R2: -0.0706\n",
      "Iteration: 37, Epoch: 1, Avg Time per Batch: 0.2364 sec, Loss: 3.37941620, MSE: 0.0296, MAE: 0.1180, RMSE: 0.1720, R2: -0.0654\n",
      "Iteration: 37, Epoch: 2, Avg Time per Batch: 0.2358 sec, Loss: 3.37925450, MSE: 0.0296, MAE: 0.1179, RMSE: 0.1720, R2: -0.0668\n",
      "Iteration: 37, Epoch: 3, Avg Time per Batch: 0.2347 sec, Loss: 3.37940179, MSE: 0.0296, MAE: 0.1179, RMSE: 0.1720, R2: -0.0664\n",
      "Iteration: 37, Epoch: 4, Avg Time per Batch: 0.2361 sec, Loss: 3.37927025, MSE: 0.0296, MAE: 0.1179, RMSE: 0.1720, R2: -0.0674\n",
      "Iteration: 37, Epoch: 5, Avg Time per Batch: 0.2359 sec, Loss: 3.37921331, MSE: 0.0296, MAE: 0.1179, RMSE: 0.1720, R2: -0.0668\n",
      "Iteration: 37, Epoch: 6, Avg Time per Batch: 0.2361 sec, Loss: 3.37909762, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0654\n",
      "Iteration: 37, Epoch: 7, Avg Time per Batch: 0.2355 sec, Loss: 3.37905790, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0660\n",
      "Iteration: 37, Epoch: 8, Avg Time per Batch: 0.2368 sec, Loss: 3.37892209, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0664\n",
      "Iteration: 37, Epoch: 9, Avg Time per Batch: 0.2354 sec, Loss: 3.37881756, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0661\n",
      "Iteration: 37, Epoch: 10, Avg Time per Batch: 0.2359 sec, Loss: 3.37872563, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0655\n",
      "Iteration: 37, Epoch: 11, Avg Time per Batch: 0.2358 sec, Loss: 3.37867711, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0651\n",
      "Iteration: 37, Epoch: 12, Avg Time per Batch: 0.2366 sec, Loss: 3.37854408, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0648\n",
      "Iteration: 37, Epoch: 13, Avg Time per Batch: 0.2366 sec, Loss: 3.37843258, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0664\n",
      "Iteration: 37, Epoch: 14, Avg Time per Batch: 0.2368 sec, Loss: 3.37835513, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0665\n",
      "Iteration: 37, Epoch: 15, Avg Time per Batch: 0.2359 sec, Loss: 3.37824435, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0675\n",
      "Iteration: 37, Epoch: 16, Avg Time per Batch: 0.2366 sec, Loss: 3.37816302, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0672\n",
      "Iteration: 37, Epoch: 17, Avg Time per Batch: 0.2348 sec, Loss: 3.37808454, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0673\n",
      "Iteration: 37, Epoch: 18, Avg Time per Batch: 0.2361 sec, Loss: 3.37797713, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0672\n",
      "Iteration: 37, Epoch: 19, Avg Time per Batch: 0.2353 sec, Loss: 3.37790203, MSE: 0.0296, MAE: 0.1178, RMSE: 0.1719, R2: -0.0666\n",
      "Epoch 19 completed in 1174.12 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 37, Epoch: 19, Loss: 3.3779020255\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is CCBG\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5115, 60, 24])\n",
      "Iteration: 38, Epoch: 0, Avg Time per Batch: 0.2370 sec, Loss: 3.39110300, MSE: 0.0450, MAE: 0.1606, RMSE: 0.2122, R2: -0.0574\n",
      "Iteration: 38, Epoch: 1, Avg Time per Batch: 0.2358 sec, Loss: 3.39088340, MSE: 0.0447, MAE: 0.1603, RMSE: 0.2113, R2: -0.0556\n",
      "Iteration: 38, Epoch: 2, Avg Time per Batch: 0.2359 sec, Loss: 3.39092608, MSE: 0.0445, MAE: 0.1601, RMSE: 0.2109, R2: -0.0520\n",
      "Iteration: 38, Epoch: 3, Avg Time per Batch: 0.2354 sec, Loss: 3.39079363, MSE: 0.0444, MAE: 0.1601, RMSE: 0.2107, R2: -0.0508\n",
      "Iteration: 38, Epoch: 4, Avg Time per Batch: 0.2359 sec, Loss: 3.39057174, MSE: 0.0443, MAE: 0.1600, RMSE: 0.2105, R2: -0.0495\n",
      "Iteration: 38, Epoch: 5, Avg Time per Batch: 0.2363 sec, Loss: 3.39052708, MSE: 0.0443, MAE: 0.1600, RMSE: 0.2105, R2: -0.0491\n",
      "Iteration: 38, Epoch: 6, Avg Time per Batch: 0.2357 sec, Loss: 3.39040144, MSE: 0.0443, MAE: 0.1600, RMSE: 0.2104, R2: -0.0490\n",
      "Iteration: 38, Epoch: 7, Avg Time per Batch: 0.2361 sec, Loss: 3.39042465, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2103, R2: -0.0485\n",
      "Iteration: 38, Epoch: 8, Avg Time per Batch: 0.2359 sec, Loss: 3.39020747, MSE: 0.0442, MAE: 0.1600, RMSE: 0.2103, R2: -0.0480\n",
      "Iteration: 38, Epoch: 9, Avg Time per Batch: 0.2365 sec, Loss: 3.39018734, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2103, R2: -0.0477\n",
      "Iteration: 38, Epoch: 10, Avg Time per Batch: 0.2359 sec, Loss: 3.39009928, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2102, R2: -0.0472\n",
      "Iteration: 38, Epoch: 11, Avg Time per Batch: 0.2376 sec, Loss: 3.39002253, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2102, R2: -0.0468\n",
      "Iteration: 38, Epoch: 12, Avg Time per Batch: 0.2358 sec, Loss: 3.38991992, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2102, R2: -0.0468\n",
      "Iteration: 38, Epoch: 13, Avg Time per Batch: 0.2360 sec, Loss: 3.38978949, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2102, R2: -0.0471\n",
      "Iteration: 38, Epoch: 14, Avg Time per Batch: 0.2349 sec, Loss: 3.38973551, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2102, R2: -0.0468\n",
      "Iteration: 38, Epoch: 15, Avg Time per Batch: 0.2358 sec, Loss: 3.38963966, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2102, R2: -0.0465\n",
      "Iteration: 38, Epoch: 16, Avg Time per Batch: 0.2365 sec, Loss: 3.38957128, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2102, R2: -0.0465\n",
      "Iteration: 38, Epoch: 17, Avg Time per Batch: 0.2351 sec, Loss: 3.38952560, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2101, R2: -0.0467\n",
      "Iteration: 38, Epoch: 18, Avg Time per Batch: 0.2364 sec, Loss: 3.38939586, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2101, R2: -0.0468\n",
      "Iteration: 38, Epoch: 19, Avg Time per Batch: 0.2348 sec, Loss: 3.38935061, MSE: 0.0442, MAE: 0.1599, RMSE: 0.2101, R2: -0.0466\n",
      "Epoch 19 completed in 1044.04 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 38, Epoch: 19, Loss: 3.3893506072\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is BIG\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5895, 60, 24])\n",
      "Iteration: 39, Epoch: 0, Avg Time per Batch: 0.2361 sec, Loss: 3.38587406, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0462\n",
      "Iteration: 39, Epoch: 1, Avg Time per Batch: 0.2353 sec, Loss: 3.38604250, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0458\n",
      "Iteration: 39, Epoch: 2, Avg Time per Batch: 0.2357 sec, Loss: 3.38593957, MSE: 0.0428, MAE: 0.1592, RMSE: 0.2069, R2: -0.0450\n",
      "Iteration: 39, Epoch: 3, Avg Time per Batch: 0.2356 sec, Loss: 3.38578150, MSE: 0.0428, MAE: 0.1592, RMSE: 0.2069, R2: -0.0444\n",
      "Iteration: 39, Epoch: 4, Avg Time per Batch: 0.2351 sec, Loss: 3.38594143, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0441\n",
      "Iteration: 39, Epoch: 5, Avg Time per Batch: 0.2361 sec, Loss: 3.38575145, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0443\n",
      "Iteration: 39, Epoch: 6, Avg Time per Batch: 0.2360 sec, Loss: 3.38568729, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2068, R2: -0.0444\n",
      "Iteration: 39, Epoch: 7, Avg Time per Batch: 0.2350 sec, Loss: 3.38567801, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0452\n",
      "Iteration: 39, Epoch: 8, Avg Time per Batch: 0.2357 sec, Loss: 3.38544518, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0454\n",
      "Iteration: 39, Epoch: 9, Avg Time per Batch: 0.2350 sec, Loss: 3.38536783, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0455\n",
      "Iteration: 39, Epoch: 10, Avg Time per Batch: 0.2346 sec, Loss: 3.38530994, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0457\n",
      "Iteration: 39, Epoch: 11, Avg Time per Batch: 0.2354 sec, Loss: 3.38521597, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0455\n",
      "Iteration: 39, Epoch: 12, Avg Time per Batch: 0.2341 sec, Loss: 3.38516738, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0455\n",
      "Iteration: 39, Epoch: 13, Avg Time per Batch: 0.2352 sec, Loss: 3.38509536, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0458\n",
      "Iteration: 39, Epoch: 14, Avg Time per Batch: 0.2355 sec, Loss: 3.38498466, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0457\n",
      "Iteration: 39, Epoch: 15, Avg Time per Batch: 0.2350 sec, Loss: 3.38491928, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0459\n",
      "Iteration: 39, Epoch: 16, Avg Time per Batch: 0.2342 sec, Loss: 3.38479726, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0457\n",
      "Iteration: 39, Epoch: 17, Avg Time per Batch: 0.2356 sec, Loss: 3.38470183, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0458\n",
      "Iteration: 39, Epoch: 18, Avg Time per Batch: 0.2363 sec, Loss: 3.38466884, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0459\n",
      "Iteration: 39, Epoch: 19, Avg Time per Batch: 0.2360 sec, Loss: 3.38455748, MSE: 0.0428, MAE: 0.1593, RMSE: 0.2069, R2: -0.0457\n",
      "Epoch 19 completed in 1165.37 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 39, Epoch: 19, Loss: 3.3845574778\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is SRTS\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([214, 60, 24])\n",
      "Iteration: 40, Epoch: 0, Avg Time per Batch: 0.2804 sec, Loss: 3.36735640, MSE: 0.0484, MAE: 0.1753, RMSE: 0.2200, R2: -0.0911\n",
      "Iteration: 40, Epoch: 1, Avg Time per Batch: 0.2994 sec, Loss: 3.38202634, MSE: 0.0477, MAE: 0.1732, RMSE: 0.2184, R2: -0.0680\n",
      "Iteration: 40, Epoch: 2, Avg Time per Batch: 0.2916 sec, Loss: 3.39312259, MSE: 0.0476, MAE: 0.1724, RMSE: 0.2182, R2: -0.0614\n",
      "Iteration: 40, Epoch: 3, Avg Time per Batch: 0.2977 sec, Loss: 3.39295201, MSE: 0.0474, MAE: 0.1721, RMSE: 0.2177, R2: -0.0568\n",
      "Iteration: 40, Epoch: 4, Avg Time per Batch: 0.2953 sec, Loss: 3.38736262, MSE: 0.0473, MAE: 0.1720, RMSE: 0.2174, R2: -0.0526\n",
      "Iteration: 40, Epoch: 5, Avg Time per Batch: 0.2965 sec, Loss: 3.38661739, MSE: 0.0472, MAE: 0.1720, RMSE: 0.2174, R2: -0.0483\n",
      "Iteration: 40, Epoch: 6, Avg Time per Batch: 0.2905 sec, Loss: 3.38806114, MSE: 0.0470, MAE: 0.1714, RMSE: 0.2167, R2: -0.0452\n",
      "Iteration: 40, Epoch: 7, Avg Time per Batch: 0.2832 sec, Loss: 3.38716576, MSE: 0.0469, MAE: 0.1711, RMSE: 0.2165, R2: -0.0427\n",
      "Iteration: 40, Epoch: 8, Avg Time per Batch: 0.2900 sec, Loss: 3.38779988, MSE: 0.0469, MAE: 0.1711, RMSE: 0.2165, R2: -0.0416\n",
      "Iteration: 40, Epoch: 9, Avg Time per Batch: 0.2893 sec, Loss: 3.38636300, MSE: 0.0468, MAE: 0.1709, RMSE: 0.2162, R2: -0.0407\n",
      "Iteration: 40, Epoch: 10, Avg Time per Batch: 0.2805 sec, Loss: 3.38757059, MSE: 0.0467, MAE: 0.1708, RMSE: 0.2162, R2: -0.0389\n",
      "Iteration: 40, Epoch: 11, Avg Time per Batch: 0.2878 sec, Loss: 3.38678802, MSE: 0.0467, MAE: 0.1708, RMSE: 0.2161, R2: -0.0395\n",
      "Iteration: 40, Epoch: 12, Avg Time per Batch: 0.2868 sec, Loss: 3.38639980, MSE: 0.0467, MAE: 0.1707, RMSE: 0.2160, R2: -0.0395\n",
      "Iteration: 40, Epoch: 13, Avg Time per Batch: 0.2854 sec, Loss: 3.38672174, MSE: 0.0466, MAE: 0.1706, RMSE: 0.2159, R2: -0.0401\n",
      "Iteration: 40, Epoch: 14, Avg Time per Batch: 0.2839 sec, Loss: 3.38735794, MSE: 0.0466, MAE: 0.1705, RMSE: 0.2158, R2: -0.0391\n",
      "Iteration: 40, Epoch: 15, Avg Time per Batch: 0.2810 sec, Loss: 3.38803509, MSE: 0.0465, MAE: 0.1704, RMSE: 0.2157, R2: -0.0390\n",
      "Iteration: 40, Epoch: 16, Avg Time per Batch: 0.2828 sec, Loss: 3.38661509, MSE: 0.0465, MAE: 0.1703, RMSE: 0.2156, R2: -0.0394\n",
      "Iteration: 40, Epoch: 17, Avg Time per Batch: 0.2931 sec, Loss: 3.38513334, MSE: 0.0465, MAE: 0.1703, RMSE: 0.2157, R2: -0.0387\n",
      "Iteration: 40, Epoch: 18, Avg Time per Batch: 0.2814 sec, Loss: 3.38582381, MSE: 0.0465, MAE: 0.1703, RMSE: 0.2156, R2: -0.0383\n",
      "Iteration: 40, Epoch: 19, Avg Time per Batch: 0.2864 sec, Loss: 3.38696015, MSE: 0.0465, MAE: 0.1703, RMSE: 0.2156, R2: -0.0374\n",
      "Epoch 19 completed in 263.94 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 40, Epoch: 19, Loss: 3.3869601488\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is INGN\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([829, 60, 24])\n",
      "Iteration: 41, Epoch: 0, Avg Time per Batch: 0.2512 sec, Loss: 3.38508962, MSE: 0.0492, MAE: 0.1709, RMSE: 0.2218, R2: -0.1013\n",
      "Iteration: 41, Epoch: 1, Avg Time per Batch: 0.2463 sec, Loss: 3.38609275, MSE: 0.0481, MAE: 0.1683, RMSE: 0.2193, R2: -0.0807\n",
      "Iteration: 41, Epoch: 2, Avg Time per Batch: 0.2419 sec, Loss: 3.38606424, MSE: 0.0479, MAE: 0.1681, RMSE: 0.2188, R2: -0.0719\n",
      "Iteration: 41, Epoch: 3, Avg Time per Batch: 0.2445 sec, Loss: 3.38848739, MSE: 0.0477, MAE: 0.1675, RMSE: 0.2183, R2: -0.0673\n",
      "Iteration: 41, Epoch: 4, Avg Time per Batch: 0.2447 sec, Loss: 3.38779587, MSE: 0.0476, MAE: 0.1674, RMSE: 0.2181, R2: -0.0655\n",
      "Iteration: 41, Epoch: 5, Avg Time per Batch: 0.2444 sec, Loss: 3.38723877, MSE: 0.0474, MAE: 0.1670, RMSE: 0.2177, R2: -0.0626\n",
      "Iteration: 41, Epoch: 6, Avg Time per Batch: 0.2440 sec, Loss: 3.38688103, MSE: 0.0473, MAE: 0.1667, RMSE: 0.2174, R2: -0.0594\n",
      "Iteration: 41, Epoch: 7, Avg Time per Batch: 0.2434 sec, Loss: 3.38688561, MSE: 0.0472, MAE: 0.1665, RMSE: 0.2173, R2: -0.0565\n",
      "Iteration: 41, Epoch: 8, Avg Time per Batch: 0.2477 sec, Loss: 3.38741305, MSE: 0.0472, MAE: 0.1665, RMSE: 0.2173, R2: -0.0551\n",
      "Iteration: 41, Epoch: 9, Avg Time per Batch: 0.2425 sec, Loss: 3.38699391, MSE: 0.0472, MAE: 0.1666, RMSE: 0.2173, R2: -0.0544\n",
      "Iteration: 41, Epoch: 10, Avg Time per Batch: 0.2444 sec, Loss: 3.38715317, MSE: 0.0472, MAE: 0.1665, RMSE: 0.2172, R2: -0.0533\n",
      "Iteration: 41, Epoch: 11, Avg Time per Batch: 0.2460 sec, Loss: 3.38673469, MSE: 0.0471, MAE: 0.1664, RMSE: 0.2171, R2: -0.0522\n",
      "Iteration: 41, Epoch: 12, Avg Time per Batch: 0.2454 sec, Loss: 3.38699352, MSE: 0.0471, MAE: 0.1663, RMSE: 0.2170, R2: -0.0519\n",
      "Iteration: 41, Epoch: 13, Avg Time per Batch: 0.2478 sec, Loss: 3.38721892, MSE: 0.0471, MAE: 0.1663, RMSE: 0.2170, R2: -0.0511\n",
      "Iteration: 41, Epoch: 14, Avg Time per Batch: 0.2454 sec, Loss: 3.38700843, MSE: 0.0471, MAE: 0.1663, RMSE: 0.2170, R2: -0.0512\n",
      "Iteration: 41, Epoch: 15, Avg Time per Batch: 0.2471 sec, Loss: 3.38693883, MSE: 0.0470, MAE: 0.1662, RMSE: 0.2169, R2: -0.0504\n",
      "Iteration: 41, Epoch: 16, Avg Time per Batch: 0.2469 sec, Loss: 3.38660944, MSE: 0.0470, MAE: 0.1662, RMSE: 0.2169, R2: -0.0501\n",
      "Iteration: 41, Epoch: 17, Avg Time per Batch: 0.2468 sec, Loss: 3.38695352, MSE: 0.0470, MAE: 0.1661, RMSE: 0.2168, R2: -0.0495\n",
      "Iteration: 41, Epoch: 18, Avg Time per Batch: 0.2447 sec, Loss: 3.38705809, MSE: 0.0470, MAE: 0.1661, RMSE: 0.2168, R2: -0.0491\n",
      "Iteration: 41, Epoch: 19, Avg Time per Batch: 0.2476 sec, Loss: 3.38702610, MSE: 0.0470, MAE: 0.1661, RMSE: 0.2168, R2: -0.0488\n",
      "Epoch 19 completed in 360.92 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 41, Epoch: 19, Loss: 3.3870261014\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is WRB\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5895, 60, 24])\n",
      "Iteration: 42, Epoch: 0, Avg Time per Batch: 0.2352 sec, Loss: 3.38253121, MSE: 0.0437, MAE: 0.1554, RMSE: 0.2091, R2: -0.0657\n",
      "Iteration: 42, Epoch: 1, Avg Time per Batch: 0.2344 sec, Loss: 3.38282737, MSE: 0.0436, MAE: 0.1550, RMSE: 0.2087, R2: -0.0533\n",
      "Iteration: 42, Epoch: 2, Avg Time per Batch: 0.2356 sec, Loss: 3.38261210, MSE: 0.0436, MAE: 0.1549, RMSE: 0.2087, R2: -0.0516\n",
      "Iteration: 42, Epoch: 3, Avg Time per Batch: 0.2339 sec, Loss: 3.38257096, MSE: 0.0435, MAE: 0.1549, RMSE: 0.2086, R2: -0.0493\n",
      "Iteration: 42, Epoch: 4, Avg Time per Batch: 0.2347 sec, Loss: 3.38266202, MSE: 0.0435, MAE: 0.1548, RMSE: 0.2086, R2: -0.0494\n",
      "Iteration: 42, Epoch: 5, Avg Time per Batch: 0.2344 sec, Loss: 3.38243048, MSE: 0.0435, MAE: 0.1548, RMSE: 0.2086, R2: -0.0490\n",
      "Iteration: 42, Epoch: 6, Avg Time per Batch: 0.2351 sec, Loss: 3.38240646, MSE: 0.0435, MAE: 0.1548, RMSE: 0.2086, R2: -0.0487\n",
      "Iteration: 42, Epoch: 7, Avg Time per Batch: 0.2342 sec, Loss: 3.38233174, MSE: 0.0435, MAE: 0.1547, RMSE: 0.2086, R2: -0.0483\n",
      "Iteration: 42, Epoch: 8, Avg Time per Batch: 0.2356 sec, Loss: 3.38222944, MSE: 0.0435, MAE: 0.1547, RMSE: 0.2085, R2: -0.0477\n",
      "Iteration: 42, Epoch: 9, Avg Time per Batch: 0.2348 sec, Loss: 3.38213340, MSE: 0.0435, MAE: 0.1547, RMSE: 0.2085, R2: -0.0474\n",
      "Iteration: 42, Epoch: 10, Avg Time per Batch: 0.2344 sec, Loss: 3.38209479, MSE: 0.0435, MAE: 0.1547, RMSE: 0.2085, R2: -0.0473\n",
      "Iteration: 42, Epoch: 11, Avg Time per Batch: 0.2350 sec, Loss: 3.38199926, MSE: 0.0435, MAE: 0.1547, RMSE: 0.2085, R2: -0.0477\n",
      "Iteration: 42, Epoch: 12, Avg Time per Batch: 0.2351 sec, Loss: 3.38194564, MSE: 0.0435, MAE: 0.1547, RMSE: 0.2085, R2: -0.0476\n",
      "Iteration: 42, Epoch: 13, Avg Time per Batch: 0.2352 sec, Loss: 3.38182282, MSE: 0.0435, MAE: 0.1547, RMSE: 0.2085, R2: -0.0475\n",
      "Iteration: 42, Epoch: 14, Avg Time per Batch: 0.2363 sec, Loss: 3.38178799, MSE: 0.0435, MAE: 0.1547, RMSE: 0.2085, R2: -0.0477\n",
      "Iteration: 42, Epoch: 15, Avg Time per Batch: 0.2352 sec, Loss: 3.38168330, MSE: 0.0435, MAE: 0.1547, RMSE: 0.2085, R2: -0.0477\n",
      "Iteration: 42, Epoch: 16, Avg Time per Batch: 0.2348 sec, Loss: 3.38162933, MSE: 0.0435, MAE: 0.1546, RMSE: 0.2085, R2: -0.0477\n",
      "Iteration: 42, Epoch: 17, Avg Time per Batch: 0.2353 sec, Loss: 3.38154437, MSE: 0.0435, MAE: 0.1546, RMSE: 0.2085, R2: -0.0474\n",
      "Iteration: 42, Epoch: 18, Avg Time per Batch: 0.2358 sec, Loss: 3.38147743, MSE: 0.0435, MAE: 0.1546, RMSE: 0.2085, R2: -0.0473\n",
      "Iteration: 42, Epoch: 19, Avg Time per Batch: 0.2353 sec, Loss: 3.38139658, MSE: 0.0435, MAE: 0.1546, RMSE: 0.2085, R2: -0.0472\n",
      "Epoch 19 completed in 1163.94 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 42, Epoch: 19, Loss: 3.3813965790\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is SHOO\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5895, 60, 24])\n",
      "Iteration: 43, Epoch: 0, Avg Time per Batch: 0.2354 sec, Loss: 3.38604643, MSE: 0.0496, MAE: 0.1671, RMSE: 0.2227, R2: -0.0709\n",
      "Iteration: 43, Epoch: 1, Avg Time per Batch: 0.2356 sec, Loss: 3.38592668, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2226, R2: -0.0607\n",
      "Iteration: 43, Epoch: 2, Avg Time per Batch: 0.2347 sec, Loss: 3.38578105, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2226, R2: -0.0564\n",
      "Iteration: 43, Epoch: 3, Avg Time per Batch: 0.2353 sec, Loss: 3.38575122, MSE: 0.0496, MAE: 0.1671, RMSE: 0.2226, R2: -0.0552\n",
      "Iteration: 43, Epoch: 4, Avg Time per Batch: 0.2358 sec, Loss: 3.38574888, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2227, R2: -0.0551\n",
      "Iteration: 43, Epoch: 5, Avg Time per Batch: 0.2366 sec, Loss: 3.38558021, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2227, R2: -0.0555\n",
      "Iteration: 43, Epoch: 6, Avg Time per Batch: 0.2353 sec, Loss: 3.38549410, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2227, R2: -0.0556\n",
      "Iteration: 43, Epoch: 7, Avg Time per Batch: 0.2365 sec, Loss: 3.38551069, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0560\n",
      "Iteration: 43, Epoch: 8, Avg Time per Batch: 0.2355 sec, Loss: 3.38540635, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0555\n",
      "Iteration: 43, Epoch: 9, Avg Time per Batch: 0.2371 sec, Loss: 3.38535997, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0559\n",
      "Iteration: 43, Epoch: 10, Avg Time per Batch: 0.2354 sec, Loss: 3.38532576, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0558\n",
      "Iteration: 43, Epoch: 11, Avg Time per Batch: 0.2355 sec, Loss: 3.38524130, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0552\n",
      "Iteration: 43, Epoch: 12, Avg Time per Batch: 0.2358 sec, Loss: 3.38518351, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0553\n",
      "Iteration: 43, Epoch: 13, Avg Time per Batch: 0.2363 sec, Loss: 3.38507290, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0550\n",
      "Iteration: 43, Epoch: 14, Avg Time per Batch: 0.2358 sec, Loss: 3.38499574, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0550\n",
      "Iteration: 43, Epoch: 15, Avg Time per Batch: 0.2364 sec, Loss: 3.38492704, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0546\n",
      "Iteration: 43, Epoch: 16, Avg Time per Batch: 0.2358 sec, Loss: 3.38486593, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0548\n",
      "Iteration: 43, Epoch: 17, Avg Time per Batch: 0.2359 sec, Loss: 3.38476794, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0549\n",
      "Iteration: 43, Epoch: 18, Avg Time per Batch: 0.2362 sec, Loss: 3.38474468, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0549\n",
      "Iteration: 43, Epoch: 19, Avg Time per Batch: 0.2368 sec, Loss: 3.38463596, MSE: 0.0496, MAE: 0.1672, RMSE: 0.2228, R2: -0.0549\n",
      "Epoch 19 completed in 1170.81 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 43, Epoch: 19, Loss: 3.3846359594\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is CEAD\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([809, 60, 24])\n",
      "Iteration: 44, Epoch: 0, Avg Time per Batch: 0.2441 sec, Loss: 3.37448877, MSE: 0.0385, MAE: 0.1455, RMSE: 0.1961, R2: -3.0786\n",
      "Iteration: 44, Epoch: 1, Avg Time per Batch: 0.2500 sec, Loss: 3.36784846, MSE: 0.0353, MAE: 0.1337, RMSE: 0.1880, R2: -1.7952\n",
      "Iteration: 44, Epoch: 2, Avg Time per Batch: 0.2477 sec, Loss: 3.36792906, MSE: 0.0343, MAE: 0.1283, RMSE: 0.1851, R2: -1.2218\n",
      "Iteration: 44, Epoch: 3, Avg Time per Batch: 0.2483 sec, Loss: 3.36808518, MSE: 0.0337, MAE: 0.1259, RMSE: 0.1835, R2: -0.9350\n",
      "Iteration: 44, Epoch: 4, Avg Time per Batch: 0.2496 sec, Loss: 3.36674680, MSE: 0.0333, MAE: 0.1243, RMSE: 0.1826, R2: -0.7583\n",
      "Iteration: 44, Epoch: 5, Avg Time per Batch: 0.2490 sec, Loss: 3.36650609, MSE: 0.0332, MAE: 0.1234, RMSE: 0.1821, R2: -0.6405\n",
      "Iteration: 44, Epoch: 6, Avg Time per Batch: 0.2453 sec, Loss: 3.36552662, MSE: 0.0330, MAE: 0.1225, RMSE: 0.1816, R2: -0.5580\n",
      "Iteration: 44, Epoch: 7, Avg Time per Batch: 0.2439 sec, Loss: 3.36620545, MSE: 0.0329, MAE: 0.1220, RMSE: 0.1813, R2: -0.4938\n",
      "Iteration: 44, Epoch: 8, Avg Time per Batch: 0.2489 sec, Loss: 3.36627041, MSE: 0.0328, MAE: 0.1215, RMSE: 0.1810, R2: -0.4449\n",
      "Iteration: 44, Epoch: 9, Avg Time per Batch: 0.2475 sec, Loss: 3.36620185, MSE: 0.0327, MAE: 0.1212, RMSE: 0.1808, R2: -0.4062\n",
      "Iteration: 44, Epoch: 10, Avg Time per Batch: 0.2482 sec, Loss: 3.36592119, MSE: 0.0326, MAE: 0.1209, RMSE: 0.1806, R2: -0.3745\n",
      "Iteration: 44, Epoch: 11, Avg Time per Batch: 0.2478 sec, Loss: 3.36548875, MSE: 0.0326, MAE: 0.1207, RMSE: 0.1804, R2: -0.3506\n",
      "Iteration: 44, Epoch: 12, Avg Time per Batch: 0.2469 sec, Loss: 3.36572104, MSE: 0.0325, MAE: 0.1204, RMSE: 0.1803, R2: -0.3308\n",
      "Iteration: 44, Epoch: 13, Avg Time per Batch: 0.2473 sec, Loss: 3.36555887, MSE: 0.0325, MAE: 0.1203, RMSE: 0.1802, R2: -0.3125\n",
      "Iteration: 44, Epoch: 14, Avg Time per Batch: 0.2472 sec, Loss: 3.36581343, MSE: 0.0324, MAE: 0.1202, RMSE: 0.1801, R2: -0.2961\n",
      "Iteration: 44, Epoch: 15, Avg Time per Batch: 0.2460 sec, Loss: 3.36550725, MSE: 0.0324, MAE: 0.1200, RMSE: 0.1801, R2: -0.2829\n",
      "Iteration: 44, Epoch: 16, Avg Time per Batch: 0.2485 sec, Loss: 3.36539425, MSE: 0.0324, MAE: 0.1198, RMSE: 0.1800, R2: -0.2694\n",
      "Iteration: 44, Epoch: 17, Avg Time per Batch: 0.2478 sec, Loss: 3.36552129, MSE: 0.0324, MAE: 0.1198, RMSE: 0.1800, R2: -0.2588\n",
      "Iteration: 44, Epoch: 18, Avg Time per Batch: 0.2457 sec, Loss: 3.36553475, MSE: 0.0324, MAE: 0.1197, RMSE: 0.1799, R2: -0.2480\n",
      "Iteration: 44, Epoch: 19, Avg Time per Batch: 0.2458 sec, Loss: 3.36553783, MSE: 0.0323, MAE: 0.1196, RMSE: 0.1798, R2: -0.2373\n",
      "Epoch 19 completed in 367.54 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 44, Epoch: 19, Loss: 3.3655378312\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is GOGL\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([5111, 60, 24])\n",
      "Iteration: 45, Epoch: 0, Avg Time per Batch: 0.2358 sec, Loss: 3.37999509, MSE: 0.0471, MAE: 0.1666, RMSE: 0.2171, R2: -0.0850\n",
      "Iteration: 45, Epoch: 1, Avg Time per Batch: 0.2364 sec, Loss: 3.37998369, MSE: 0.0463, MAE: 0.1655, RMSE: 0.2153, R2: -0.0658\n",
      "Iteration: 45, Epoch: 2, Avg Time per Batch: 0.2362 sec, Loss: 3.37926521, MSE: 0.0461, MAE: 0.1651, RMSE: 0.2147, R2: -0.0597\n",
      "Iteration: 45, Epoch: 3, Avg Time per Batch: 0.2355 sec, Loss: 3.37935256, MSE: 0.0460, MAE: 0.1650, RMSE: 0.2144, R2: -0.0585\n",
      "Iteration: 45, Epoch: 4, Avg Time per Batch: 0.2366 sec, Loss: 3.37906643, MSE: 0.0459, MAE: 0.1649, RMSE: 0.2142, R2: -0.0556\n",
      "Iteration: 45, Epoch: 5, Avg Time per Batch: 0.2361 sec, Loss: 3.37887367, MSE: 0.0458, MAE: 0.1648, RMSE: 0.2141, R2: -0.0538\n",
      "Iteration: 45, Epoch: 6, Avg Time per Batch: 0.2363 sec, Loss: 3.37894719, MSE: 0.0458, MAE: 0.1648, RMSE: 0.2140, R2: -0.0537\n",
      "Iteration: 45, Epoch: 7, Avg Time per Batch: 0.2352 sec, Loss: 3.37890768, MSE: 0.0458, MAE: 0.1647, RMSE: 0.2139, R2: -0.0527\n",
      "Iteration: 45, Epoch: 8, Avg Time per Batch: 0.2377 sec, Loss: 3.37878060, MSE: 0.0457, MAE: 0.1647, RMSE: 0.2139, R2: -0.0523\n",
      "Iteration: 45, Epoch: 9, Avg Time per Batch: 0.2355 sec, Loss: 3.37868626, MSE: 0.0457, MAE: 0.1647, RMSE: 0.2138, R2: -0.0521\n",
      "Iteration: 45, Epoch: 10, Avg Time per Batch: 0.2352 sec, Loss: 3.37869646, MSE: 0.0457, MAE: 0.1647, RMSE: 0.2138, R2: -0.0518\n",
      "Iteration: 45, Epoch: 11, Avg Time per Batch: 0.2362 sec, Loss: 3.37865511, MSE: 0.0457, MAE: 0.1646, RMSE: 0.2138, R2: -0.0512\n",
      "Iteration: 45, Epoch: 12, Avg Time per Batch: 0.2359 sec, Loss: 3.37853080, MSE: 0.0457, MAE: 0.1646, RMSE: 0.2138, R2: -0.0506\n",
      "Iteration: 45, Epoch: 13, Avg Time per Batch: 0.2372 sec, Loss: 3.37848543, MSE: 0.0457, MAE: 0.1646, RMSE: 0.2137, R2: -0.0503\n",
      "Iteration: 45, Epoch: 14, Avg Time per Batch: 0.2354 sec, Loss: 3.37839329, MSE: 0.0457, MAE: 0.1646, RMSE: 0.2137, R2: -0.0503\n",
      "Iteration: 45, Epoch: 15, Avg Time per Batch: 0.2361 sec, Loss: 3.37841560, MSE: 0.0457, MAE: 0.1646, RMSE: 0.2137, R2: -0.0499\n",
      "Iteration: 45, Epoch: 16, Avg Time per Batch: 0.2371 sec, Loss: 3.37831616, MSE: 0.0457, MAE: 0.1646, RMSE: 0.2137, R2: -0.0499\n",
      "Iteration: 45, Epoch: 17, Avg Time per Batch: 0.2364 sec, Loss: 3.37826905, MSE: 0.0456, MAE: 0.1646, RMSE: 0.2137, R2: -0.0500\n",
      "Iteration: 45, Epoch: 18, Avg Time per Batch: 0.2362 sec, Loss: 3.37822373, MSE: 0.0456, MAE: 0.1646, RMSE: 0.2136, R2: -0.0500\n",
      "Iteration: 45, Epoch: 19, Avg Time per Batch: 0.2365 sec, Loss: 3.37818879, MSE: 0.0456, MAE: 0.1646, RMSE: 0.2136, R2: -0.0496\n",
      "Epoch 19 completed in 1046.13 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 45, Epoch: 19, Loss: 3.3781887936\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is EDN\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([2543, 60, 24])\n",
      "Iteration: 46, Epoch: 0, Avg Time per Batch: 0.2385 sec, Loss: 3.36918778, MSE: 0.0384, MAE: 0.1429, RMSE: 0.1961, R2: -0.0630\n",
      "Iteration: 46, Epoch: 1, Avg Time per Batch: 0.2394 sec, Loss: 3.36975342, MSE: 0.0382, MAE: 0.1424, RMSE: 0.1955, R2: -0.0523\n",
      "Iteration: 46, Epoch: 2, Avg Time per Batch: 0.2382 sec, Loss: 3.36933252, MSE: 0.0381, MAE: 0.1421, RMSE: 0.1952, R2: -0.0515\n",
      "Iteration: 46, Epoch: 3, Avg Time per Batch: 0.2385 sec, Loss: 3.36901393, MSE: 0.0381, MAE: 0.1420, RMSE: 0.1951, R2: -0.0515\n",
      "Iteration: 46, Epoch: 4, Avg Time per Batch: 0.2382 sec, Loss: 3.36896806, MSE: 0.0380, MAE: 0.1419, RMSE: 0.1950, R2: -0.0495\n",
      "Iteration: 46, Epoch: 5, Avg Time per Batch: 0.2386 sec, Loss: 3.36910865, MSE: 0.0380, MAE: 0.1419, RMSE: 0.1950, R2: -0.0488\n",
      "Iteration: 46, Epoch: 6, Avg Time per Batch: 0.2354 sec, Loss: 3.36911608, MSE: 0.0381, MAE: 0.1419, RMSE: 0.1951, R2: -0.0497\n",
      "Iteration: 46, Epoch: 7, Avg Time per Batch: 0.2369 sec, Loss: 3.36918264, MSE: 0.0380, MAE: 0.1419, RMSE: 0.1950, R2: -0.0496\n",
      "Iteration: 46, Epoch: 8, Avg Time per Batch: 0.2398 sec, Loss: 3.36896970, MSE: 0.0380, MAE: 0.1418, RMSE: 0.1950, R2: -0.0503\n",
      "Iteration: 46, Epoch: 9, Avg Time per Batch: 0.2392 sec, Loss: 3.36912424, MSE: 0.0380, MAE: 0.1418, RMSE: 0.1950, R2: -0.0497\n",
      "Iteration: 46, Epoch: 10, Avg Time per Batch: 0.2371 sec, Loss: 3.36901430, MSE: 0.0380, MAE: 0.1418, RMSE: 0.1949, R2: -0.0500\n",
      "Iteration: 46, Epoch: 11, Avg Time per Batch: 0.2385 sec, Loss: 3.36909344, MSE: 0.0380, MAE: 0.1418, RMSE: 0.1950, R2: -0.0499\n",
      "Iteration: 46, Epoch: 12, Avg Time per Batch: 0.2386 sec, Loss: 3.36895507, MSE: 0.0380, MAE: 0.1418, RMSE: 0.1949, R2: -0.0496\n",
      "Iteration: 46, Epoch: 13, Avg Time per Batch: 0.2371 sec, Loss: 3.36898500, MSE: 0.0380, MAE: 0.1418, RMSE: 0.1950, R2: -0.0498\n",
      "Iteration: 46, Epoch: 14, Avg Time per Batch: 0.2390 sec, Loss: 3.36892156, MSE: 0.0380, MAE: 0.1418, RMSE: 0.1950, R2: -0.0494\n",
      "Iteration: 46, Epoch: 15, Avg Time per Batch: 0.2366 sec, Loss: 3.36889521, MSE: 0.0380, MAE: 0.1418, RMSE: 0.1949, R2: -0.0494\n",
      "Iteration: 46, Epoch: 16, Avg Time per Batch: 0.2391 sec, Loss: 3.36886918, MSE: 0.0380, MAE: 0.1418, RMSE: 0.1949, R2: -0.0497\n",
      "Iteration: 46, Epoch: 17, Avg Time per Batch: 0.2380 sec, Loss: 3.36887942, MSE: 0.0380, MAE: 0.1418, RMSE: 0.1949, R2: -0.0491\n",
      "Iteration: 46, Epoch: 18, Avg Time per Batch: 0.2391 sec, Loss: 3.36883112, MSE: 0.0380, MAE: 0.1417, RMSE: 0.1949, R2: -0.0488\n",
      "Iteration: 46, Epoch: 19, Avg Time per Batch: 0.2396 sec, Loss: 3.36886641, MSE: 0.0380, MAE: 0.1417, RMSE: 0.1949, R2: -0.0487\n",
      "Epoch 19 completed in 641.06 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 46, Epoch: 19, Loss: 3.3688664077\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is GYRE\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([2803, 60, 24])\n",
      "Iteration: 47, Epoch: 0, Avg Time per Batch: 0.2373 sec, Loss: 3.37478868, MSE: 0.0461, MAE: 0.1543, RMSE: 0.2148, R2: -14.2327\n",
      "Iteration: 47, Epoch: 1, Avg Time per Batch: 0.2392 sec, Loss: 3.37548769, MSE: 0.0460, MAE: 0.1545, RMSE: 0.2145, R2: -8.2799\n",
      "Iteration: 47, Epoch: 2, Avg Time per Batch: 0.2356 sec, Loss: 3.37603836, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2145, R2: -7.0154\n",
      "Iteration: 47, Epoch: 3, Avg Time per Batch: 0.2364 sec, Loss: 3.37584946, MSE: 0.0460, MAE: 0.1543, RMSE: 0.2145, R2: -6.1827\n",
      "Iteration: 47, Epoch: 4, Avg Time per Batch: 0.2350 sec, Loss: 3.37574781, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2145, R2: -5.5606\n",
      "Iteration: 47, Epoch: 5, Avg Time per Batch: 0.2384 sec, Loss: 3.37619372, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2144, R2: -5.7704\n",
      "Iteration: 47, Epoch: 6, Avg Time per Batch: 0.2369 sec, Loss: 3.37600934, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2144, R2: -5.6870\n",
      "Iteration: 47, Epoch: 7, Avg Time per Batch: 0.2366 sec, Loss: 3.37605047, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2145, R2: -5.2660\n",
      "Iteration: 47, Epoch: 8, Avg Time per Batch: 0.2373 sec, Loss: 3.37610677, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2145, R2: -5.1701\n",
      "Iteration: 47, Epoch: 9, Avg Time per Batch: 0.2379 sec, Loss: 3.37613215, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2145, R2: -4.8787\n",
      "Iteration: 47, Epoch: 10, Avg Time per Batch: 0.2364 sec, Loss: 3.37601254, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2144, R2: -4.8270\n",
      "Iteration: 47, Epoch: 11, Avg Time per Batch: 0.2381 sec, Loss: 3.37607235, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2144, R2: -4.8463\n",
      "Iteration: 47, Epoch: 12, Avg Time per Batch: 0.2365 sec, Loss: 3.37608297, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2144, R2: -4.8429\n",
      "Iteration: 47, Epoch: 13, Avg Time per Batch: 0.2374 sec, Loss: 3.37602035, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2144, R2: -4.8944\n",
      "Iteration: 47, Epoch: 14, Avg Time per Batch: 0.2356 sec, Loss: 3.37601497, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2144, R2: -4.8939\n",
      "Iteration: 47, Epoch: 15, Avg Time per Batch: 0.2360 sec, Loss: 3.37608223, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2145, R2: -5.1679\n",
      "Iteration: 47, Epoch: 16, Avg Time per Batch: 0.2385 sec, Loss: 3.37601747, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2144, R2: -5.1192\n",
      "Iteration: 47, Epoch: 17, Avg Time per Batch: 0.2368 sec, Loss: 3.37597504, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2144, R2: -4.9900\n",
      "Iteration: 47, Epoch: 18, Avg Time per Batch: 0.2383 sec, Loss: 3.37593334, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2144, R2: -4.9106\n",
      "Iteration: 47, Epoch: 19, Avg Time per Batch: 0.2362 sec, Loss: 3.37599538, MSE: 0.0460, MAE: 0.1544, RMSE: 0.2144, R2: -4.9701\n",
      "Epoch 19 completed in 675.56 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 47, Epoch: 19, Loss: 3.3759953827\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is TTNP\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([2623, 60, 24])\n",
      "Iteration: 48, Epoch: 0, Avg Time per Batch: 0.2372 sec, Loss: 3.36933297, MSE: 0.0393, MAE: 0.1466, RMSE: 0.1982, R2: -0.0642\n",
      "Iteration: 48, Epoch: 1, Avg Time per Batch: 0.2361 sec, Loss: 3.36906381, MSE: 0.0392, MAE: 0.1464, RMSE: 0.1979, R2: -0.0633\n",
      "Iteration: 48, Epoch: 2, Avg Time per Batch: 0.2371 sec, Loss: 3.36905809, MSE: 0.0391, MAE: 0.1464, RMSE: 0.1978, R2: -0.0612\n",
      "Iteration: 48, Epoch: 3, Avg Time per Batch: 0.2366 sec, Loss: 3.36893318, MSE: 0.0391, MAE: 0.1463, RMSE: 0.1977, R2: -0.0600\n",
      "Iteration: 48, Epoch: 4, Avg Time per Batch: 0.2362 sec, Loss: 3.36882143, MSE: 0.0391, MAE: 0.1462, RMSE: 0.1977, R2: -0.0601\n",
      "Iteration: 48, Epoch: 5, Avg Time per Batch: 0.2370 sec, Loss: 3.36882211, MSE: 0.0391, MAE: 0.1462, RMSE: 0.1977, R2: -0.0594\n",
      "Iteration: 48, Epoch: 6, Avg Time per Batch: 0.2378 sec, Loss: 3.36878898, MSE: 0.0391, MAE: 0.1462, RMSE: 0.1976, R2: -0.0592\n",
      "Iteration: 48, Epoch: 7, Avg Time per Batch: 0.2359 sec, Loss: 3.36863416, MSE: 0.0391, MAE: 0.1462, RMSE: 0.1976, R2: -0.0588\n",
      "Iteration: 48, Epoch: 8, Avg Time per Batch: 0.2383 sec, Loss: 3.36875462, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1976, R2: -0.0596\n",
      "Iteration: 48, Epoch: 9, Avg Time per Batch: 0.2373 sec, Loss: 3.36881227, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1976, R2: -0.0598\n",
      "Iteration: 48, Epoch: 10, Avg Time per Batch: 0.2371 sec, Loss: 3.36872168, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1976, R2: -0.0595\n",
      "Iteration: 48, Epoch: 11, Avg Time per Batch: 0.2368 sec, Loss: 3.36881078, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1976, R2: -0.0593\n",
      "Iteration: 48, Epoch: 12, Avg Time per Batch: 0.2362 sec, Loss: 3.36875705, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1976, R2: -0.0596\n",
      "Iteration: 48, Epoch: 13, Avg Time per Batch: 0.2373 sec, Loss: 3.36868848, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1976, R2: -0.0590\n",
      "Iteration: 48, Epoch: 14, Avg Time per Batch: 0.2364 sec, Loss: 3.36870067, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1976, R2: -0.0590\n",
      "Iteration: 48, Epoch: 15, Avg Time per Batch: 0.2381 sec, Loss: 3.36866709, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1975, R2: -0.0592\n",
      "Iteration: 48, Epoch: 16, Avg Time per Batch: 0.2376 sec, Loss: 3.36863625, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1976, R2: -0.0593\n",
      "Iteration: 48, Epoch: 17, Avg Time per Batch: 0.2356 sec, Loss: 3.36865210, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1976, R2: -0.0595\n",
      "Iteration: 48, Epoch: 18, Avg Time per Batch: 0.2385 sec, Loss: 3.36867432, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1976, R2: -0.0596\n",
      "Iteration: 48, Epoch: 19, Avg Time per Batch: 0.2366 sec, Loss: 3.36862216, MSE: 0.0390, MAE: 0.1462, RMSE: 0.1976, R2: -0.0595\n",
      "Epoch 19 completed in 644.71 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 48, Epoch: 19, Loss: 3.3686221646\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is SNY\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([3756, 60, 24])\n",
      "Iteration: 49, Epoch: 0, Avg Time per Batch: 0.2374 sec, Loss: 3.37568079, MSE: 0.0457, MAE: 0.1617, RMSE: 0.2137, R2: -0.0614\n",
      "Iteration: 49, Epoch: 1, Avg Time per Batch: 0.2359 sec, Loss: 3.37522119, MSE: 0.0455, MAE: 0.1615, RMSE: 0.2133, R2: -0.0555\n",
      "Iteration: 49, Epoch: 2, Avg Time per Batch: 0.2365 sec, Loss: 3.37494958, MSE: 0.0455, MAE: 0.1616, RMSE: 0.2133, R2: -0.0563\n",
      "Iteration: 49, Epoch: 3, Avg Time per Batch: 0.2363 sec, Loss: 3.37513012, MSE: 0.0455, MAE: 0.1614, RMSE: 0.2132, R2: -0.0538\n",
      "Iteration: 49, Epoch: 4, Avg Time per Batch: 0.2361 sec, Loss: 3.37512998, MSE: 0.0454, MAE: 0.1614, RMSE: 0.2131, R2: -0.0529\n",
      "Iteration: 49, Epoch: 5, Avg Time per Batch: 0.2355 sec, Loss: 3.37511239, MSE: 0.0454, MAE: 0.1614, RMSE: 0.2131, R2: -0.0529\n",
      "Iteration: 49, Epoch: 6, Avg Time per Batch: 0.2366 sec, Loss: 3.37489363, MSE: 0.0454, MAE: 0.1614, RMSE: 0.2131, R2: -0.0521\n",
      "Iteration: 49, Epoch: 7, Avg Time per Batch: 0.2353 sec, Loss: 3.37495456, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2131, R2: -0.0513\n",
      "Iteration: 49, Epoch: 8, Avg Time per Batch: 0.2367 sec, Loss: 3.37494821, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0507\n",
      "Iteration: 49, Epoch: 9, Avg Time per Batch: 0.2361 sec, Loss: 3.37501060, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0502\n",
      "Iteration: 49, Epoch: 10, Avg Time per Batch: 0.2371 sec, Loss: 3.37494996, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0494\n",
      "Iteration: 49, Epoch: 11, Avg Time per Batch: 0.2351 sec, Loss: 3.37478740, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0488\n",
      "Iteration: 49, Epoch: 12, Avg Time per Batch: 0.2373 sec, Loss: 3.37481292, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0490\n",
      "Iteration: 49, Epoch: 13, Avg Time per Batch: 0.2353 sec, Loss: 3.37487462, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0491\n",
      "Iteration: 49, Epoch: 14, Avg Time per Batch: 0.2349 sec, Loss: 3.37485239, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0491\n",
      "Iteration: 49, Epoch: 15, Avg Time per Batch: 0.2359 sec, Loss: 3.37481044, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0494\n",
      "Iteration: 49, Epoch: 16, Avg Time per Batch: 0.2358 sec, Loss: 3.37481201, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0492\n",
      "Iteration: 49, Epoch: 17, Avg Time per Batch: 0.2380 sec, Loss: 3.37475335, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0494\n",
      "Iteration: 49, Epoch: 18, Avg Time per Batch: 0.2357 sec, Loss: 3.37469309, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0491\n",
      "Iteration: 49, Epoch: 19, Avg Time per Batch: 0.2367 sec, Loss: 3.37474064, MSE: 0.0454, MAE: 0.1613, RMSE: 0.2130, R2: -0.0496\n",
      "Epoch 19 completed in 826.31 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 49, Epoch: 19, Loss: 3.3747406353\n"
     ]
    }
   ],
   "source": [
    "# Train the model <<< THIS SEEMS TO WORK WELL!!!!\n",
    "writer = SummaryWriter('newRun/MegaNumMuncher3')\n",
    "global_step = 0\n",
    "\n",
    "epochs = 20\n",
    "model.train()\n",
    "iteration = 0\n",
    "l1_lambda = 0.0000900 #0.0000900 TO STABALZE\n",
    "start_time = time.time()\n",
    "tpb_list = []\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.0010\n",
    "weightDecay = 0.00001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weightDecay)\n",
    "filepath = f\"C:\\\\Machine Learning\\\\SPY VIX 9 Input Output Model\\\\MegaNumMuncher3_Checkpoint-0.pth\"\n",
    "load_checkpoint(filepath, model, optimizer, device)\n",
    "\n",
    "for iteration in range(50):\n",
    "    TensorIntSystem = TensorIntegrationSystem(\"1994-01-01\", \"2018-01-01\")\n",
    "    TensorIntSystem.CreateCombinedDataFrame()\n",
    "    # Extract feature names from the DataFrame\n",
    "    feature_names = TensorIntSystem.FormattedCombinedDataFrame.columns.tolist()\n",
    "    \n",
    "    x_train_tensor, y_train_tensor = TensorIntSystem.CreateTensors()\n",
    "    new_learning_rate, batch_size, new_weightDecay = FindOptimalTraining(x_train_tensor)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_learning_rate\n",
    "        param_group['weight_decay'] = new_weightDecay\n",
    "\n",
    "    # Initialize metrics for each iteration\n",
    "    running_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_r2 = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        permutation = torch.randperm(x_train_tensor.size(0))\n",
    "\n",
    "        for i in range(0, x_train_tensor.size(0), batch_size):\n",
    "            batch_start_time = time.time()\n",
    "\n",
    "            # Adjust batch size for the last batch\n",
    "            #end_idx = min(i + batch_size, x_train_tensor.size(0))\n",
    "            \n",
    "            end_idx = i + batch_size\n",
    "            # Adjust for the last batch if it has fewer samples\n",
    "            # Skip the batch if it does not fit the expected shape\n",
    "            if end_idx > x_train_tensor.size(0) - batch_size:\n",
    "                continue\n",
    "\n",
    "            indices = permutation[i:end_idx]\n",
    "        \n",
    "            batch_seq = x_train_tensor[indices].to(device)\n",
    "            batch_label = y_train_tensor[indices].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_seq)\n",
    "            y_pred = y_pred.view(batch_label.size())\n",
    "            loss = loss_function(y_pred, batch_label)\n",
    "\n",
    "            l1_penalty = sum(p.abs().sum() for p in model.parameters())\n",
    "            total_loss = loss + l1_lambda * l1_penalty\n",
    "            running_loss += total_loss.item()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_pred_flat = y_pred.view(y_pred.size(0), -1).cpu().detach().numpy()\n",
    "            batch_label_flat = batch_label.view(batch_label.size(0), -1).cpu().detach().numpy()\n",
    "\n",
    "            mse = mean_squared_error(batch_label_flat, y_pred_flat)\n",
    "            mae = mean_absolute_error(batch_label_flat, y_pred_flat)\n",
    "            r2 = r2_score(batch_label_flat, y_pred_flat)\n",
    "\n",
    "            total_mse += mse\n",
    "            total_mae += mae\n",
    "            total_r2 += r2\n",
    "            num_batches += 1\n",
    "\n",
    "            batch_end_time = time.time()\n",
    "            tpb_list.append(batch_end_time - batch_start_time)\n",
    "            \n",
    "            # Assuming `writer` is your SummaryWriter instance and `global_step` is defined\n",
    "            calculate_and_log_feature_metrics(batch_label_flat, y_pred_flat, feature_names, global_step)\n",
    "\n",
    "        # Calculate average metrics for each epoch\n",
    "        avg_tpb = sum(tpb_list) / len(tpb_list) if tpb_list else 0\n",
    "        avg_mse = total_mse / num_batches if num_batches else float('nan')\n",
    "        avg_mae = total_mae / num_batches if num_batches else float('nan')\n",
    "        avg_r2 = total_r2 / num_batches if num_batches else float('nan')\n",
    "        avg_rmse = sqrt(avg_mse) if avg_mse >= 0 else float('nan')\n",
    "        avg_loss = running_loss / num_batches if num_batches else float('nan')\n",
    "        global_step += 1\n",
    "        \n",
    "        print(f'Iteration: {iteration}, Epoch: {epoch}, Avg Time per Batch: {avg_tpb:.4f} sec, '\n",
    "              f'Loss: {avg_loss:.8f}, MSE: {avg_mse:.4f}, MAE: {avg_mae:.4f}, '\n",
    "              f'RMSE: {avg_rmse:.4f}, R2: {avg_r2:.4f}')\n",
    "        AddDataToTensorboard(avg_tpb, avg_loss, avg_mse, avg_mae, avg_rmse, avg_r2, model, global_step) #This will map data to tensorboard\n",
    "        \n",
    "        tpb_list.clear()\n",
    "        # At the end of your training loop\n",
    "        with open('C:\\\\Machine Learning\\\\SPY VIX 9 Input Output Model\\\\last_step.txt', 'w') as f:\n",
    "            f.write(str(global_step))\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    print(f'Epoch {epoch} completed in {epoch_end_time - epoch_start_time:.2f} seconds')\n",
    "\n",
    "    del x_train_tensor, y_train_tensor  # Delete model and tensors\n",
    "    torch.cuda.empty_cache()  # Clear cache\n",
    "    import gc\n",
    "    gc.collect()  # Invoke garbage collector\n",
    "\n",
    "    save_checkpoint(model, optimizer, epoch, avg_loss, filepath)\n",
    "    print(f'Checkpoint saved at Iteration: {iteration}, Epoch: {epoch}, Loss: {avg_loss:.10f}')\n",
    "    \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorIntSystem = TensorIntegrationSystem(\"2018-01-01\", \"2024-01-01\")\n",
    "TensorIntSystem.CreateCombinedDataFrame()\n",
    "# Extract feature names from the DataFrame\n",
    "x_train_tensor, y_train_tensor = TensorIntSystem.CreateTensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth at epoch 19 with loss 3.374740635321058\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is WB\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([2295, 60, 24])\n",
      "Iteration: 0, Epoch: 0, Avg Time per Batch: 0.0822 sec, Loss: 3.35805530, MSE: 0.0521, MAE: 0.1798, RMSE: 0.2283, R2: -0.4668\n",
      "Iteration: 0, Epoch: 1, Avg Time per Batch: 0.0806 sec, Loss: 3.35805962, MSE: 0.0521, MAE: 0.1798, RMSE: 0.2283, R2: -0.4671\n",
      "Iteration: 0, Epoch: 2, Avg Time per Batch: 0.0812 sec, Loss: 3.35801347, MSE: 0.0521, MAE: 0.1797, RMSE: 0.2282, R2: -0.4656\n",
      "Iteration: 0, Epoch: 3, Avg Time per Batch: 0.0785 sec, Loss: 3.35804623, MSE: 0.0521, MAE: 0.1797, RMSE: 0.2283, R2: -0.4729\n",
      "Iteration: 0, Epoch: 4, Avg Time per Batch: 0.0779 sec, Loss: 3.35802833, MSE: 0.0521, MAE: 0.1797, RMSE: 0.2282, R2: -0.4734\n",
      "Epoch 4 completed in 94.69 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 0, Epoch: 4, Loss: 3.3580283288\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is HALL\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([6993, 60, 24])\n",
      "Iteration: 1, Epoch: 0, Avg Time per Batch: 0.0776 sec, Loss: 3.34190156, MSE: 0.0360, MAE: 0.1424, RMSE: 0.1896, R2: -4.0210\n",
      "Iteration: 1, Epoch: 1, Avg Time per Batch: 0.0749 sec, Loss: 3.34191283, MSE: 0.0360, MAE: 0.1424, RMSE: 0.1897, R2: -4.0256\n",
      "Iteration: 1, Epoch: 2, Avg Time per Batch: 0.0763 sec, Loss: 3.34191791, MSE: 0.0360, MAE: 0.1424, RMSE: 0.1897, R2: -4.0247\n",
      "Iteration: 1, Epoch: 3, Avg Time per Batch: 0.0766 sec, Loss: 3.34193578, MSE: 0.0360, MAE: 0.1425, RMSE: 0.1897, R2: -4.0347\n",
      "Iteration: 1, Epoch: 4, Avg Time per Batch: 0.0766 sec, Loss: 3.34194151, MSE: 0.0360, MAE: 0.1425, RMSE: 0.1897, R2: -4.0027\n",
      "Epoch 4 completed in 163.55 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 1, Epoch: 4, Loss: 3.3419415118\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is CNXN\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([6353, 60, 24])\n",
      "Iteration: 2, Epoch: 0, Avg Time per Batch: 0.0764 sec, Loss: 3.35583604, MSE: 0.0499, MAE: 0.1753, RMSE: 0.2234, R2: -7.0652\n",
      "Iteration: 2, Epoch: 1, Avg Time per Batch: 0.0753 sec, Loss: 3.35585372, MSE: 0.0499, MAE: 0.1753, RMSE: 0.2234, R2: -7.0201\n",
      "Iteration: 2, Epoch: 2, Avg Time per Batch: 0.0754 sec, Loss: 3.35585429, MSE: 0.0499, MAE: 0.1753, RMSE: 0.2234, R2: -6.9673\n",
      "Iteration: 2, Epoch: 3, Avg Time per Batch: 0.0751 sec, Loss: 3.35585900, MSE: 0.0499, MAE: 0.1753, RMSE: 0.2234, R2: -6.9767\n",
      "Iteration: 2, Epoch: 4, Avg Time per Batch: 0.0754 sec, Loss: 3.35584953, MSE: 0.0499, MAE: 0.1753, RMSE: 0.2234, R2: -6.9902\n",
      "Epoch 4 completed in 153.10 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 2, Epoch: 4, Loss: 3.3558495350\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is BXP\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([6530, 60, 24])\n",
      "Iteration: 3, Epoch: 0, Avg Time per Batch: 0.0766 sec, Loss: 3.35469482, MSE: 0.0488, MAE: 0.1680, RMSE: 0.2208, R2: -0.1499\n",
      "Iteration: 3, Epoch: 1, Avg Time per Batch: 0.0767 sec, Loss: 3.35471077, MSE: 0.0488, MAE: 0.1680, RMSE: 0.2208, R2: -0.1522\n",
      "Iteration: 3, Epoch: 2, Avg Time per Batch: 0.0760 sec, Loss: 3.35470673, MSE: 0.0488, MAE: 0.1680, RMSE: 0.2208, R2: -0.1523\n",
      "Iteration: 3, Epoch: 3, Avg Time per Batch: 0.0752 sec, Loss: 3.35471336, MSE: 0.0488, MAE: 0.1680, RMSE: 0.2208, R2: -0.1534\n",
      "Iteration: 3, Epoch: 4, Avg Time per Batch: 0.0754 sec, Loss: 3.35471439, MSE: 0.0488, MAE: 0.1680, RMSE: 0.2208, R2: -0.1519\n",
      "Epoch 4 completed in 155.66 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 3, Epoch: 4, Loss: 3.3547143892\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is NOAH\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([3158, 60, 24])\n",
      "Iteration: 4, Epoch: 0, Avg Time per Batch: 0.0785 sec, Loss: 3.35030744, MSE: 0.0444, MAE: 0.1637, RMSE: 0.2106, R2: -0.3265\n",
      "Iteration: 4, Epoch: 1, Avg Time per Batch: 0.0771 sec, Loss: 3.35034614, MSE: 0.0444, MAE: 0.1638, RMSE: 0.2107, R2: -0.3255\n",
      "Iteration: 4, Epoch: 2, Avg Time per Batch: 0.0766 sec, Loss: 3.35037627, MSE: 0.0444, MAE: 0.1638, RMSE: 0.2108, R2: -0.3293\n",
      "Iteration: 4, Epoch: 3, Avg Time per Batch: 0.0762 sec, Loss: 3.35035921, MSE: 0.0444, MAE: 0.1638, RMSE: 0.2108, R2: -0.3284\n",
      "Iteration: 4, Epoch: 4, Avg Time per Batch: 0.0776 sec, Loss: 3.35035317, MSE: 0.0444, MAE: 0.1638, RMSE: 0.2107, R2: -0.3277\n",
      "Epoch 4 completed in 105.41 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 4, Epoch: 4, Loss: 3.3503531662\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is SHEL\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([7195, 60, 24])\n",
      "Iteration: 5, Epoch: 0, Avg Time per Batch: 0.0758 sec, Loss: 3.34515584, MSE: 0.0392, MAE: 0.1516, RMSE: 0.1980, R2: -5.4320\n",
      "Iteration: 5, Epoch: 1, Avg Time per Batch: 0.0759 sec, Loss: 3.34517209, MSE: 0.0392, MAE: 0.1517, RMSE: 0.1981, R2: -5.4430\n",
      "Iteration: 5, Epoch: 2, Avg Time per Batch: 0.0749 sec, Loss: 3.34518231, MSE: 0.0392, MAE: 0.1517, RMSE: 0.1981, R2: -5.4487\n",
      "Iteration: 5, Epoch: 3, Avg Time per Batch: 0.0756 sec, Loss: 3.34518874, MSE: 0.0392, MAE: 0.1517, RMSE: 0.1981, R2: -5.4718\n",
      "Iteration: 5, Epoch: 4, Avg Time per Batch: 0.0748 sec, Loss: 3.34518994, MSE: 0.0392, MAE: 0.1517, RMSE: 0.1981, R2: -5.4647\n",
      "Epoch 4 completed in 164.63 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 5, Epoch: 4, Loss: 3.3451899351\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is OI\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([7404, 60, 24])\n",
      "Iteration: 6, Epoch: 0, Avg Time per Batch: 0.0746 sec, Loss: 3.34279576, MSE: 0.0369, MAE: 0.1474, RMSE: 0.1920, R2: -0.8502\n",
      "Iteration: 6, Epoch: 1, Avg Time per Batch: 0.0753 sec, Loss: 3.34278856, MSE: 0.0368, MAE: 0.1474, RMSE: 0.1920, R2: -0.8485\n",
      "Iteration: 6, Epoch: 2, Avg Time per Batch: 0.0760 sec, Loss: 3.34279093, MSE: 0.0369, MAE: 0.1474, RMSE: 0.1920, R2: -0.8494\n",
      "Iteration: 6, Epoch: 3, Avg Time per Batch: 0.0751 sec, Loss: 3.34278821, MSE: 0.0368, MAE: 0.1474, RMSE: 0.1920, R2: -0.8515\n",
      "Iteration: 6, Epoch: 4, Avg Time per Batch: 0.0752 sec, Loss: 3.34278669, MSE: 0.0368, MAE: 0.1474, RMSE: 0.1920, R2: -0.8512\n",
      "Epoch 4 completed in 165.87 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 6, Epoch: 4, Loss: 3.3427866851\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is ELP\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([6289, 60, 24])\n",
      "Iteration: 7, Epoch: 0, Avg Time per Batch: 0.0753 sec, Loss: 3.34887250, MSE: 0.0429, MAE: 0.1593, RMSE: 0.2072, R2: -1.0836\n",
      "Iteration: 7, Epoch: 1, Avg Time per Batch: 0.0762 sec, Loss: 3.34887620, MSE: 0.0429, MAE: 0.1594, RMSE: 0.2072, R2: -1.0782\n",
      "Iteration: 7, Epoch: 2, Avg Time per Batch: 0.0752 sec, Loss: 3.34886136, MSE: 0.0429, MAE: 0.1593, RMSE: 0.2072, R2: -1.0797\n",
      "Iteration: 7, Epoch: 3, Avg Time per Batch: 0.0751 sec, Loss: 3.34884818, MSE: 0.0429, MAE: 0.1593, RMSE: 0.2071, R2: -1.0783\n",
      "Iteration: 7, Epoch: 4, Avg Time per Batch: 0.0763 sec, Loss: 3.34884860, MSE: 0.0429, MAE: 0.1593, RMSE: 0.2071, R2: -1.0793\n",
      "Epoch 4 completed in 149.98 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 7, Epoch: 4, Loss: 3.3488485960\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is NERV\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([2244, 60, 24])\n",
      "Iteration: 8, Epoch: 0, Avg Time per Batch: 0.0800 sec, Loss: 3.35552112, MSE: 0.0496, MAE: 0.1719, RMSE: 0.2227, R2: -753.2702\n",
      "Iteration: 8, Epoch: 1, Avg Time per Batch: 0.0791 sec, Loss: 3.35549746, MSE: 0.0496, MAE: 0.1719, RMSE: 0.2226, R2: -740.3796\n",
      "Iteration: 8, Epoch: 2, Avg Time per Batch: 0.0801 sec, Loss: 3.35550006, MSE: 0.0496, MAE: 0.1719, RMSE: 0.2226, R2: -739.5181\n",
      "Iteration: 8, Epoch: 3, Avg Time per Batch: 0.0774 sec, Loss: 3.35550625, MSE: 0.0496, MAE: 0.1719, RMSE: 0.2226, R2: -738.1602\n",
      "Iteration: 8, Epoch: 4, Avg Time per Batch: 0.0795 sec, Loss: 3.35551024, MSE: 0.0496, MAE: 0.1719, RMSE: 0.2226, R2: -758.3698\n",
      "Epoch 4 completed in 92.68 seconds\n",
      "Checkpoint saved to C:\\Machine Learning\\SPY VIX 9 Input Output Model\\MegaNumMuncher3_Checkpoint-0.pth\n",
      "Checkpoint saved at Iteration: 8, Epoch: 4, Loss: 3.3555102383\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "This Ticker is MODD\n",
      "Data validation completed.\n",
      "Learning Rate: 0.001500, Batch Size: 32, Weight Decay: 0.0000005000 , X-Tensor Shape: torch.Size([326, 60, 24])\n",
      "Iteration: 9, Epoch: 0, Avg Time per Batch: 0.1085 sec, Loss: 3.39268788, MSE: 0.0867, MAE: 0.2390, RMSE: 0.2945, R2: -65.5953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 98\u001b[0m\n\u001b[0;32m     93\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Avg Time per Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_tpb\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_mse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_mae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, R2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_r2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m AddDataToTensorboard(avg_tpb, avg_loss, avg_mse, avg_mae, avg_rmse, avg_r2, model, global_step) \u001b[38;5;66;03m#This will map data to tensorboard\u001b[39;00m\n\u001b[0;32m    100\u001b[0m tpb_list\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# At the end of your training loop\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 25\u001b[0m, in \u001b[0;36mAddDataToTensorboard\u001b[1;34m(avg_tpb, avg_loss, avg_mse, avg_mae, avg_rmse, avg_r2, model, global_step)\u001b[0m\n\u001b[0;32m     23\u001b[0m tag \u001b[38;5;241m=\u001b[39m tag\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_histogram(tag, value\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), global_step)\n\u001b[0;32m     26\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_histogram(tag\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/grad\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), global_step)\n",
      "File \u001b[1;32mc:\\Users\\MtG\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:517\u001b[0m, in \u001b[0;36mSummaryWriter.add_histogram\u001b[1;34m(self, tag, values, global_step, bins, walltime, max_bins)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bins, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m bins \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    515\u001b[0m     bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_bins\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(\n\u001b[1;32m--> 517\u001b[0m     histogram(tag, values, bins, max_bins\u001b[38;5;241m=\u001b[39mmax_bins), global_step, walltime\n\u001b[0;32m    518\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MtG\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\summary.py:459\u001b[0m, in \u001b[0;36mhistogram\u001b[1;34m(name, values, bins, max_bins)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Outputs a `Summary` protocol buffer with a histogram.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03mThe generated\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;124;03m[`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m  buffer.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    458\u001b[0m values \u001b[38;5;241m=\u001b[39m make_np(values)\n\u001b[1;32m--> 459\u001b[0m hist \u001b[38;5;241m=\u001b[39m make_histogram(values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m), bins, max_bins)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Summary(value\u001b[38;5;241m=\u001b[39m[Summary\u001b[38;5;241m.\u001b[39mValue(tag\u001b[38;5;241m=\u001b[39mname, histo\u001b[38;5;241m=\u001b[39mhist)])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is the validation Cycle system.\n",
    "\n",
    "filepath = f\"C:\\\\Machine Learning\\\\SPY VIX 9 Input Output Model\\\\MegaNumMuncher3_Checkpoint-0.pth\"\n",
    "load_checkpoint(filepath, model, optimizer, device)\n",
    "\n",
    "writer = SummaryWriter(f'ValidationTesting/MegaNumMuncher2')\n",
    "global_step = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    epochs = 5\n",
    "    model.eval()\n",
    "    iteration = 0\n",
    "    l1_lambda = 0.0000900 #0.0000900 TO STABALZE\n",
    "    start_time = time.time()\n",
    "    tpb_list = []\n",
    "    learning_rate = 0.0010\n",
    "    weightDecay = 0.00001\n",
    "    \n",
    "    for iteration in range(50):\n",
    "        TensorIntSystem = TensorIntegrationSystem(\"2018-01-01\", \"2024-01-01\")\n",
    "        TensorIntSystem.CreateCombinedDataFrame()\n",
    "        # Extract feature names from the DataFrame\n",
    "        feature_names = TensorIntSystem.FormattedCombinedDataFrame.columns.tolist()\n",
    "        \n",
    "        x_train_tensor, y_train_tensor = TensorIntSystem.CreateTensors()\n",
    "        new_learning_rate, batch_size, new_weightDecay = FindOptimalTraining(x_train_tensor)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_learning_rate\n",
    "            param_group['weight_decay'] = new_weightDecay\n",
    "\n",
    "        # Initialize metrics for each iteration\n",
    "        running_loss = 0.0\n",
    "        total_mse = 0.0\n",
    "        total_mae = 0.0\n",
    "        total_r2 = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            permutation = torch.randperm(x_train_tensor.size(0))\n",
    "\n",
    "            for i in range(0, x_train_tensor.size(0), batch_size):\n",
    "                batch_start_time = time.time()\n",
    "\n",
    "                # Adjust batch size for the last batch\n",
    "                #end_idx = min(i + batch_size, x_train_tensor.size(0))\n",
    "                \n",
    "                end_idx = i + batch_size\n",
    "                # Adjust for the last batch if it has fewer samples\n",
    "                # Skip the batch if it does not fit the expected shape\n",
    "                if end_idx > x_train_tensor.size(0) - batch_size:\n",
    "                    continue\n",
    "\n",
    "                indices = permutation[i:end_idx]\n",
    "            \n",
    "                batch_seq = x_train_tensor[indices].to(device)\n",
    "                batch_label = y_train_tensor[indices].to(device)\n",
    "\n",
    "                y_pred = model(batch_seq)\n",
    "                y_pred = y_pred.view(batch_label.size())\n",
    "                loss = loss_function(y_pred, batch_label)\n",
    "\n",
    "                l1_penalty = sum(p.abs().sum() for p in model.parameters())\n",
    "                total_loss = loss + l1_lambda * l1_penalty\n",
    "                running_loss += total_loss.item()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2)\n",
    "\n",
    "                y_pred_flat = y_pred.view(y_pred.size(0), -1).cpu().detach().numpy()\n",
    "                batch_label_flat = batch_label.view(batch_label.size(0), -1).cpu().detach().numpy()\n",
    "\n",
    "                mse = mean_squared_error(batch_label_flat, y_pred_flat)\n",
    "                mae = mean_absolute_error(batch_label_flat, y_pred_flat)\n",
    "                r2 = r2_score(batch_label_flat, y_pred_flat)\n",
    "\n",
    "                total_mse += mse\n",
    "                total_mae += mae\n",
    "                total_r2 += r2\n",
    "                num_batches += 1\n",
    "\n",
    "                batch_end_time = time.time()\n",
    "                tpb_list.append(batch_end_time - batch_start_time)\n",
    "                \n",
    "                # Assuming `writer` is your SummaryWriter instance and `global_step` is defined\n",
    "                calculate_and_log_feature_metrics(batch_label_flat, y_pred_flat, feature_names, global_step)\n",
    "\n",
    "            # Calculate average metrics for each epoch\n",
    "            avg_tpb = sum(tpb_list) / len(tpb_list) if tpb_list else 0\n",
    "            avg_mse = total_mse / num_batches if num_batches else float('nan')\n",
    "            avg_mae = total_mae / num_batches if num_batches else float('nan')\n",
    "            avg_r2 = total_r2 / num_batches if num_batches else float('nan')\n",
    "            avg_rmse = sqrt(avg_mse) if avg_mse >= 0 else float('nan')\n",
    "            avg_loss = running_loss / num_batches if num_batches else float('nan')\n",
    "            global_step += 1\n",
    "            \n",
    "            print(f'Iteration: {iteration}, Epoch: {epoch}, Avg Time per Batch: {avg_tpb:.4f} sec, '\n",
    "                f'Loss: {avg_loss:.8f}, MSE: {avg_mse:.4f}, MAE: {avg_mae:.4f}, '\n",
    "                f'RMSE: {avg_rmse:.4f}, R2: {avg_r2:.4f}')\n",
    "            AddDataToTensorboard(avg_tpb, avg_loss, avg_mse, avg_mae, avg_rmse, avg_r2, model, global_step) #This will map data to tensorboard\n",
    "            \n",
    "            tpb_list.clear()\n",
    "            # At the end of your training loop\n",
    "            with open('C:\\\\Machine Learning\\\\SPY VIX 9 Input Output Model\\\\last_step.txt', 'w') as f:\n",
    "                f.write(str(global_step))\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f'Epoch {epoch} completed in {epoch_end_time - epoch_start_time:.2f} seconds')\n",
    "\n",
    "        del x_train_tensor, y_train_tensor  # Delete model and tensors\n",
    "        torch.cuda.empty_cache()  # Clear cache\n",
    "        import gc\n",
    "        gc.collect()  # Invoke garbage collector\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, avg_loss, filepath)\n",
    "        print(f'Checkpoint saved at Iteration: {iteration}, Epoch: {epoch}, Loss: {avg_loss:.10f}')\n",
    "        \n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTUNA TESTER\n",
    "\n",
    "# {'learning_rate': 0.0013167734585262175,\n",
    "#  'weight_decay': 1.2816205616802949e-10,\n",
    "#  'batch_size': 24}\n",
    "\n",
    "def CreateTensors():\n",
    "    \"\"\"\n",
    "    Testing different sizes in Optuna\n",
    "    \"\"\"\n",
    "    TensorIntSystem = TensorIntegrationSystem(\"1994-01-01\", \"2018-01-01\")\n",
    "    TensorIntSystem.CreateCombinedDataFrame()\n",
    "    x_train_tensor, y_train_tensor = TensorIntSystem.CreateTensors()\n",
    "    return x_train_tensor, y_train_tensor\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized by Optuna\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-10, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 24, 32, 40, 48, 56, 64])\n",
    "    l1_lambda = trial.suggest_float('l1_lambda', 0.00005, 0.001, log=True)\n",
    "    \n",
    "    epochs = 5\n",
    "    l1_lambda = 0.001\n",
    "    tpb_list = []\n",
    "    \n",
    "    model = HybridLSTMGRUAttentionWithCNN(input_size=30, \n",
    "                                        hidden_layer_size=600, \n",
    "                                        num_layers=30, \n",
    "                                        output_size=30 * 30,\n",
    "                                        dropout_rate=0.6,\n",
    "                                        num_heads=6, \n",
    "                                        kernel_size=6).to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    x_train_tensor, y_train_tensor = CreateTensors()\n",
    "    \n",
    "    # Initialize metrics for each iteration\n",
    "    running_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_r2 = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        permutation = torch.randperm(x_train_tensor.size(0))\n",
    "\n",
    "        for i in range(0, x_train_tensor.size(0), batch_size):\n",
    "            batch_start_time = time.time()\n",
    "\n",
    "            end_idx = i + batch_size\n",
    "            # Adjust for the last batch if it has fewer samples\n",
    "            # Skip the batch if it does not fit the expected shape\n",
    "            if end_idx > x_train_tensor.size(0) - batch_size:\n",
    "                continue\n",
    "\n",
    "            indices = permutation[i:end_idx]\n",
    "        \n",
    "            batch_seq = x_train_tensor[indices].to(device)\n",
    "            batch_label = y_train_tensor[indices].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_seq)\n",
    "            y_pred = y_pred.view(batch_label.size())\n",
    "            loss = loss_function(y_pred, batch_label)\n",
    "\n",
    "            l1_penalty = sum(p.abs().sum() for p in model.parameters())\n",
    "            total_loss = loss + l1_lambda * l1_penalty\n",
    "            running_loss += total_loss.item()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_pred_flat = y_pred.view(y_pred.size(0), -1).cpu().detach().numpy()\n",
    "            batch_label_flat = batch_label.view(batch_label.size(0), -1).cpu().detach().numpy()\n",
    "\n",
    "            mse = mean_squared_error(batch_label_flat, y_pred_flat)\n",
    "            mae = mean_absolute_error(batch_label_flat, y_pred_flat)\n",
    "            r2 = r2_score(batch_label_flat, y_pred_flat)\n",
    "\n",
    "            total_mse += mse\n",
    "            total_mae += mae\n",
    "            total_r2 += r2\n",
    "            num_batches += 1\n",
    "\n",
    "            batch_end_time = time.time()\n",
    "            tpb_list.append(batch_end_time - batch_start_time)\n",
    "\n",
    "        # Calculate average metrics for each epoch\n",
    "        avg_tpb = sum(tpb_list) / len(tpb_list) if tpb_list else 0\n",
    "        avg_mse = total_mse / num_batches if num_batches else float('nan')\n",
    "        avg_mae = total_mae / num_batches if num_batches else float('nan')\n",
    "        avg_r2 = total_r2 / num_batches if num_batches else float('nan')\n",
    "        avg_rmse = sqrt(avg_mse) if avg_mse >= 0 else float('nan')\n",
    "        avg_loss = running_loss / num_batches if num_batches else float('nan')\n",
    "        \n",
    "        # Inside the epoch loop\n",
    "        print(f'Trial: {trial.number}, Epoch: {epoch}, Avg Time per Batch: {avg_tpb:.4f} sec, '\n",
    "            f'Loss: {avg_loss:.8f}, MSE: {avg_mse:.4f}, MAE: {avg_mae:.4f}, '\n",
    "            f'RMSE: {avg_rmse:.4f}, R2: {avg_r2:.4f}')\n",
    "        \n",
    "        tpb_list.clear()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    print(f'Epoch {epoch} completed in {epoch_end_time - epoch_start_time:.2f} seconds')\n",
    "\n",
    "    del x_train_tensor, y_train_tensor  # Delete model and tensors\n",
    "    torch.cuda.empty_cache()  # Clear cache\n",
    "    import gc\n",
    "    gc.collect()  # Invoke garbage collector\n",
    "\n",
    "    #print(f'Checkpoint saved at Epoch: {epoch}, Loss: {avg_loss:.10f}')\n",
    "    \n",
    "    # At the end of the objective function\n",
    "    return avg_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study object and specify the direction is 'minimize'.\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Optimize the study, the objective function is being called here.\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_x</th>\n",
       "      <th>High_x</th>\n",
       "      <th>Low_x</th>\n",
       "      <th>Close_x</th>\n",
       "      <th>Volume</th>\n",
       "      <th>EMA20_Close_x</th>\n",
       "      <th>EMA40_Close_x</th>\n",
       "      <th>EMA60_Close_x</th>\n",
       "      <th>RSI20_Close_x</th>\n",
       "      <th>RSI40_Close_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Close_y</th>\n",
       "      <th>EMA20_Close_y</th>\n",
       "      <th>EMA40_Close_y</th>\n",
       "      <th>EMA60_Close_y</th>\n",
       "      <th>RSI20_Close_y</th>\n",
       "      <th>RSI40_Close_y</th>\n",
       "      <th>RSI60_Close_y</th>\n",
       "      <th>shifted_day_of_year</th>\n",
       "      <th>cos_shifted_annual</th>\n",
       "      <th>sin_shifted_annual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-07-12</th>\n",
       "      <td>189.174968</td>\n",
       "      <td>191.189594</td>\n",
       "      <td>187.968198</td>\n",
       "      <td>189.264740</td>\n",
       "      <td>60750200</td>\n",
       "      <td>186.967469</td>\n",
       "      <td>182.281242</td>\n",
       "      <td>177.955451</td>\n",
       "      <td>62.503099</td>\n",
       "      <td>62.598253</td>\n",
       "      <td>...</td>\n",
       "      <td>13.540000</td>\n",
       "      <td>14.388288</td>\n",
       "      <td>15.115892</td>\n",
       "      <td>15.890125</td>\n",
       "      <td>43.573185</td>\n",
       "      <td>44.658958</td>\n",
       "      <td>45.457967</td>\n",
       "      <td>134</td>\n",
       "      <td>-0.670089</td>\n",
       "      <td>0.742281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-13</th>\n",
       "      <td>189.992798</td>\n",
       "      <td>190.680963</td>\n",
       "      <td>189.274713</td>\n",
       "      <td>190.032684</td>\n",
       "      <td>41342300</td>\n",
       "      <td>187.259395</td>\n",
       "      <td>182.659361</td>\n",
       "      <td>178.351425</td>\n",
       "      <td>63.539759</td>\n",
       "      <td>63.079535</td>\n",
       "      <td>...</td>\n",
       "      <td>13.610000</td>\n",
       "      <td>14.314166</td>\n",
       "      <td>15.042434</td>\n",
       "      <td>15.815366</td>\n",
       "      <td>43.901797</td>\n",
       "      <td>44.795659</td>\n",
       "      <td>45.539996</td>\n",
       "      <td>135</td>\n",
       "      <td>-0.682758</td>\n",
       "      <td>0.730644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-14</th>\n",
       "      <td>189.723515</td>\n",
       "      <td>190.670982</td>\n",
       "      <td>189.125121</td>\n",
       "      <td>190.182297</td>\n",
       "      <td>41573900</td>\n",
       "      <td>187.537766</td>\n",
       "      <td>183.026333</td>\n",
       "      <td>178.739323</td>\n",
       "      <td>63.745310</td>\n",
       "      <td>63.174223</td>\n",
       "      <td>...</td>\n",
       "      <td>13.340000</td>\n",
       "      <td>14.221388</td>\n",
       "      <td>14.959388</td>\n",
       "      <td>15.734207</td>\n",
       "      <td>42.887713</td>\n",
       "      <td>44.362148</td>\n",
       "      <td>45.272916</td>\n",
       "      <td>136</td>\n",
       "      <td>-0.695225</td>\n",
       "      <td>0.718792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-17</th>\n",
       "      <td>191.389049</td>\n",
       "      <td>193.802619</td>\n",
       "      <td>191.299292</td>\n",
       "      <td>193.473495</td>\n",
       "      <td>50520200</td>\n",
       "      <td>188.103074</td>\n",
       "      <td>183.535951</td>\n",
       "      <td>179.222411</td>\n",
       "      <td>67.931691</td>\n",
       "      <td>65.188552</td>\n",
       "      <td>...</td>\n",
       "      <td>13.480000</td>\n",
       "      <td>14.150779</td>\n",
       "      <td>14.887223</td>\n",
       "      <td>15.660298</td>\n",
       "      <td>43.598794</td>\n",
       "      <td>44.647029</td>\n",
       "      <td>45.441638</td>\n",
       "      <td>139</td>\n",
       "      <td>-0.731378</td>\n",
       "      <td>0.681972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-18</th>\n",
       "      <td>192.835210</td>\n",
       "      <td>193.812596</td>\n",
       "      <td>191.907678</td>\n",
       "      <td>193.214188</td>\n",
       "      <td>48353800</td>\n",
       "      <td>188.589847</td>\n",
       "      <td>184.008060</td>\n",
       "      <td>179.681157</td>\n",
       "      <td>67.287306</td>\n",
       "      <td>64.901678</td>\n",
       "      <td>...</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>14.069753</td>\n",
       "      <td>14.809797</td>\n",
       "      <td>15.582912</td>\n",
       "      <td>42.876310</td>\n",
       "      <td>44.347594</td>\n",
       "      <td>45.259198</td>\n",
       "      <td>140</td>\n",
       "      <td>-0.743001</td>\n",
       "      <td>0.669290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-19</th>\n",
       "      <td>192.585876</td>\n",
       "      <td>197.702207</td>\n",
       "      <td>192.137062</td>\n",
       "      <td>194.580551</td>\n",
       "      <td>80507300</td>\n",
       "      <td>189.160390</td>\n",
       "      <td>184.523791</td>\n",
       "      <td>180.169662</td>\n",
       "      <td>68.922418</td>\n",
       "      <td>65.717028</td>\n",
       "      <td>...</td>\n",
       "      <td>13.760000</td>\n",
       "      <td>14.040253</td>\n",
       "      <td>14.758588</td>\n",
       "      <td>15.523144</td>\n",
       "      <td>45.314081</td>\n",
       "      <td>45.309005</td>\n",
       "      <td>45.824467</td>\n",
       "      <td>141</td>\n",
       "      <td>-0.754404</td>\n",
       "      <td>0.656411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-20</th>\n",
       "      <td>194.570572</td>\n",
       "      <td>195.946903</td>\n",
       "      <td>191.987471</td>\n",
       "      <td>192.615799</td>\n",
       "      <td>59581200</td>\n",
       "      <td>189.489476</td>\n",
       "      <td>184.918523</td>\n",
       "      <td>180.577732</td>\n",
       "      <td>64.074717</td>\n",
       "      <td>63.540106</td>\n",
       "      <td>...</td>\n",
       "      <td>13.990000</td>\n",
       "      <td>14.035467</td>\n",
       "      <td>14.721096</td>\n",
       "      <td>15.472877</td>\n",
       "      <td>46.515380</td>\n",
       "      <td>45.789263</td>\n",
       "      <td>46.107438</td>\n",
       "      <td>142</td>\n",
       "      <td>-0.765584</td>\n",
       "      <td>0.643337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-21</th>\n",
       "      <td>193.583223</td>\n",
       "      <td>194.450902</td>\n",
       "      <td>190.720854</td>\n",
       "      <td>191.428970</td>\n",
       "      <td>71917800</td>\n",
       "      <td>189.674190</td>\n",
       "      <td>185.236106</td>\n",
       "      <td>180.933510</td>\n",
       "      <td>61.331770</td>\n",
       "      <td>62.262298</td>\n",
       "      <td>...</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>13.993994</td>\n",
       "      <td>14.666408</td>\n",
       "      <td>15.411471</td>\n",
       "      <td>44.760358</td>\n",
       "      <td>45.100495</td>\n",
       "      <td>45.695861</td>\n",
       "      <td>143</td>\n",
       "      <td>-0.776537</td>\n",
       "      <td>0.630072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-24</th>\n",
       "      <td>192.895048</td>\n",
       "      <td>194.391054</td>\n",
       "      <td>191.738132</td>\n",
       "      <td>192.236801</td>\n",
       "      <td>45377800</td>\n",
       "      <td>189.918248</td>\n",
       "      <td>185.577604</td>\n",
       "      <td>181.304110</td>\n",
       "      <td>62.482500</td>\n",
       "      <td>62.784774</td>\n",
       "      <td>...</td>\n",
       "      <td>13.910000</td>\n",
       "      <td>13.985994</td>\n",
       "      <td>14.629510</td>\n",
       "      <td>15.362243</td>\n",
       "      <td>46.450845</td>\n",
       "      <td>45.765579</td>\n",
       "      <td>46.084894</td>\n",
       "      <td>146</td>\n",
       "      <td>-0.808005</td>\n",
       "      <td>0.589176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-25</th>\n",
       "      <td>192.815256</td>\n",
       "      <td>193.922301</td>\n",
       "      <td>192.406344</td>\n",
       "      <td>193.104477</td>\n",
       "      <td>37283200</td>\n",
       "      <td>190.221699</td>\n",
       "      <td>185.944768</td>\n",
       "      <td>181.691007</td>\n",
       "      <td>63.703722</td>\n",
       "      <td>63.343849</td>\n",
       "      <td>...</td>\n",
       "      <td>13.860000</td>\n",
       "      <td>13.973995</td>\n",
       "      <td>14.591973</td>\n",
       "      <td>15.312989</td>\n",
       "      <td>46.210746</td>\n",
       "      <td>45.674045</td>\n",
       "      <td>46.030805</td>\n",
       "      <td>147</td>\n",
       "      <td>-0.818020</td>\n",
       "      <td>0.575190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-26</th>\n",
       "      <td>193.154355</td>\n",
       "      <td>195.119111</td>\n",
       "      <td>192.805296</td>\n",
       "      <td>193.982147</td>\n",
       "      <td>47471900</td>\n",
       "      <td>190.579837</td>\n",
       "      <td>186.336835</td>\n",
       "      <td>182.093996</td>\n",
       "      <td>64.919561</td>\n",
       "      <td>63.906383</td>\n",
       "      <td>...</td>\n",
       "      <td>13.190000</td>\n",
       "      <td>13.899329</td>\n",
       "      <td>14.523584</td>\n",
       "      <td>15.243383</td>\n",
       "      <td>43.070545</td>\n",
       "      <td>44.452144</td>\n",
       "      <td>45.306179</td>\n",
       "      <td>148</td>\n",
       "      <td>-0.827793</td>\n",
       "      <td>0.561034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-27</th>\n",
       "      <td>195.498099</td>\n",
       "      <td>196.674950</td>\n",
       "      <td>192.037337</td>\n",
       "      <td>192.705551</td>\n",
       "      <td>47460200</td>\n",
       "      <td>190.782286</td>\n",
       "      <td>186.647504</td>\n",
       "      <td>182.441915</td>\n",
       "      <td>61.752426</td>\n",
       "      <td>62.476063</td>\n",
       "      <td>...</td>\n",
       "      <td>14.410000</td>\n",
       "      <td>13.947964</td>\n",
       "      <td>14.518043</td>\n",
       "      <td>15.216059</td>\n",
       "      <td>49.631071</td>\n",
       "      <td>47.095406</td>\n",
       "      <td>46.855384</td>\n",
       "      <td>149</td>\n",
       "      <td>-0.837321</td>\n",
       "      <td>0.546711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-28</th>\n",
       "      <td>194.151694</td>\n",
       "      <td>196.106482</td>\n",
       "      <td>193.623106</td>\n",
       "      <td>195.308609</td>\n",
       "      <td>48291400</td>\n",
       "      <td>191.213364</td>\n",
       "      <td>187.069997</td>\n",
       "      <td>182.863774</td>\n",
       "      <td>65.377797</td>\n",
       "      <td>64.153926</td>\n",
       "      <td>...</td>\n",
       "      <td>13.330000</td>\n",
       "      <td>13.889110</td>\n",
       "      <td>14.460090</td>\n",
       "      <td>15.154221</td>\n",
       "      <td>44.818280</td>\n",
       "      <td>45.144928</td>\n",
       "      <td>45.690299</td>\n",
       "      <td>150</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>0.532227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31</th>\n",
       "      <td>195.537995</td>\n",
       "      <td>195.966858</td>\n",
       "      <td>194.740122</td>\n",
       "      <td>195.926956</td>\n",
       "      <td>38824100</td>\n",
       "      <td>191.662278</td>\n",
       "      <td>187.502044</td>\n",
       "      <td>183.292075</td>\n",
       "      <td>66.179392</td>\n",
       "      <td>64.540228</td>\n",
       "      <td>...</td>\n",
       "      <td>13.630000</td>\n",
       "      <td>13.864433</td>\n",
       "      <td>14.419598</td>\n",
       "      <td>15.104246</td>\n",
       "      <td>46.339773</td>\n",
       "      <td>45.784631</td>\n",
       "      <td>46.069119</td>\n",
       "      <td>153</td>\n",
       "      <td>-0.872929</td>\n",
       "      <td>0.487847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01</th>\n",
       "      <td>195.717515</td>\n",
       "      <td>196.206201</td>\n",
       "      <td>194.760064</td>\n",
       "      <td>195.089188</td>\n",
       "      <td>35175100</td>\n",
       "      <td>191.988650</td>\n",
       "      <td>187.872149</td>\n",
       "      <td>183.678866</td>\n",
       "      <td>64.064040</td>\n",
       "      <td>63.587984</td>\n",
       "      <td>...</td>\n",
       "      <td>13.930000</td>\n",
       "      <td>13.870678</td>\n",
       "      <td>14.395715</td>\n",
       "      <td>15.065746</td>\n",
       "      <td>47.853259</td>\n",
       "      <td>46.425421</td>\n",
       "      <td>46.448977</td>\n",
       "      <td>154</td>\n",
       "      <td>-0.881192</td>\n",
       "      <td>0.472759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-02</th>\n",
       "      <td>194.520703</td>\n",
       "      <td>194.660329</td>\n",
       "      <td>191.339209</td>\n",
       "      <td>192.067261</td>\n",
       "      <td>50389300</td>\n",
       "      <td>191.996137</td>\n",
       "      <td>188.076788</td>\n",
       "      <td>183.953895</td>\n",
       "      <td>57.130371</td>\n",
       "      <td>60.296690</td>\n",
       "      <td>...</td>\n",
       "      <td>16.090000</td>\n",
       "      <td>14.082042</td>\n",
       "      <td>14.478363</td>\n",
       "      <td>15.099328</td>\n",
       "      <td>57.037165</td>\n",
       "      <td>50.726105</td>\n",
       "      <td>49.075280</td>\n",
       "      <td>155</td>\n",
       "      <td>-0.889193</td>\n",
       "      <td>0.457531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-03</th>\n",
       "      <td>191.059955</td>\n",
       "      <td>191.857813</td>\n",
       "      <td>190.182293</td>\n",
       "      <td>190.661011</td>\n",
       "      <td>61235200</td>\n",
       "      <td>191.868982</td>\n",
       "      <td>188.202848</td>\n",
       "      <td>184.173801</td>\n",
       "      <td>54.254059</td>\n",
       "      <td>58.843037</td>\n",
       "      <td>...</td>\n",
       "      <td>15.920000</td>\n",
       "      <td>14.257085</td>\n",
       "      <td>14.548687</td>\n",
       "      <td>15.126236</td>\n",
       "      <td>56.216928</td>\n",
       "      <td>50.399520</td>\n",
       "      <td>48.883399</td>\n",
       "      <td>156</td>\n",
       "      <td>-0.896932</td>\n",
       "      <td>0.442168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-04</th>\n",
       "      <td>185.026063</td>\n",
       "      <td>186.881111</td>\n",
       "      <td>181.435642</td>\n",
       "      <td>181.505463</td>\n",
       "      <td>115799700</td>\n",
       "      <td>190.881980</td>\n",
       "      <td>187.876146</td>\n",
       "      <td>184.086314</td>\n",
       "      <td>40.336444</td>\n",
       "      <td>50.683734</td>\n",
       "      <td>...</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>14.527839</td>\n",
       "      <td>14.673141</td>\n",
       "      <td>15.190949</td>\n",
       "      <td>60.379924</td>\n",
       "      <td>52.573301</td>\n",
       "      <td>50.256304</td>\n",
       "      <td>157</td>\n",
       "      <td>-0.904405</td>\n",
       "      <td>0.426674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-07</th>\n",
       "      <td>181.645075</td>\n",
       "      <td>182.642413</td>\n",
       "      <td>176.877804</td>\n",
       "      <td>178.373810</td>\n",
       "      <td>97576100</td>\n",
       "      <td>189.690726</td>\n",
       "      <td>187.412618</td>\n",
       "      <td>183.899019</td>\n",
       "      <td>36.925860</td>\n",
       "      <td>48.332572</td>\n",
       "      <td>...</td>\n",
       "      <td>15.770000</td>\n",
       "      <td>14.646140</td>\n",
       "      <td>14.726647</td>\n",
       "      <td>15.209934</td>\n",
       "      <td>54.258995</td>\n",
       "      <td>50.038194</td>\n",
       "      <td>48.755342</td>\n",
       "      <td>160</td>\n",
       "      <td>-0.925211</td>\n",
       "      <td>0.379453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-08</th>\n",
       "      <td>179.211566</td>\n",
       "      <td>179.790024</td>\n",
       "      <td>177.107183</td>\n",
       "      <td>179.321274</td>\n",
       "      <td>67823000</td>\n",
       "      <td>188.703159</td>\n",
       "      <td>187.017918</td>\n",
       "      <td>183.748929</td>\n",
       "      <td>38.579756</td>\n",
       "      <td>49.065748</td>\n",
       "      <td>...</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>14.774127</td>\n",
       "      <td>14.788274</td>\n",
       "      <td>15.235510</td>\n",
       "      <td>55.052368</td>\n",
       "      <td>50.443605</td>\n",
       "      <td>49.011508</td>\n",
       "      <td>161</td>\n",
       "      <td>-0.931601</td>\n",
       "      <td>0.363482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-09</th>\n",
       "      <td>180.388433</td>\n",
       "      <td>180.448271</td>\n",
       "      <td>176.538710</td>\n",
       "      <td>177.715576</td>\n",
       "      <td>60378500</td>\n",
       "      <td>187.656722</td>\n",
       "      <td>186.564145</td>\n",
       "      <td>183.551114</td>\n",
       "      <td>36.855747</td>\n",
       "      <td>47.884655</td>\n",
       "      <td>...</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>14.887067</td>\n",
       "      <td>14.845431</td>\n",
       "      <td>15.259264</td>\n",
       "      <td>54.915646</td>\n",
       "      <td>50.386422</td>\n",
       "      <td>48.977556</td>\n",
       "      <td>162</td>\n",
       "      <td>-0.937716</td>\n",
       "      <td>0.347403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-10</th>\n",
       "      <td>179.002129</td>\n",
       "      <td>180.268752</td>\n",
       "      <td>177.127145</td>\n",
       "      <td>177.496155</td>\n",
       "      <td>54686900</td>\n",
       "      <td>186.689049</td>\n",
       "      <td>186.121804</td>\n",
       "      <td>183.352591</td>\n",
       "      <td>36.620353</td>\n",
       "      <td>47.723646</td>\n",
       "      <td>...</td>\n",
       "      <td>15.850000</td>\n",
       "      <td>14.978775</td>\n",
       "      <td>14.894434</td>\n",
       "      <td>15.278633</td>\n",
       "      <td>54.394250</td>\n",
       "      <td>50.172532</td>\n",
       "      <td>48.851368</td>\n",
       "      <td>163</td>\n",
       "      <td>-0.943553</td>\n",
       "      <td>0.331221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-11</th>\n",
       "      <td>177.086708</td>\n",
       "      <td>178.384986</td>\n",
       "      <td>176.317717</td>\n",
       "      <td>177.556076</td>\n",
       "      <td>51988100</td>\n",
       "      <td>185.819242</td>\n",
       "      <td>185.703964</td>\n",
       "      <td>183.162541</td>\n",
       "      <td>36.736504</td>\n",
       "      <td>47.772832</td>\n",
       "      <td>...</td>\n",
       "      <td>14.840000</td>\n",
       "      <td>14.965558</td>\n",
       "      <td>14.891779</td>\n",
       "      <td>15.264251</td>\n",
       "      <td>49.822294</td>\n",
       "      <td>48.243912</td>\n",
       "      <td>47.703742</td>\n",
       "      <td>164</td>\n",
       "      <td>-0.949111</td>\n",
       "      <td>0.314942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-14</th>\n",
       "      <td>177.735847</td>\n",
       "      <td>179.453585</td>\n",
       "      <td>177.076712</td>\n",
       "      <td>179.223892</td>\n",
       "      <td>43675600</td>\n",
       "      <td>185.191114</td>\n",
       "      <td>185.387863</td>\n",
       "      <td>183.033405</td>\n",
       "      <td>39.960197</td>\n",
       "      <td>49.138956</td>\n",
       "      <td>...</td>\n",
       "      <td>14.820000</td>\n",
       "      <td>14.951696</td>\n",
       "      <td>14.888278</td>\n",
       "      <td>15.249686</td>\n",
       "      <td>49.735156</td>\n",
       "      <td>48.206277</td>\n",
       "      <td>47.681185</td>\n",
       "      <td>167</td>\n",
       "      <td>-0.964094</td>\n",
       "      <td>0.265563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-15</th>\n",
       "      <td>178.644649</td>\n",
       "      <td>179.243850</td>\n",
       "      <td>176.817055</td>\n",
       "      <td>177.216522</td>\n",
       "      <td>43622600</td>\n",
       "      <td>184.431629</td>\n",
       "      <td>184.989261</td>\n",
       "      <td>182.842688</td>\n",
       "      <td>37.536858</td>\n",
       "      <td>47.601890</td>\n",
       "      <td>...</td>\n",
       "      <td>16.459999</td>\n",
       "      <td>15.095344</td>\n",
       "      <td>14.964947</td>\n",
       "      <td>15.289368</td>\n",
       "      <td>56.327922</td>\n",
       "      <td>51.395129</td>\n",
       "      <td>49.665914</td>\n",
       "      <td>168</td>\n",
       "      <td>-0.968519</td>\n",
       "      <td>0.248940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-16</th>\n",
       "      <td>176.896953</td>\n",
       "      <td>178.305086</td>\n",
       "      <td>176.267777</td>\n",
       "      <td>176.337692</td>\n",
       "      <td>46964900</td>\n",
       "      <td>183.660778</td>\n",
       "      <td>184.567233</td>\n",
       "      <td>182.629409</td>\n",
       "      <td>36.516325</td>\n",
       "      <td>46.942555</td>\n",
       "      <td>...</td>\n",
       "      <td>16.780001</td>\n",
       "      <td>15.255787</td>\n",
       "      <td>15.053486</td>\n",
       "      <td>15.338241</td>\n",
       "      <td>57.473559</td>\n",
       "      <td>51.986721</td>\n",
       "      <td>50.041973</td>\n",
       "      <td>169</td>\n",
       "      <td>-0.972658</td>\n",
       "      <td>0.232243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-17</th>\n",
       "      <td>176.906941</td>\n",
       "      <td>177.276449</td>\n",
       "      <td>173.251752</td>\n",
       "      <td>173.771072</td>\n",
       "      <td>66062900</td>\n",
       "      <td>182.718901</td>\n",
       "      <td>184.040591</td>\n",
       "      <td>182.338972</td>\n",
       "      <td>33.699703</td>\n",
       "      <td>45.072537</td>\n",
       "      <td>...</td>\n",
       "      <td>17.889999</td>\n",
       "      <td>15.506664</td>\n",
       "      <td>15.191853</td>\n",
       "      <td>15.421905</td>\n",
       "      <td>61.190832</td>\n",
       "      <td>53.979502</td>\n",
       "      <td>51.324810</td>\n",
       "      <td>170</td>\n",
       "      <td>-0.976509</td>\n",
       "      <td>0.215477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-18</th>\n",
       "      <td>172.073301</td>\n",
       "      <td>174.869620</td>\n",
       "      <td>171.733752</td>\n",
       "      <td>174.260422</td>\n",
       "      <td>61114200</td>\n",
       "      <td>181.913332</td>\n",
       "      <td>183.563510</td>\n",
       "      <td>182.074101</td>\n",
       "      <td>34.710396</td>\n",
       "      <td>45.497109</td>\n",
       "      <td>...</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>15.677458</td>\n",
       "      <td>15.294689</td>\n",
       "      <td>15.483482</td>\n",
       "      <td>58.337707</td>\n",
       "      <td>52.785143</td>\n",
       "      <td>50.622165</td>\n",
       "      <td>171</td>\n",
       "      <td>-0.980071</td>\n",
       "      <td>0.198648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-21</th>\n",
       "      <td>174.839667</td>\n",
       "      <td>175.898270</td>\n",
       "      <td>173.511415</td>\n",
       "      <td>175.608643</td>\n",
       "      <td>46311900</td>\n",
       "      <td>181.312885</td>\n",
       "      <td>183.175467</td>\n",
       "      <td>181.862119</td>\n",
       "      <td>37.474653</td>\n",
       "      <td>46.662134</td>\n",
       "      <td>...</td>\n",
       "      <td>17.129999</td>\n",
       "      <td>15.815796</td>\n",
       "      <td>15.384216</td>\n",
       "      <td>15.537466</td>\n",
       "      <td>57.524205</td>\n",
       "      <td>52.442233</td>\n",
       "      <td>50.419907</td>\n",
       "      <td>174</td>\n",
       "      <td>-0.989013</td>\n",
       "      <td>0.147827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-22</th>\n",
       "      <td>176.827036</td>\n",
       "      <td>177.446216</td>\n",
       "      <td>176.018105</td>\n",
       "      <td>176.996811</td>\n",
       "      <td>42084200</td>\n",
       "      <td>180.901830</td>\n",
       "      <td>182.874069</td>\n",
       "      <td>181.702601</td>\n",
       "      <td>40.217889</td>\n",
       "      <td>47.839559</td>\n",
       "      <td>...</td>\n",
       "      <td>16.969999</td>\n",
       "      <td>15.925720</td>\n",
       "      <td>15.461572</td>\n",
       "      <td>15.584435</td>\n",
       "      <td>56.740329</td>\n",
       "      <td>52.115420</td>\n",
       "      <td>50.227828</td>\n",
       "      <td>175</td>\n",
       "      <td>-0.991410</td>\n",
       "      <td>0.130793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-23</th>\n",
       "      <td>178.285129</td>\n",
       "      <td>181.311141</td>\n",
       "      <td>178.095376</td>\n",
       "      <td>180.881699</td>\n",
       "      <td>52722800</td>\n",
       "      <td>180.899913</td>\n",
       "      <td>182.776881</td>\n",
       "      <td>181.675686</td>\n",
       "      <td>47.060189</td>\n",
       "      <td>50.947620</td>\n",
       "      <td>...</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>15.930889</td>\n",
       "      <td>15.486861</td>\n",
       "      <td>15.597404</td>\n",
       "      <td>52.114910</td>\n",
       "      <td>50.132746</td>\n",
       "      <td>49.051985</td>\n",
       "      <td>176</td>\n",
       "      <td>-0.993513</td>\n",
       "      <td>0.113720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-24</th>\n",
       "      <td>180.432283</td>\n",
       "      <td>180.861725</td>\n",
       "      <td>175.778410</td>\n",
       "      <td>176.147934</td>\n",
       "      <td>54945800</td>\n",
       "      <td>180.447344</td>\n",
       "      <td>182.453517</td>\n",
       "      <td>181.494448</td>\n",
       "      <td>41.035977</td>\n",
       "      <td>47.416584</td>\n",
       "      <td>...</td>\n",
       "      <td>17.200001</td>\n",
       "      <td>16.051757</td>\n",
       "      <td>15.570429</td>\n",
       "      <td>15.649948</td>\n",
       "      <td>56.694283</td>\n",
       "      <td>52.420581</td>\n",
       "      <td>50.504092</td>\n",
       "      <td>177</td>\n",
       "      <td>-0.995322</td>\n",
       "      <td>0.096613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-25</th>\n",
       "      <td>177.146623</td>\n",
       "      <td>178.914283</td>\n",
       "      <td>175.588678</td>\n",
       "      <td>178.375000</td>\n",
       "      <td>51449600</td>\n",
       "      <td>180.249978</td>\n",
       "      <td>182.254565</td>\n",
       "      <td>181.392171</td>\n",
       "      <td>44.551114</td>\n",
       "      <td>49.118204</td>\n",
       "      <td>...</td>\n",
       "      <td>15.680000</td>\n",
       "      <td>16.016352</td>\n",
       "      <td>15.575774</td>\n",
       "      <td>15.650933</td>\n",
       "      <td>50.376127</td>\n",
       "      <td>49.517578</td>\n",
       "      <td>48.743844</td>\n",
       "      <td>178</td>\n",
       "      <td>-0.996837</td>\n",
       "      <td>0.079477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-28</th>\n",
       "      <td>179.853052</td>\n",
       "      <td>180.352394</td>\n",
       "      <td>178.315085</td>\n",
       "      <td>179.952927</td>\n",
       "      <td>43820700</td>\n",
       "      <td>180.221687</td>\n",
       "      <td>182.142290</td>\n",
       "      <td>181.344983</td>\n",
       "      <td>46.911514</td>\n",
       "      <td>50.287247</td>\n",
       "      <td>...</td>\n",
       "      <td>15.080000</td>\n",
       "      <td>15.927175</td>\n",
       "      <td>15.551589</td>\n",
       "      <td>15.632214</td>\n",
       "      <td>48.146658</td>\n",
       "      <td>48.431706</td>\n",
       "      <td>48.071270</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.999609</td>\n",
       "      <td>0.027950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-29</th>\n",
       "      <td>179.463564</td>\n",
       "      <td>184.656719</td>\n",
       "      <td>179.263830</td>\n",
       "      <td>183.877747</td>\n",
       "      <td>53003900</td>\n",
       "      <td>180.569883</td>\n",
       "      <td>182.226947</td>\n",
       "      <td>181.428024</td>\n",
       "      <td>52.235174</td>\n",
       "      <td>53.039744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.450000</td>\n",
       "      <td>15.786492</td>\n",
       "      <td>15.497853</td>\n",
       "      <td>15.593453</td>\n",
       "      <td>45.901391</td>\n",
       "      <td>47.314337</td>\n",
       "      <td>47.373292</td>\n",
       "      <td>182</td>\n",
       "      <td>-0.999942</td>\n",
       "      <td>0.010751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-30</th>\n",
       "      <td>184.696681</td>\n",
       "      <td>187.602856</td>\n",
       "      <td>184.496947</td>\n",
       "      <td>187.403107</td>\n",
       "      <td>60813900</td>\n",
       "      <td>181.220666</td>\n",
       "      <td>182.479442</td>\n",
       "      <td>181.623929</td>\n",
       "      <td>56.371722</td>\n",
       "      <td>55.318840</td>\n",
       "      <td>...</td>\n",
       "      <td>13.880000</td>\n",
       "      <td>15.604921</td>\n",
       "      <td>15.418934</td>\n",
       "      <td>15.537274</td>\n",
       "      <td>43.949455</td>\n",
       "      <td>46.322617</td>\n",
       "      <td>46.748753</td>\n",
       "      <td>183</td>\n",
       "      <td>-0.999979</td>\n",
       "      <td>-0.006451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-31</th>\n",
       "      <td>187.592859</td>\n",
       "      <td>188.871173</td>\n",
       "      <td>187.233332</td>\n",
       "      <td>187.622818</td>\n",
       "      <td>60794500</td>\n",
       "      <td>181.830395</td>\n",
       "      <td>182.730339</td>\n",
       "      <td>181.820614</td>\n",
       "      <td>56.618192</td>\n",
       "      <td>55.457024</td>\n",
       "      <td>...</td>\n",
       "      <td>13.570000</td>\n",
       "      <td>15.411119</td>\n",
       "      <td>15.328742</td>\n",
       "      <td>15.472774</td>\n",
       "      <td>42.904948</td>\n",
       "      <td>45.787283</td>\n",
       "      <td>46.410355</td>\n",
       "      <td>184</td>\n",
       "      <td>-0.999720</td>\n",
       "      <td>-0.023651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-01</th>\n",
       "      <td>189.240698</td>\n",
       "      <td>189.670125</td>\n",
       "      <td>188.032284</td>\n",
       "      <td>189.210739</td>\n",
       "      <td>45732600</td>\n",
       "      <td>182.533285</td>\n",
       "      <td>183.046456</td>\n",
       "      <td>182.062913</td>\n",
       "      <td>58.405832</td>\n",
       "      <td>56.455273</td>\n",
       "      <td>...</td>\n",
       "      <td>13.090000</td>\n",
       "      <td>15.190060</td>\n",
       "      <td>15.219535</td>\n",
       "      <td>15.394650</td>\n",
       "      <td>41.304966</td>\n",
       "      <td>44.962097</td>\n",
       "      <td>45.887323</td>\n",
       "      <td>185</td>\n",
       "      <td>-0.999166</td>\n",
       "      <td>-0.040844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-05</th>\n",
       "      <td>188.032279</td>\n",
       "      <td>189.730039</td>\n",
       "      <td>187.363162</td>\n",
       "      <td>189.450409</td>\n",
       "      <td>45280000</td>\n",
       "      <td>183.192059</td>\n",
       "      <td>183.358844</td>\n",
       "      <td>182.305126</td>\n",
       "      <td>58.676372</td>\n",
       "      <td>56.605819</td>\n",
       "      <td>...</td>\n",
       "      <td>14.010000</td>\n",
       "      <td>15.077674</td>\n",
       "      <td>15.160533</td>\n",
       "      <td>15.349252</td>\n",
       "      <td>45.412000</td>\n",
       "      <td>46.845275</td>\n",
       "      <td>47.050436</td>\n",
       "      <td>189</td>\n",
       "      <td>-0.993993</td>\n",
       "      <td>-0.109446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-06</th>\n",
       "      <td>188.152109</td>\n",
       "      <td>188.601529</td>\n",
       "      <td>181.231234</td>\n",
       "      <td>182.669342</td>\n",
       "      <td>81755800</td>\n",
       "      <td>183.142276</td>\n",
       "      <td>183.325209</td>\n",
       "      <td>182.317067</td>\n",
       "      <td>49.154475</td>\n",
       "      <td>51.444569</td>\n",
       "      <td>...</td>\n",
       "      <td>14.450000</td>\n",
       "      <td>15.017895</td>\n",
       "      <td>15.125873</td>\n",
       "      <td>15.319768</td>\n",
       "      <td>47.269503</td>\n",
       "      <td>47.722684</td>\n",
       "      <td>47.598249</td>\n",
       "      <td>190</td>\n",
       "      <td>-0.991963</td>\n",
       "      <td>-0.126528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-07</th>\n",
       "      <td>174.949512</td>\n",
       "      <td>177.975539</td>\n",
       "      <td>173.311670</td>\n",
       "      <td>177.326385</td>\n",
       "      <td>112488800</td>\n",
       "      <td>182.588382</td>\n",
       "      <td>183.032584</td>\n",
       "      <td>182.153438</td>\n",
       "      <td>43.323469</td>\n",
       "      <td>47.914072</td>\n",
       "      <td>...</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>14.959048</td>\n",
       "      <td>15.090465</td>\n",
       "      <td>15.289612</td>\n",
       "      <td>47.077881</td>\n",
       "      <td>47.631048</td>\n",
       "      <td>47.541409</td>\n",
       "      <td>191</td>\n",
       "      <td>-0.989640</td>\n",
       "      <td>-0.143572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-08</th>\n",
       "      <td>178.115346</td>\n",
       "      <td>180.002859</td>\n",
       "      <td>177.556070</td>\n",
       "      <td>177.945557</td>\n",
       "      <td>65551300</td>\n",
       "      <td>182.146208</td>\n",
       "      <td>182.784436</td>\n",
       "      <td>182.015475</td>\n",
       "      <td>44.131913</td>\n",
       "      <td>48.335490</td>\n",
       "      <td>...</td>\n",
       "      <td>13.840000</td>\n",
       "      <td>14.852472</td>\n",
       "      <td>15.029466</td>\n",
       "      <td>15.242083</td>\n",
       "      <td>44.930554</td>\n",
       "      <td>46.603111</td>\n",
       "      <td>46.903454</td>\n",
       "      <td>192</td>\n",
       "      <td>-0.987024</td>\n",
       "      <td>-0.160575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-11</th>\n",
       "      <td>179.833081</td>\n",
       "      <td>180.062774</td>\n",
       "      <td>177.106662</td>\n",
       "      <td>179.124008</td>\n",
       "      <td>58953100</td>\n",
       "      <td>181.858379</td>\n",
       "      <td>182.605879</td>\n",
       "      <td>181.920673</td>\n",
       "      <td>45.684123</td>\n",
       "      <td>49.138786</td>\n",
       "      <td>...</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>14.752237</td>\n",
       "      <td>14.969492</td>\n",
       "      <td>15.194802</td>\n",
       "      <td>44.776992</td>\n",
       "      <td>46.529545</td>\n",
       "      <td>46.857780</td>\n",
       "      <td>195</td>\n",
       "      <td>-0.977426</td>\n",
       "      <td>-0.211276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-12</th>\n",
       "      <td>179.253844</td>\n",
       "      <td>179.893002</td>\n",
       "      <td>174.589990</td>\n",
       "      <td>176.068039</td>\n",
       "      <td>90370200</td>\n",
       "      <td>181.306918</td>\n",
       "      <td>182.286960</td>\n",
       "      <td>181.728783</td>\n",
       "      <td>42.463663</td>\n",
       "      <td>47.187405</td>\n",
       "      <td>...</td>\n",
       "      <td>14.230000</td>\n",
       "      <td>14.702500</td>\n",
       "      <td>14.933420</td>\n",
       "      <td>15.163169</td>\n",
       "      <td>46.833204</td>\n",
       "      <td>47.444252</td>\n",
       "      <td>47.417556</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.973648</td>\n",
       "      <td>-0.228058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-13</th>\n",
       "      <td>176.277751</td>\n",
       "      <td>177.066720</td>\n",
       "      <td>173.751081</td>\n",
       "      <td>173.980789</td>\n",
       "      <td>84267900</td>\n",
       "      <td>180.609192</td>\n",
       "      <td>181.881781</td>\n",
       "      <td>181.474751</td>\n",
       "      <td>40.415330</td>\n",
       "      <td>45.910236</td>\n",
       "      <td>...</td>\n",
       "      <td>13.480000</td>\n",
       "      <td>14.586071</td>\n",
       "      <td>14.862521</td>\n",
       "      <td>15.107983</td>\n",
       "      <td>43.836437</td>\n",
       "      <td>46.035453</td>\n",
       "      <td>46.547861</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.969581</td>\n",
       "      <td>-0.244772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-14</th>\n",
       "      <td>173.771073</td>\n",
       "      <td>175.868316</td>\n",
       "      <td>173.351627</td>\n",
       "      <td>175.508789</td>\n",
       "      <td>60895800</td>\n",
       "      <td>180.123439</td>\n",
       "      <td>181.570903</td>\n",
       "      <td>181.279145</td>\n",
       "      <td>42.550795</td>\n",
       "      <td>46.987555</td>\n",
       "      <td>...</td>\n",
       "      <td>12.820000</td>\n",
       "      <td>14.417874</td>\n",
       "      <td>14.762886</td>\n",
       "      <td>15.032967</td>\n",
       "      <td>41.383506</td>\n",
       "      <td>44.833880</td>\n",
       "      <td>45.796171</td>\n",
       "      <td>198</td>\n",
       "      <td>-0.965227</td>\n",
       "      <td>-0.261414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-15</th>\n",
       "      <td>176.247791</td>\n",
       "      <td>176.267769</td>\n",
       "      <td>173.591303</td>\n",
       "      <td>174.779724</td>\n",
       "      <td>109205100</td>\n",
       "      <td>179.614514</td>\n",
       "      <td>181.239626</td>\n",
       "      <td>181.066049</td>\n",
       "      <td>41.798415</td>\n",
       "      <td>46.533992</td>\n",
       "      <td>...</td>\n",
       "      <td>13.790000</td>\n",
       "      <td>14.358076</td>\n",
       "      <td>14.715428</td>\n",
       "      <td>14.992214</td>\n",
       "      <td>46.053514</td>\n",
       "      <td>46.922185</td>\n",
       "      <td>47.073605</td>\n",
       "      <td>199</td>\n",
       "      <td>-0.960587</td>\n",
       "      <td>-0.277979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-18</th>\n",
       "      <td>176.247795</td>\n",
       "      <td>179.143988</td>\n",
       "      <td>175.938205</td>\n",
       "      <td>177.735840</td>\n",
       "      <td>67257600</td>\n",
       "      <td>179.435593</td>\n",
       "      <td>181.068710</td>\n",
       "      <td>180.956862</td>\n",
       "      <td>45.882534</td>\n",
       "      <td>48.597425</td>\n",
       "      <td>...</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.323974</td>\n",
       "      <td>14.680529</td>\n",
       "      <td>14.959683</td>\n",
       "      <td>47.015504</td>\n",
       "      <td>47.364613</td>\n",
       "      <td>47.346805</td>\n",
       "      <td>202</td>\n",
       "      <td>-0.944969</td>\n",
       "      <td>-0.327160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-19</th>\n",
       "      <td>177.286433</td>\n",
       "      <td>179.393657</td>\n",
       "      <td>176.896946</td>\n",
       "      <td>178.834396</td>\n",
       "      <td>51826900</td>\n",
       "      <td>179.378336</td>\n",
       "      <td>180.959719</td>\n",
       "      <td>180.887273</td>\n",
       "      <td>47.328366</td>\n",
       "      <td>49.342589</td>\n",
       "      <td>...</td>\n",
       "      <td>14.110000</td>\n",
       "      <td>14.303595</td>\n",
       "      <td>14.652698</td>\n",
       "      <td>14.931824</td>\n",
       "      <td>47.531392</td>\n",
       "      <td>47.599270</td>\n",
       "      <td>47.491186</td>\n",
       "      <td>203</td>\n",
       "      <td>-0.939201</td>\n",
       "      <td>-0.343367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-20</th>\n",
       "      <td>179.024138</td>\n",
       "      <td>179.463562</td>\n",
       "      <td>175.169216</td>\n",
       "      <td>175.259109</td>\n",
       "      <td>58436200</td>\n",
       "      <td>178.986029</td>\n",
       "      <td>180.681640</td>\n",
       "      <td>180.702743</td>\n",
       "      <td>43.359810</td>\n",
       "      <td>47.065132</td>\n",
       "      <td>...</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>14.383253</td>\n",
       "      <td>14.676469</td>\n",
       "      <td>14.938650</td>\n",
       "      <td>52.125809</td>\n",
       "      <td>49.750701</td>\n",
       "      <td>48.827380</td>\n",
       "      <td>204</td>\n",
       "      <td>-0.933156</td>\n",
       "      <td>-0.359472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-21</th>\n",
       "      <td>174.320351</td>\n",
       "      <td>176.068049</td>\n",
       "      <td>173.631257</td>\n",
       "      <td>173.701157</td>\n",
       "      <td>63047900</td>\n",
       "      <td>178.482707</td>\n",
       "      <td>180.341129</td>\n",
       "      <td>180.473183</td>\n",
       "      <td>41.753876</td>\n",
       "      <td>46.113873</td>\n",
       "      <td>...</td>\n",
       "      <td>17.540001</td>\n",
       "      <td>14.683896</td>\n",
       "      <td>14.816154</td>\n",
       "      <td>15.023940</td>\n",
       "      <td>60.590034</td>\n",
       "      <td>54.240630</td>\n",
       "      <td>51.737559</td>\n",
       "      <td>205</td>\n",
       "      <td>-0.926834</td>\n",
       "      <td>-0.375470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-22</th>\n",
       "      <td>174.440176</td>\n",
       "      <td>176.847008</td>\n",
       "      <td>173.820996</td>\n",
       "      <td>174.560013</td>\n",
       "      <td>56725400</td>\n",
       "      <td>178.109117</td>\n",
       "      <td>180.059123</td>\n",
       "      <td>180.279309</td>\n",
       "      <td>42.979380</td>\n",
       "      <td>46.722713</td>\n",
       "      <td>...</td>\n",
       "      <td>17.200001</td>\n",
       "      <td>14.923525</td>\n",
       "      <td>14.932439</td>\n",
       "      <td>15.095286</td>\n",
       "      <td>59.033606</td>\n",
       "      <td>53.545453</td>\n",
       "      <td>51.317112</td>\n",
       "      <td>206</td>\n",
       "      <td>-0.920239</td>\n",
       "      <td>-0.391358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-25</th>\n",
       "      <td>173.970796</td>\n",
       "      <td>176.737156</td>\n",
       "      <td>173.920859</td>\n",
       "      <td>175.848328</td>\n",
       "      <td>46172700</td>\n",
       "      <td>177.893804</td>\n",
       "      <td>179.853719</td>\n",
       "      <td>180.134031</td>\n",
       "      <td>44.812808</td>\n",
       "      <td>47.633005</td>\n",
       "      <td>...</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>15.111760</td>\n",
       "      <td>15.028417</td>\n",
       "      <td>15.154457</td>\n",
       "      <td>57.657957</td>\n",
       "      <td>52.931516</td>\n",
       "      <td>50.945615</td>\n",
       "      <td>209</td>\n",
       "      <td>-0.898825</td>\n",
       "      <td>-0.438307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-26</th>\n",
       "      <td>174.589987</td>\n",
       "      <td>174.969477</td>\n",
       "      <td>171.434141</td>\n",
       "      <td>171.733749</td>\n",
       "      <td>64588900</td>\n",
       "      <td>177.307132</td>\n",
       "      <td>179.457622</td>\n",
       "      <td>179.858612</td>\n",
       "      <td>40.441250</td>\n",
       "      <td>45.108393</td>\n",
       "      <td>...</td>\n",
       "      <td>18.940001</td>\n",
       "      <td>15.476355</td>\n",
       "      <td>15.219226</td>\n",
       "      <td>15.278573</td>\n",
       "      <td>63.710919</td>\n",
       "      <td>56.416690</td>\n",
       "      <td>53.284252</td>\n",
       "      <td>210</td>\n",
       "      <td>-0.891153</td>\n",
       "      <td>-0.453703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-27</th>\n",
       "      <td>172.392870</td>\n",
       "      <td>172.812316</td>\n",
       "      <td>168.827576</td>\n",
       "      <td>170.205750</td>\n",
       "      <td>66921800</td>\n",
       "      <td>176.630810</td>\n",
       "      <td>179.006312</td>\n",
       "      <td>179.542124</td>\n",
       "      <td>38.955725</td>\n",
       "      <td>44.215792</td>\n",
       "      <td>...</td>\n",
       "      <td>18.219999</td>\n",
       "      <td>15.737654</td>\n",
       "      <td>15.365606</td>\n",
       "      <td>15.375013</td>\n",
       "      <td>60.497883</td>\n",
       "      <td>54.943995</td>\n",
       "      <td>52.387820</td>\n",
       "      <td>211</td>\n",
       "      <td>-0.883217</td>\n",
       "      <td>-0.468965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-28</th>\n",
       "      <td>169.117194</td>\n",
       "      <td>171.803657</td>\n",
       "      <td>167.399456</td>\n",
       "      <td>170.465424</td>\n",
       "      <td>56294400</td>\n",
       "      <td>176.043630</td>\n",
       "      <td>178.589683</td>\n",
       "      <td>179.244527</td>\n",
       "      <td>39.354233</td>\n",
       "      <td>44.407534</td>\n",
       "      <td>...</td>\n",
       "      <td>17.340000</td>\n",
       "      <td>15.890259</td>\n",
       "      <td>15.461917</td>\n",
       "      <td>15.439439</td>\n",
       "      <td>56.811796</td>\n",
       "      <td>53.203046</td>\n",
       "      <td>51.314796</td>\n",
       "      <td>212</td>\n",
       "      <td>-0.875019</td>\n",
       "      <td>-0.484089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-29</th>\n",
       "      <td>171.793673</td>\n",
       "      <td>172.842295</td>\n",
       "      <td>170.115876</td>\n",
       "      <td>170.984741</td>\n",
       "      <td>51814200</td>\n",
       "      <td>175.561831</td>\n",
       "      <td>178.218710</td>\n",
       "      <td>178.973715</td>\n",
       "      <td>40.176374</td>\n",
       "      <td>44.796731</td>\n",
       "      <td>...</td>\n",
       "      <td>17.520000</td>\n",
       "      <td>16.045472</td>\n",
       "      <td>15.562312</td>\n",
       "      <td>15.507654</td>\n",
       "      <td>57.371035</td>\n",
       "      <td>53.512070</td>\n",
       "      <td>51.521343</td>\n",
       "      <td>213</td>\n",
       "      <td>-0.866562</td>\n",
       "      <td>-0.499069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-02</th>\n",
       "      <td>170.994723</td>\n",
       "      <td>174.070672</td>\n",
       "      <td>170.705096</td>\n",
       "      <td>173.521393</td>\n",
       "      <td>52164500</td>\n",
       "      <td>175.367504</td>\n",
       "      <td>177.989573</td>\n",
       "      <td>178.794950</td>\n",
       "      <td>44.074532</td>\n",
       "      <td>46.667288</td>\n",
       "      <td>...</td>\n",
       "      <td>17.610001</td>\n",
       "      <td>16.194475</td>\n",
       "      <td>15.662199</td>\n",
       "      <td>15.576584</td>\n",
       "      <td>57.659593</td>\n",
       "      <td>53.668966</td>\n",
       "      <td>51.625697</td>\n",
       "      <td>216</td>\n",
       "      <td>-0.839665</td>\n",
       "      <td>-0.543105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-03</th>\n",
       "      <td>172.033357</td>\n",
       "      <td>173.401565</td>\n",
       "      <td>170.595264</td>\n",
       "      <td>172.173172</td>\n",
       "      <td>49594600</td>\n",
       "      <td>175.063282</td>\n",
       "      <td>177.705846</td>\n",
       "      <td>178.577843</td>\n",
       "      <td>42.524287</td>\n",
       "      <td>45.820908</td>\n",
       "      <td>...</td>\n",
       "      <td>19.780001</td>\n",
       "      <td>16.535953</td>\n",
       "      <td>15.863067</td>\n",
       "      <td>15.714401</td>\n",
       "      <td>63.867193</td>\n",
       "      <td>57.237933</td>\n",
       "      <td>54.050905</td>\n",
       "      <td>217</td>\n",
       "      <td>-0.830198</td>\n",
       "      <td>-0.557468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-04</th>\n",
       "      <td>170.864893</td>\n",
       "      <td>173.980798</td>\n",
       "      <td>170.745055</td>\n",
       "      <td>173.431519</td>\n",
       "      <td>53020300</td>\n",
       "      <td>174.907876</td>\n",
       "      <td>177.497342</td>\n",
       "      <td>178.409111</td>\n",
       "      <td>44.444097</td>\n",
       "      <td>46.745486</td>\n",
       "      <td>...</td>\n",
       "      <td>18.580000</td>\n",
       "      <td>16.730625</td>\n",
       "      <td>15.995601</td>\n",
       "      <td>15.808355</td>\n",
       "      <td>58.845181</td>\n",
       "      <td>54.841861</td>\n",
       "      <td>52.568785</td>\n",
       "      <td>218</td>\n",
       "      <td>-0.820486</td>\n",
       "      <td>-0.571667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open_x     High_x      Low_x    Close_x     Volume  \\\n",
       "Date                                                                \n",
       "2023-07-12 189.174968 191.189594 187.968198 189.264740   60750200   \n",
       "2023-07-13 189.992798 190.680963 189.274713 190.032684   41342300   \n",
       "2023-07-14 189.723515 190.670982 189.125121 190.182297   41573900   \n",
       "2023-07-17 191.389049 193.802619 191.299292 193.473495   50520200   \n",
       "2023-07-18 192.835210 193.812596 191.907678 193.214188   48353800   \n",
       "2023-07-19 192.585876 197.702207 192.137062 194.580551   80507300   \n",
       "2023-07-20 194.570572 195.946903 191.987471 192.615799   59581200   \n",
       "2023-07-21 193.583223 194.450902 190.720854 191.428970   71917800   \n",
       "2023-07-24 192.895048 194.391054 191.738132 192.236801   45377800   \n",
       "2023-07-25 192.815256 193.922301 192.406344 193.104477   37283200   \n",
       "2023-07-26 193.154355 195.119111 192.805296 193.982147   47471900   \n",
       "2023-07-27 195.498099 196.674950 192.037337 192.705551   47460200   \n",
       "2023-07-28 194.151694 196.106482 193.623106 195.308609   48291400   \n",
       "2023-07-31 195.537995 195.966858 194.740122 195.926956   38824100   \n",
       "2023-08-01 195.717515 196.206201 194.760064 195.089188   35175100   \n",
       "2023-08-02 194.520703 194.660329 191.339209 192.067261   50389300   \n",
       "2023-08-03 191.059955 191.857813 190.182293 190.661011   61235200   \n",
       "2023-08-04 185.026063 186.881111 181.435642 181.505463  115799700   \n",
       "2023-08-07 181.645075 182.642413 176.877804 178.373810   97576100   \n",
       "2023-08-08 179.211566 179.790024 177.107183 179.321274   67823000   \n",
       "2023-08-09 180.388433 180.448271 176.538710 177.715576   60378500   \n",
       "2023-08-10 179.002129 180.268752 177.127145 177.496155   54686900   \n",
       "2023-08-11 177.086708 178.384986 176.317717 177.556076   51988100   \n",
       "2023-08-14 177.735847 179.453585 177.076712 179.223892   43675600   \n",
       "2023-08-15 178.644649 179.243850 176.817055 177.216522   43622600   \n",
       "2023-08-16 176.896953 178.305086 176.267777 176.337692   46964900   \n",
       "2023-08-17 176.906941 177.276449 173.251752 173.771072   66062900   \n",
       "2023-08-18 172.073301 174.869620 171.733752 174.260422   61114200   \n",
       "2023-08-21 174.839667 175.898270 173.511415 175.608643   46311900   \n",
       "2023-08-22 176.827036 177.446216 176.018105 176.996811   42084200   \n",
       "2023-08-23 178.285129 181.311141 178.095376 180.881699   52722800   \n",
       "2023-08-24 180.432283 180.861725 175.778410 176.147934   54945800   \n",
       "2023-08-25 177.146623 178.914283 175.588678 178.375000   51449600   \n",
       "2023-08-28 179.853052 180.352394 178.315085 179.952927   43820700   \n",
       "2023-08-29 179.463564 184.656719 179.263830 183.877747   53003900   \n",
       "2023-08-30 184.696681 187.602856 184.496947 187.403107   60813900   \n",
       "2023-08-31 187.592859 188.871173 187.233332 187.622818   60794500   \n",
       "2023-09-01 189.240698 189.670125 188.032284 189.210739   45732600   \n",
       "2023-09-05 188.032279 189.730039 187.363162 189.450409   45280000   \n",
       "2023-09-06 188.152109 188.601529 181.231234 182.669342   81755800   \n",
       "2023-09-07 174.949512 177.975539 173.311670 177.326385  112488800   \n",
       "2023-09-08 178.115346 180.002859 177.556070 177.945557   65551300   \n",
       "2023-09-11 179.833081 180.062774 177.106662 179.124008   58953100   \n",
       "2023-09-12 179.253844 179.893002 174.589990 176.068039   90370200   \n",
       "2023-09-13 176.277751 177.066720 173.751081 173.980789   84267900   \n",
       "2023-09-14 173.771073 175.868316 173.351627 175.508789   60895800   \n",
       "2023-09-15 176.247791 176.267769 173.591303 174.779724  109205100   \n",
       "2023-09-18 176.247795 179.143988 175.938205 177.735840   67257600   \n",
       "2023-09-19 177.286433 179.393657 176.896946 178.834396   51826900   \n",
       "2023-09-20 179.024138 179.463562 175.169216 175.259109   58436200   \n",
       "2023-09-21 174.320351 176.068049 173.631257 173.701157   63047900   \n",
       "2023-09-22 174.440176 176.847008 173.820996 174.560013   56725400   \n",
       "2023-09-25 173.970796 176.737156 173.920859 175.848328   46172700   \n",
       "2023-09-26 174.589987 174.969477 171.434141 171.733749   64588900   \n",
       "2023-09-27 172.392870 172.812316 168.827576 170.205750   66921800   \n",
       "2023-09-28 169.117194 171.803657 167.399456 170.465424   56294400   \n",
       "2023-09-29 171.793673 172.842295 170.115876 170.984741   51814200   \n",
       "2023-10-02 170.994723 174.070672 170.705096 173.521393   52164500   \n",
       "2023-10-03 172.033357 173.401565 170.595264 172.173172   49594600   \n",
       "2023-10-04 170.864893 173.980798 170.745055 173.431519   53020300   \n",
       "\n",
       "            EMA20_Close_x  EMA40_Close_x  EMA60_Close_x  RSI20_Close_x  \\\n",
       "Date                                                                     \n",
       "2023-07-12     186.967469     182.281242     177.955451      62.503099   \n",
       "2023-07-13     187.259395     182.659361     178.351425      63.539759   \n",
       "2023-07-14     187.537766     183.026333     178.739323      63.745310   \n",
       "2023-07-17     188.103074     183.535951     179.222411      67.931691   \n",
       "2023-07-18     188.589847     184.008060     179.681157      67.287306   \n",
       "2023-07-19     189.160390     184.523791     180.169662      68.922418   \n",
       "2023-07-20     189.489476     184.918523     180.577732      64.074717   \n",
       "2023-07-21     189.674190     185.236106     180.933510      61.331770   \n",
       "2023-07-24     189.918248     185.577604     181.304110      62.482500   \n",
       "2023-07-25     190.221699     185.944768     181.691007      63.703722   \n",
       "2023-07-26     190.579837     186.336835     182.093996      64.919561   \n",
       "2023-07-27     190.782286     186.647504     182.441915      61.752426   \n",
       "2023-07-28     191.213364     187.069997     182.863774      65.377797   \n",
       "2023-07-31     191.662278     187.502044     183.292075      66.179392   \n",
       "2023-08-01     191.988650     187.872149     183.678866      64.064040   \n",
       "2023-08-02     191.996137     188.076788     183.953895      57.130371   \n",
       "2023-08-03     191.868982     188.202848     184.173801      54.254059   \n",
       "2023-08-04     190.881980     187.876146     184.086314      40.336444   \n",
       "2023-08-07     189.690726     187.412618     183.899019      36.925860   \n",
       "2023-08-08     188.703159     187.017918     183.748929      38.579756   \n",
       "2023-08-09     187.656722     186.564145     183.551114      36.855747   \n",
       "2023-08-10     186.689049     186.121804     183.352591      36.620353   \n",
       "2023-08-11     185.819242     185.703964     183.162541      36.736504   \n",
       "2023-08-14     185.191114     185.387863     183.033405      39.960197   \n",
       "2023-08-15     184.431629     184.989261     182.842688      37.536858   \n",
       "2023-08-16     183.660778     184.567233     182.629409      36.516325   \n",
       "2023-08-17     182.718901     184.040591     182.338972      33.699703   \n",
       "2023-08-18     181.913332     183.563510     182.074101      34.710396   \n",
       "2023-08-21     181.312885     183.175467     181.862119      37.474653   \n",
       "2023-08-22     180.901830     182.874069     181.702601      40.217889   \n",
       "2023-08-23     180.899913     182.776881     181.675686      47.060189   \n",
       "2023-08-24     180.447344     182.453517     181.494448      41.035977   \n",
       "2023-08-25     180.249978     182.254565     181.392171      44.551114   \n",
       "2023-08-28     180.221687     182.142290     181.344983      46.911514   \n",
       "2023-08-29     180.569883     182.226947     181.428024      52.235174   \n",
       "2023-08-30     181.220666     182.479442     181.623929      56.371722   \n",
       "2023-08-31     181.830395     182.730339     181.820614      56.618192   \n",
       "2023-09-01     182.533285     183.046456     182.062913      58.405832   \n",
       "2023-09-05     183.192059     183.358844     182.305126      58.676372   \n",
       "2023-09-06     183.142276     183.325209     182.317067      49.154475   \n",
       "2023-09-07     182.588382     183.032584     182.153438      43.323469   \n",
       "2023-09-08     182.146208     182.784436     182.015475      44.131913   \n",
       "2023-09-11     181.858379     182.605879     181.920673      45.684123   \n",
       "2023-09-12     181.306918     182.286960     181.728783      42.463663   \n",
       "2023-09-13     180.609192     181.881781     181.474751      40.415330   \n",
       "2023-09-14     180.123439     181.570903     181.279145      42.550795   \n",
       "2023-09-15     179.614514     181.239626     181.066049      41.798415   \n",
       "2023-09-18     179.435593     181.068710     180.956862      45.882534   \n",
       "2023-09-19     179.378336     180.959719     180.887273      47.328366   \n",
       "2023-09-20     178.986029     180.681640     180.702743      43.359810   \n",
       "2023-09-21     178.482707     180.341129     180.473183      41.753876   \n",
       "2023-09-22     178.109117     180.059123     180.279309      42.979380   \n",
       "2023-09-25     177.893804     179.853719     180.134031      44.812808   \n",
       "2023-09-26     177.307132     179.457622     179.858612      40.441250   \n",
       "2023-09-27     176.630810     179.006312     179.542124      38.955725   \n",
       "2023-09-28     176.043630     178.589683     179.244527      39.354233   \n",
       "2023-09-29     175.561831     178.218710     178.973715      40.176374   \n",
       "2023-10-02     175.367504     177.989573     178.794950      44.074532   \n",
       "2023-10-03     175.063282     177.705846     178.577843      42.524287   \n",
       "2023-10-04     174.907876     177.497342     178.409111      44.444097   \n",
       "\n",
       "            RSI40_Close_x  ...   Close_y  EMA20_Close_y  EMA40_Close_y  \\\n",
       "Date                       ...                                           \n",
       "2023-07-12      62.598253  ... 13.540000      14.388288      15.115892   \n",
       "2023-07-13      63.079535  ... 13.610000      14.314166      15.042434   \n",
       "2023-07-14      63.174223  ... 13.340000      14.221388      14.959388   \n",
       "2023-07-17      65.188552  ... 13.480000      14.150779      14.887223   \n",
       "2023-07-18      64.901678  ... 13.300000      14.069753      14.809797   \n",
       "2023-07-19      65.717028  ... 13.760000      14.040253      14.758588   \n",
       "2023-07-20      63.540106  ... 13.990000      14.035467      14.721096   \n",
       "2023-07-21      62.262298  ... 13.600000      13.993994      14.666408   \n",
       "2023-07-24      62.784774  ... 13.910000      13.985994      14.629510   \n",
       "2023-07-25      63.343849  ... 13.860000      13.973995      14.591973   \n",
       "2023-07-26      63.906383  ... 13.190000      13.899329      14.523584   \n",
       "2023-07-27      62.476063  ... 14.410000      13.947964      14.518043   \n",
       "2023-07-28      64.153926  ... 13.330000      13.889110      14.460090   \n",
       "2023-07-31      64.540228  ... 13.630000      13.864433      14.419598   \n",
       "2023-08-01      63.587984  ... 13.930000      13.870678      14.395715   \n",
       "2023-08-02      60.296690  ... 16.090000      14.082042      14.478363   \n",
       "2023-08-03      58.843037  ... 15.920000      14.257085      14.548687   \n",
       "2023-08-04      50.683734  ... 17.100000      14.527839      14.673141   \n",
       "2023-08-07      48.332572  ... 15.770000      14.646140      14.726647   \n",
       "2023-08-08      49.065748  ... 15.990000      14.774127      14.788274   \n",
       "2023-08-09      47.884655  ... 15.960000      14.887067      14.845431   \n",
       "2023-08-10      47.723646  ... 15.850000      14.978775      14.894434   \n",
       "2023-08-11      47.772832  ... 14.840000      14.965558      14.891779   \n",
       "2023-08-14      49.138956  ... 14.820000      14.951696      14.888278   \n",
       "2023-08-15      47.601890  ... 16.459999      15.095344      14.964947   \n",
       "2023-08-16      46.942555  ... 16.780001      15.255787      15.053486   \n",
       "2023-08-17      45.072537  ... 17.889999      15.506664      15.191853   \n",
       "2023-08-18      45.497109  ... 17.299999      15.677458      15.294689   \n",
       "2023-08-21      46.662134  ... 17.129999      15.815796      15.384216   \n",
       "2023-08-22      47.839559  ... 16.969999      15.925720      15.461572   \n",
       "2023-08-23      50.947620  ... 15.980000      15.930889      15.486861   \n",
       "2023-08-24      47.416584  ... 17.200001      16.051757      15.570429   \n",
       "2023-08-25      49.118204  ... 15.680000      16.016352      15.575774   \n",
       "2023-08-28      50.287247  ... 15.080000      15.927175      15.551589   \n",
       "2023-08-29      53.039744  ... 14.450000      15.786492      15.497853   \n",
       "2023-08-30      55.318840  ... 13.880000      15.604921      15.418934   \n",
       "2023-08-31      55.457024  ... 13.570000      15.411119      15.328742   \n",
       "2023-09-01      56.455273  ... 13.090000      15.190060      15.219535   \n",
       "2023-09-05      56.605819  ... 14.010000      15.077674      15.160533   \n",
       "2023-09-06      51.444569  ... 14.450000      15.017895      15.125873   \n",
       "2023-09-07      47.914072  ... 14.400000      14.959048      15.090465   \n",
       "2023-09-08      48.335490  ... 13.840000      14.852472      15.029466   \n",
       "2023-09-11      49.138786  ... 13.800000      14.752237      14.969492   \n",
       "2023-09-12      47.187405  ... 14.230000      14.702500      14.933420   \n",
       "2023-09-13      45.910236  ... 13.480000      14.586071      14.862521   \n",
       "2023-09-14      46.987555  ... 12.820000      14.417874      14.762886   \n",
       "2023-09-15      46.533992  ... 13.790000      14.358076      14.715428   \n",
       "2023-09-18      48.597425  ... 14.000000      14.323974      14.680529   \n",
       "2023-09-19      49.342589  ... 14.110000      14.303595      14.652698   \n",
       "2023-09-20      47.065132  ... 15.140000      14.383253      14.676469   \n",
       "2023-09-21      46.113873  ... 17.540001      14.683896      14.816154   \n",
       "2023-09-22      46.722713  ... 17.200001      14.923525      14.932439   \n",
       "2023-09-25      47.633005  ... 16.900000      15.111760      15.028417   \n",
       "2023-09-26      45.108393  ... 18.940001      15.476355      15.219226   \n",
       "2023-09-27      44.215792  ... 18.219999      15.737654      15.365606   \n",
       "2023-09-28      44.407534  ... 17.340000      15.890259      15.461917   \n",
       "2023-09-29      44.796731  ... 17.520000      16.045472      15.562312   \n",
       "2023-10-02      46.667288  ... 17.610001      16.194475      15.662199   \n",
       "2023-10-03      45.820908  ... 19.780001      16.535953      15.863067   \n",
       "2023-10-04      46.745486  ... 18.580000      16.730625      15.995601   \n",
       "\n",
       "            EMA60_Close_y  RSI20_Close_y  RSI40_Close_y  RSI60_Close_y  \\\n",
       "Date                                                                     \n",
       "2023-07-12      15.890125      43.573185      44.658958      45.457967   \n",
       "2023-07-13      15.815366      43.901797      44.795659      45.539996   \n",
       "2023-07-14      15.734207      42.887713      44.362148      45.272916   \n",
       "2023-07-17      15.660298      43.598794      44.647029      45.441638   \n",
       "2023-07-18      15.582912      42.876310      44.347594      45.259198   \n",
       "2023-07-19      15.523144      45.314081      45.309005      45.824467   \n",
       "2023-07-20      15.472877      46.515380      45.789263      46.107438   \n",
       "2023-07-21      15.411471      44.760358      45.100495      45.695861   \n",
       "2023-07-24      15.362243      46.450845      45.765579      46.084894   \n",
       "2023-07-25      15.312989      46.210746      45.674045      46.030805   \n",
       "2023-07-26      15.243383      43.070545      44.452144      45.306179   \n",
       "2023-07-27      15.216059      49.631071      47.095406      46.855384   \n",
       "2023-07-28      15.154221      44.818280      45.144928      45.690299   \n",
       "2023-07-31      15.104246      46.339773      45.784631      46.069119   \n",
       "2023-08-01      15.065746      47.853259      46.425421      46.448977   \n",
       "2023-08-02      15.099328      57.037165      50.726105      49.075280   \n",
       "2023-08-03      15.126236      56.216928      50.399520      48.883399   \n",
       "2023-08-04      15.190949      60.379924      52.573301      50.256304   \n",
       "2023-08-07      15.209934      54.258995      50.038194      48.755342   \n",
       "2023-08-08      15.235510      55.052368      50.443605      49.011508   \n",
       "2023-08-09      15.259264      54.915646      50.386422      48.977556   \n",
       "2023-08-10      15.278633      54.394250      50.172532      48.851368   \n",
       "2023-08-11      15.264251      49.822294      48.243912      47.703742   \n",
       "2023-08-14      15.249686      49.735156      48.206277      47.681185   \n",
       "2023-08-15      15.289368      56.327922      51.395129      49.665914   \n",
       "2023-08-16      15.338241      57.473559      51.986721      50.041973   \n",
       "2023-08-17      15.421905      61.190832      53.979502      51.324810   \n",
       "2023-08-18      15.483482      58.337707      52.785143      50.622165   \n",
       "2023-08-21      15.537466      57.524205      52.442233      50.419907   \n",
       "2023-08-22      15.584435      56.740329      52.115420      50.227828   \n",
       "2023-08-23      15.597404      52.114910      50.132746      49.051985   \n",
       "2023-08-24      15.649948      56.694283      52.420581      50.504092   \n",
       "2023-08-25      15.650933      50.376127      49.517578      48.743844   \n",
       "2023-08-28      15.632214      48.146658      48.431706      48.071270   \n",
       "2023-08-29      15.593453      45.901391      47.314337      47.373292   \n",
       "2023-08-30      15.537274      43.949455      46.322617      46.748753   \n",
       "2023-08-31      15.472774      42.904948      45.787283      46.410355   \n",
       "2023-09-01      15.394650      41.304966      44.962097      45.887323   \n",
       "2023-09-05      15.349252      45.412000      46.845275      47.050436   \n",
       "2023-09-06      15.319768      47.269503      47.722684      47.598249   \n",
       "2023-09-07      15.289612      47.077881      47.631048      47.541409   \n",
       "2023-09-08      15.242083      44.930554      46.603111      46.903454   \n",
       "2023-09-11      15.194802      44.776992      46.529545      46.857780   \n",
       "2023-09-12      15.163169      46.833204      47.444252      47.417556   \n",
       "2023-09-13      15.107983      43.836437      46.035453      46.547861   \n",
       "2023-09-14      15.032967      41.383506      44.833880      45.796171   \n",
       "2023-09-15      14.992214      46.053514      46.922185      47.073605   \n",
       "2023-09-18      14.959683      47.015504      47.364613      47.346805   \n",
       "2023-09-19      14.931824      47.531392      47.599270      47.491186   \n",
       "2023-09-20      14.938650      52.125809      49.750701      48.827380   \n",
       "2023-09-21      15.023940      60.590034      54.240630      51.737559   \n",
       "2023-09-22      15.095286      59.033606      53.545453      51.317112   \n",
       "2023-09-25      15.154457      57.657957      52.931516      50.945615   \n",
       "2023-09-26      15.278573      63.710919      56.416690      53.284252   \n",
       "2023-09-27      15.375013      60.497883      54.943995      52.387820   \n",
       "2023-09-28      15.439439      56.811796      53.203046      51.314796   \n",
       "2023-09-29      15.507654      57.371035      53.512070      51.521343   \n",
       "2023-10-02      15.576584      57.659593      53.668966      51.625697   \n",
       "2023-10-03      15.714401      63.867193      57.237933      54.050905   \n",
       "2023-10-04      15.808355      58.845181      54.841861      52.568785   \n",
       "\n",
       "            shifted_day_of_year  cos_shifted_annual  sin_shifted_annual  \n",
       "Date                                                                     \n",
       "2023-07-12                  134           -0.670089            0.742281  \n",
       "2023-07-13                  135           -0.682758            0.730644  \n",
       "2023-07-14                  136           -0.695225            0.718792  \n",
       "2023-07-17                  139           -0.731378            0.681972  \n",
       "2023-07-18                  140           -0.743001            0.669290  \n",
       "2023-07-19                  141           -0.754404            0.656411  \n",
       "2023-07-20                  142           -0.765584            0.643337  \n",
       "2023-07-21                  143           -0.776537            0.630072  \n",
       "2023-07-24                  146           -0.808005            0.589176  \n",
       "2023-07-25                  147           -0.818020            0.575190  \n",
       "2023-07-26                  148           -0.827793            0.561034  \n",
       "2023-07-27                  149           -0.837321            0.546711  \n",
       "2023-07-28                  150           -0.846602            0.532227  \n",
       "2023-07-31                  153           -0.872929            0.487847  \n",
       "2023-08-01                  154           -0.881192            0.472759  \n",
       "2023-08-02                  155           -0.889193            0.457531  \n",
       "2023-08-03                  156           -0.896932            0.442168  \n",
       "2023-08-04                  157           -0.904405            0.426674  \n",
       "2023-08-07                  160           -0.925211            0.379453  \n",
       "2023-08-08                  161           -0.931601            0.363482  \n",
       "2023-08-09                  162           -0.937716            0.347403  \n",
       "2023-08-10                  163           -0.943553            0.331221  \n",
       "2023-08-11                  164           -0.949111            0.314942  \n",
       "2023-08-14                  167           -0.964094            0.265563  \n",
       "2023-08-15                  168           -0.968519            0.248940  \n",
       "2023-08-16                  169           -0.972658            0.232243  \n",
       "2023-08-17                  170           -0.976509            0.215477  \n",
       "2023-08-18                  171           -0.980071            0.198648  \n",
       "2023-08-21                  174           -0.989013            0.147827  \n",
       "2023-08-22                  175           -0.991410            0.130793  \n",
       "2023-08-23                  176           -0.993513            0.113720  \n",
       "2023-08-24                  177           -0.995322            0.096613  \n",
       "2023-08-25                  178           -0.996837            0.079477  \n",
       "2023-08-28                  181           -0.999609            0.027950  \n",
       "2023-08-29                  182           -0.999942            0.010751  \n",
       "2023-08-30                  183           -0.999979           -0.006451  \n",
       "2023-08-31                  184           -0.999720           -0.023651  \n",
       "2023-09-01                  185           -0.999166           -0.040844  \n",
       "2023-09-05                  189           -0.993993           -0.109446  \n",
       "2023-09-06                  190           -0.991963           -0.126528  \n",
       "2023-09-07                  191           -0.989640           -0.143572  \n",
       "2023-09-08                  192           -0.987024           -0.160575  \n",
       "2023-09-11                  195           -0.977426           -0.211276  \n",
       "2023-09-12                  196           -0.973648           -0.228058  \n",
       "2023-09-13                  197           -0.969581           -0.244772  \n",
       "2023-09-14                  198           -0.965227           -0.261414  \n",
       "2023-09-15                  199           -0.960587           -0.277979  \n",
       "2023-09-18                  202           -0.944969           -0.327160  \n",
       "2023-09-19                  203           -0.939201           -0.343367  \n",
       "2023-09-20                  204           -0.933156           -0.359472  \n",
       "2023-09-21                  205           -0.926834           -0.375470  \n",
       "2023-09-22                  206           -0.920239           -0.391358  \n",
       "2023-09-25                  209           -0.898825           -0.438307  \n",
       "2023-09-26                  210           -0.891153           -0.453703  \n",
       "2023-09-27                  211           -0.883217           -0.468965  \n",
       "2023-09-28                  212           -0.875019           -0.484089  \n",
       "2023-09-29                  213           -0.866562           -0.499069  \n",
       "2023-10-02                  216           -0.839665           -0.543105  \n",
       "2023-10-03                  217           -0.830198           -0.557468  \n",
       "2023-10-04                  218           -0.820486           -0.571667  \n",
       "\n",
       "[60 rows x 24 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a new raw data array to predict\n",
    "new_raw_data = newdf.values\n",
    "\n",
    "# Preprocess the new data (standardize and normalize)\n",
    "preprocessed_data = preprocess_input(new_raw_data, standard_scaler, min_max_scaler)\n",
    "\n",
    "# Prepare the data for prediction\n",
    "prepared_data = prepare_data_for_prediction(preprocessed_data, look_back)\n",
    "\n",
    "# Make a prediction\n",
    "prepared_data_tensor = torch.from_numpy(prepared_data).float().to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prepared_data_tensor = prepared_data_tensor.unsqueeze(0)\n",
    "    prediction = model(prepared_data_tensor)\n",
    "    prediction = prediction.cpu().numpy()\n",
    "\n",
    "# Reshape the predictions to match your output format\n",
    "predictions_reshaped = prediction.reshape(-1, num_features)  # Adjust this as per your model's output\n",
    "\n",
    "# Apply the inverse transformations to revert the predictions to original scale\n",
    "predictions_min_max_inversed = min_max_scaler.inverse_transform(predictions_reshaped)\n",
    "original_scale_predictions = standard_scaler.inverse_transform(predictions_min_max_inversed)\n",
    "\n",
    "# Create a DataFrame for the reverted predictions\n",
    "reverted_df = pd.DataFrame(original_scale_predictions, columns=df.columns[:num_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_x</th>\n",
       "      <th>High_x</th>\n",
       "      <th>Low_x</th>\n",
       "      <th>Close_x</th>\n",
       "      <th>Volume</th>\n",
       "      <th>EMA20_Close_x</th>\n",
       "      <th>EMA40_Close_x</th>\n",
       "      <th>EMA60_Close_x</th>\n",
       "      <th>RSI20_Close_x</th>\n",
       "      <th>RSI40_Close_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Close_y</th>\n",
       "      <th>EMA20_Close_y</th>\n",
       "      <th>EMA40_Close_y</th>\n",
       "      <th>EMA60_Close_y</th>\n",
       "      <th>RSI20_Close_y</th>\n",
       "      <th>RSI40_Close_y</th>\n",
       "      <th>RSI60_Close_y</th>\n",
       "      <th>shifted_day_of_year</th>\n",
       "      <th>cos_shifted_annual</th>\n",
       "      <th>sin_shifted_annual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87.211418</td>\n",
       "      <td>88.378296</td>\n",
       "      <td>86.803787</td>\n",
       "      <td>86.958466</td>\n",
       "      <td>946947520.000000</td>\n",
       "      <td>83.730179</td>\n",
       "      <td>82.526726</td>\n",
       "      <td>81.223221</td>\n",
       "      <td>60.528824</td>\n",
       "      <td>58.296509</td>\n",
       "      <td>...</td>\n",
       "      <td>19.839705</td>\n",
       "      <td>19.367441</td>\n",
       "      <td>19.308647</td>\n",
       "      <td>19.410965</td>\n",
       "      <td>51.167896</td>\n",
       "      <td>51.843208</td>\n",
       "      <td>51.288277</td>\n",
       "      <td>190.546097</td>\n",
       "      <td>-0.004181</td>\n",
       "      <td>-0.050945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.042038</td>\n",
       "      <td>88.199036</td>\n",
       "      <td>86.686440</td>\n",
       "      <td>86.900658</td>\n",
       "      <td>890388480.000000</td>\n",
       "      <td>83.625900</td>\n",
       "      <td>82.463181</td>\n",
       "      <td>81.228577</td>\n",
       "      <td>60.446342</td>\n",
       "      <td>58.435841</td>\n",
       "      <td>...</td>\n",
       "      <td>19.542809</td>\n",
       "      <td>19.322601</td>\n",
       "      <td>19.360321</td>\n",
       "      <td>19.463125</td>\n",
       "      <td>50.764824</td>\n",
       "      <td>51.567406</td>\n",
       "      <td>51.071781</td>\n",
       "      <td>190.944687</td>\n",
       "      <td>-0.001361</td>\n",
       "      <td>-0.050009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86.802452</td>\n",
       "      <td>88.103111</td>\n",
       "      <td>86.402641</td>\n",
       "      <td>86.851761</td>\n",
       "      <td>801209728.000000</td>\n",
       "      <td>83.619713</td>\n",
       "      <td>82.461517</td>\n",
       "      <td>81.294991</td>\n",
       "      <td>60.624031</td>\n",
       "      <td>58.475307</td>\n",
       "      <td>...</td>\n",
       "      <td>19.130501</td>\n",
       "      <td>19.354540</td>\n",
       "      <td>19.276316</td>\n",
       "      <td>19.382448</td>\n",
       "      <td>50.036205</td>\n",
       "      <td>51.149681</td>\n",
       "      <td>50.752056</td>\n",
       "      <td>193.016357</td>\n",
       "      <td>-0.001870</td>\n",
       "      <td>-0.049512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86.487709</td>\n",
       "      <td>87.830811</td>\n",
       "      <td>86.118042</td>\n",
       "      <td>86.621788</td>\n",
       "      <td>767622784.000000</td>\n",
       "      <td>83.614433</td>\n",
       "      <td>82.491531</td>\n",
       "      <td>81.183426</td>\n",
       "      <td>59.727512</td>\n",
       "      <td>57.898697</td>\n",
       "      <td>...</td>\n",
       "      <td>19.801800</td>\n",
       "      <td>19.322884</td>\n",
       "      <td>19.315168</td>\n",
       "      <td>19.450840</td>\n",
       "      <td>50.454590</td>\n",
       "      <td>51.432079</td>\n",
       "      <td>50.973419</td>\n",
       "      <td>193.568359</td>\n",
       "      <td>-0.000645</td>\n",
       "      <td>-0.050532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86.368454</td>\n",
       "      <td>87.603996</td>\n",
       "      <td>85.996964</td>\n",
       "      <td>86.493233</td>\n",
       "      <td>1037429504.000000</td>\n",
       "      <td>83.578468</td>\n",
       "      <td>82.368446</td>\n",
       "      <td>81.309959</td>\n",
       "      <td>59.756145</td>\n",
       "      <td>57.958862</td>\n",
       "      <td>...</td>\n",
       "      <td>19.341043</td>\n",
       "      <td>19.371004</td>\n",
       "      <td>19.342754</td>\n",
       "      <td>19.374475</td>\n",
       "      <td>49.940075</td>\n",
       "      <td>51.144272</td>\n",
       "      <td>50.755882</td>\n",
       "      <td>193.173004</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>-0.049836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>86.453842</td>\n",
       "      <td>87.843292</td>\n",
       "      <td>86.128029</td>\n",
       "      <td>86.489441</td>\n",
       "      <td>978492096.000000</td>\n",
       "      <td>83.613396</td>\n",
       "      <td>82.286819</td>\n",
       "      <td>81.345818</td>\n",
       "      <td>59.682049</td>\n",
       "      <td>57.875252</td>\n",
       "      <td>...</td>\n",
       "      <td>19.645220</td>\n",
       "      <td>19.339613</td>\n",
       "      <td>19.310829</td>\n",
       "      <td>19.374384</td>\n",
       "      <td>50.268562</td>\n",
       "      <td>51.315720</td>\n",
       "      <td>50.857185</td>\n",
       "      <td>193.804352</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>-0.049477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86.729782</td>\n",
       "      <td>88.108459</td>\n",
       "      <td>86.509819</td>\n",
       "      <td>87.024673</td>\n",
       "      <td>833890560.000000</td>\n",
       "      <td>83.614944</td>\n",
       "      <td>82.379341</td>\n",
       "      <td>81.290665</td>\n",
       "      <td>60.150543</td>\n",
       "      <td>58.047993</td>\n",
       "      <td>...</td>\n",
       "      <td>19.745634</td>\n",
       "      <td>19.328209</td>\n",
       "      <td>19.348251</td>\n",
       "      <td>19.404387</td>\n",
       "      <td>50.572029</td>\n",
       "      <td>51.580471</td>\n",
       "      <td>51.090702</td>\n",
       "      <td>194.693420</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>-0.048251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>87.164406</td>\n",
       "      <td>88.562454</td>\n",
       "      <td>86.732292</td>\n",
       "      <td>87.172791</td>\n",
       "      <td>894394432.000000</td>\n",
       "      <td>83.500031</td>\n",
       "      <td>82.469276</td>\n",
       "      <td>81.322487</td>\n",
       "      <td>59.954502</td>\n",
       "      <td>57.976070</td>\n",
       "      <td>...</td>\n",
       "      <td>19.982641</td>\n",
       "      <td>19.411467</td>\n",
       "      <td>19.323660</td>\n",
       "      <td>19.398363</td>\n",
       "      <td>51.246265</td>\n",
       "      <td>51.948685</td>\n",
       "      <td>51.261475</td>\n",
       "      <td>191.875198</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>-0.048134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>87.266518</td>\n",
       "      <td>88.646683</td>\n",
       "      <td>87.127541</td>\n",
       "      <td>87.403305</td>\n",
       "      <td>865003520.000000</td>\n",
       "      <td>83.557335</td>\n",
       "      <td>82.519508</td>\n",
       "      <td>81.275597</td>\n",
       "      <td>60.299408</td>\n",
       "      <td>58.239246</td>\n",
       "      <td>...</td>\n",
       "      <td>19.873808</td>\n",
       "      <td>19.386909</td>\n",
       "      <td>19.383232</td>\n",
       "      <td>19.384977</td>\n",
       "      <td>51.052052</td>\n",
       "      <td>51.838978</td>\n",
       "      <td>51.237701</td>\n",
       "      <td>192.713562</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>-0.048420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>87.323799</td>\n",
       "      <td>88.719864</td>\n",
       "      <td>87.007256</td>\n",
       "      <td>87.513100</td>\n",
       "      <td>815534976.000000</td>\n",
       "      <td>83.618416</td>\n",
       "      <td>82.475250</td>\n",
       "      <td>81.390152</td>\n",
       "      <td>60.415390</td>\n",
       "      <td>58.207253</td>\n",
       "      <td>...</td>\n",
       "      <td>19.643829</td>\n",
       "      <td>19.348822</td>\n",
       "      <td>19.328451</td>\n",
       "      <td>19.388992</td>\n",
       "      <td>50.653000</td>\n",
       "      <td>51.458393</td>\n",
       "      <td>51.024021</td>\n",
       "      <td>189.656952</td>\n",
       "      <td>0.004507</td>\n",
       "      <td>-0.046360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>87.266541</td>\n",
       "      <td>88.708008</td>\n",
       "      <td>86.941895</td>\n",
       "      <td>87.386246</td>\n",
       "      <td>811039744.000000</td>\n",
       "      <td>83.663338</td>\n",
       "      <td>82.485115</td>\n",
       "      <td>81.420868</td>\n",
       "      <td>60.067055</td>\n",
       "      <td>58.018085</td>\n",
       "      <td>...</td>\n",
       "      <td>19.631489</td>\n",
       "      <td>19.343689</td>\n",
       "      <td>19.307459</td>\n",
       "      <td>19.437685</td>\n",
       "      <td>50.752811</td>\n",
       "      <td>51.558784</td>\n",
       "      <td>50.992119</td>\n",
       "      <td>190.601349</td>\n",
       "      <td>0.007167</td>\n",
       "      <td>-0.047230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>87.265610</td>\n",
       "      <td>88.612579</td>\n",
       "      <td>87.016838</td>\n",
       "      <td>87.539001</td>\n",
       "      <td>841540416.000000</td>\n",
       "      <td>83.717712</td>\n",
       "      <td>82.548645</td>\n",
       "      <td>81.472855</td>\n",
       "      <td>60.112961</td>\n",
       "      <td>57.862000</td>\n",
       "      <td>...</td>\n",
       "      <td>19.543217</td>\n",
       "      <td>19.374763</td>\n",
       "      <td>19.315952</td>\n",
       "      <td>19.387850</td>\n",
       "      <td>50.656067</td>\n",
       "      <td>51.490055</td>\n",
       "      <td>51.008400</td>\n",
       "      <td>189.864105</td>\n",
       "      <td>0.010479</td>\n",
       "      <td>-0.045660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>87.258102</td>\n",
       "      <td>88.510399</td>\n",
       "      <td>86.716179</td>\n",
       "      <td>87.256134</td>\n",
       "      <td>951623552.000000</td>\n",
       "      <td>83.835815</td>\n",
       "      <td>82.630409</td>\n",
       "      <td>81.573944</td>\n",
       "      <td>59.783504</td>\n",
       "      <td>57.768341</td>\n",
       "      <td>...</td>\n",
       "      <td>19.853672</td>\n",
       "      <td>19.389791</td>\n",
       "      <td>19.397221</td>\n",
       "      <td>19.435562</td>\n",
       "      <td>51.124481</td>\n",
       "      <td>51.803581</td>\n",
       "      <td>51.228424</td>\n",
       "      <td>190.740189</td>\n",
       "      <td>0.007076</td>\n",
       "      <td>-0.045370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>86.917717</td>\n",
       "      <td>88.286469</td>\n",
       "      <td>86.598381</td>\n",
       "      <td>86.989639</td>\n",
       "      <td>921405312.000000</td>\n",
       "      <td>83.814697</td>\n",
       "      <td>82.496330</td>\n",
       "      <td>81.405624</td>\n",
       "      <td>59.396507</td>\n",
       "      <td>57.419224</td>\n",
       "      <td>...</td>\n",
       "      <td>20.037889</td>\n",
       "      <td>19.366808</td>\n",
       "      <td>19.327314</td>\n",
       "      <td>19.444807</td>\n",
       "      <td>50.975349</td>\n",
       "      <td>51.747242</td>\n",
       "      <td>51.227005</td>\n",
       "      <td>190.852707</td>\n",
       "      <td>0.011517</td>\n",
       "      <td>-0.044179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>86.942673</td>\n",
       "      <td>88.178261</td>\n",
       "      <td>86.465263</td>\n",
       "      <td>86.774834</td>\n",
       "      <td>844027392.000000</td>\n",
       "      <td>83.826698</td>\n",
       "      <td>82.720062</td>\n",
       "      <td>81.443489</td>\n",
       "      <td>59.008152</td>\n",
       "      <td>57.298809</td>\n",
       "      <td>...</td>\n",
       "      <td>20.037483</td>\n",
       "      <td>19.502844</td>\n",
       "      <td>19.382137</td>\n",
       "      <td>19.401579</td>\n",
       "      <td>51.181931</td>\n",
       "      <td>51.823334</td>\n",
       "      <td>51.297226</td>\n",
       "      <td>190.148956</td>\n",
       "      <td>0.010627</td>\n",
       "      <td>-0.043559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>86.849892</td>\n",
       "      <td>88.324593</td>\n",
       "      <td>86.513161</td>\n",
       "      <td>87.078400</td>\n",
       "      <td>909426624.000000</td>\n",
       "      <td>83.823433</td>\n",
       "      <td>82.570427</td>\n",
       "      <td>81.457291</td>\n",
       "      <td>59.709156</td>\n",
       "      <td>57.540867</td>\n",
       "      <td>...</td>\n",
       "      <td>19.898146</td>\n",
       "      <td>19.488909</td>\n",
       "      <td>19.426672</td>\n",
       "      <td>19.404345</td>\n",
       "      <td>50.590912</td>\n",
       "      <td>51.585686</td>\n",
       "      <td>51.128433</td>\n",
       "      <td>187.934372</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>-0.044894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>87.194130</td>\n",
       "      <td>88.545868</td>\n",
       "      <td>86.905228</td>\n",
       "      <td>87.255196</td>\n",
       "      <td>803806912.000000</td>\n",
       "      <td>83.799217</td>\n",
       "      <td>82.587311</td>\n",
       "      <td>81.427094</td>\n",
       "      <td>59.742855</td>\n",
       "      <td>57.674969</td>\n",
       "      <td>...</td>\n",
       "      <td>20.060862</td>\n",
       "      <td>19.511982</td>\n",
       "      <td>19.341248</td>\n",
       "      <td>19.431828</td>\n",
       "      <td>50.652470</td>\n",
       "      <td>51.598526</td>\n",
       "      <td>51.186619</td>\n",
       "      <td>190.289825</td>\n",
       "      <td>0.013067</td>\n",
       "      <td>-0.042392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>87.101685</td>\n",
       "      <td>88.336075</td>\n",
       "      <td>86.761375</td>\n",
       "      <td>87.107124</td>\n",
       "      <td>828306688.000000</td>\n",
       "      <td>83.898438</td>\n",
       "      <td>82.686150</td>\n",
       "      <td>81.522598</td>\n",
       "      <td>60.064156</td>\n",
       "      <td>57.805172</td>\n",
       "      <td>...</td>\n",
       "      <td>19.746765</td>\n",
       "      <td>19.439611</td>\n",
       "      <td>19.396122</td>\n",
       "      <td>19.384899</td>\n",
       "      <td>50.527599</td>\n",
       "      <td>51.494137</td>\n",
       "      <td>51.041462</td>\n",
       "      <td>190.260391</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>-0.041696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>87.450096</td>\n",
       "      <td>88.701736</td>\n",
       "      <td>87.061577</td>\n",
       "      <td>87.274696</td>\n",
       "      <td>745115712.000000</td>\n",
       "      <td>83.841354</td>\n",
       "      <td>82.570091</td>\n",
       "      <td>81.497818</td>\n",
       "      <td>59.871189</td>\n",
       "      <td>57.736755</td>\n",
       "      <td>...</td>\n",
       "      <td>19.871058</td>\n",
       "      <td>19.516706</td>\n",
       "      <td>19.412365</td>\n",
       "      <td>19.434956</td>\n",
       "      <td>50.869877</td>\n",
       "      <td>51.769711</td>\n",
       "      <td>51.211948</td>\n",
       "      <td>186.537964</td>\n",
       "      <td>0.013682</td>\n",
       "      <td>-0.041594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>87.146973</td>\n",
       "      <td>88.504311</td>\n",
       "      <td>87.016350</td>\n",
       "      <td>87.416168</td>\n",
       "      <td>849390464.000000</td>\n",
       "      <td>83.792152</td>\n",
       "      <td>82.651932</td>\n",
       "      <td>81.624222</td>\n",
       "      <td>59.792389</td>\n",
       "      <td>57.783043</td>\n",
       "      <td>...</td>\n",
       "      <td>19.901266</td>\n",
       "      <td>19.440308</td>\n",
       "      <td>19.408546</td>\n",
       "      <td>19.430687</td>\n",
       "      <td>50.884521</td>\n",
       "      <td>51.731770</td>\n",
       "      <td>51.179256</td>\n",
       "      <td>188.464996</td>\n",
       "      <td>0.014583</td>\n",
       "      <td>-0.040094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>87.212822</td>\n",
       "      <td>88.529907</td>\n",
       "      <td>86.869125</td>\n",
       "      <td>87.199532</td>\n",
       "      <td>824095680.000000</td>\n",
       "      <td>83.908211</td>\n",
       "      <td>82.681038</td>\n",
       "      <td>81.560844</td>\n",
       "      <td>60.152386</td>\n",
       "      <td>57.892338</td>\n",
       "      <td>...</td>\n",
       "      <td>19.627054</td>\n",
       "      <td>19.443377</td>\n",
       "      <td>19.378775</td>\n",
       "      <td>19.405169</td>\n",
       "      <td>50.031181</td>\n",
       "      <td>51.216278</td>\n",
       "      <td>50.876930</td>\n",
       "      <td>183.098801</td>\n",
       "      <td>0.017195</td>\n",
       "      <td>-0.040119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>87.411743</td>\n",
       "      <td>88.531555</td>\n",
       "      <td>86.766914</td>\n",
       "      <td>87.415375</td>\n",
       "      <td>798740096.000000</td>\n",
       "      <td>83.843475</td>\n",
       "      <td>82.629089</td>\n",
       "      <td>81.664291</td>\n",
       "      <td>59.943920</td>\n",
       "      <td>57.833721</td>\n",
       "      <td>...</td>\n",
       "      <td>19.376436</td>\n",
       "      <td>19.430180</td>\n",
       "      <td>19.325537</td>\n",
       "      <td>19.365179</td>\n",
       "      <td>49.857735</td>\n",
       "      <td>51.164490</td>\n",
       "      <td>50.790318</td>\n",
       "      <td>182.550293</td>\n",
       "      <td>0.018988</td>\n",
       "      <td>-0.039584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>87.194656</td>\n",
       "      <td>88.676750</td>\n",
       "      <td>87.142532</td>\n",
       "      <td>87.640205</td>\n",
       "      <td>766999552.000000</td>\n",
       "      <td>83.863541</td>\n",
       "      <td>82.717628</td>\n",
       "      <td>81.613579</td>\n",
       "      <td>60.282959</td>\n",
       "      <td>57.807774</td>\n",
       "      <td>...</td>\n",
       "      <td>19.452875</td>\n",
       "      <td>19.353798</td>\n",
       "      <td>19.314909</td>\n",
       "      <td>19.372129</td>\n",
       "      <td>49.697975</td>\n",
       "      <td>51.124619</td>\n",
       "      <td>50.739811</td>\n",
       "      <td>181.424072</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>-0.039010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>87.528488</td>\n",
       "      <td>88.985672</td>\n",
       "      <td>87.370888</td>\n",
       "      <td>87.703651</td>\n",
       "      <td>839732736.000000</td>\n",
       "      <td>83.918488</td>\n",
       "      <td>82.667168</td>\n",
       "      <td>81.718193</td>\n",
       "      <td>60.505993</td>\n",
       "      <td>58.101524</td>\n",
       "      <td>...</td>\n",
       "      <td>19.287966</td>\n",
       "      <td>19.227152</td>\n",
       "      <td>19.229275</td>\n",
       "      <td>19.324915</td>\n",
       "      <td>49.839596</td>\n",
       "      <td>51.026726</td>\n",
       "      <td>50.675674</td>\n",
       "      <td>178.173264</td>\n",
       "      <td>0.020286</td>\n",
       "      <td>-0.037636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>87.786293</td>\n",
       "      <td>89.150558</td>\n",
       "      <td>87.478333</td>\n",
       "      <td>87.820503</td>\n",
       "      <td>832502528.000000</td>\n",
       "      <td>83.991264</td>\n",
       "      <td>82.814896</td>\n",
       "      <td>81.649460</td>\n",
       "      <td>60.592743</td>\n",
       "      <td>57.969723</td>\n",
       "      <td>...</td>\n",
       "      <td>18.989660</td>\n",
       "      <td>19.277592</td>\n",
       "      <td>19.241556</td>\n",
       "      <td>19.333048</td>\n",
       "      <td>49.712193</td>\n",
       "      <td>50.967922</td>\n",
       "      <td>50.690048</td>\n",
       "      <td>177.934418</td>\n",
       "      <td>0.021983</td>\n",
       "      <td>-0.037288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>87.616005</td>\n",
       "      <td>89.107979</td>\n",
       "      <td>87.363625</td>\n",
       "      <td>87.755867</td>\n",
       "      <td>867173248.000000</td>\n",
       "      <td>84.018875</td>\n",
       "      <td>82.843903</td>\n",
       "      <td>81.705788</td>\n",
       "      <td>60.415546</td>\n",
       "      <td>57.875359</td>\n",
       "      <td>...</td>\n",
       "      <td>18.962221</td>\n",
       "      <td>19.221981</td>\n",
       "      <td>19.185495</td>\n",
       "      <td>19.302622</td>\n",
       "      <td>49.451191</td>\n",
       "      <td>50.848206</td>\n",
       "      <td>50.537003</td>\n",
       "      <td>173.871490</td>\n",
       "      <td>0.022582</td>\n",
       "      <td>-0.035634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>87.581940</td>\n",
       "      <td>89.064514</td>\n",
       "      <td>87.405334</td>\n",
       "      <td>87.844551</td>\n",
       "      <td>901885824.000000</td>\n",
       "      <td>84.088554</td>\n",
       "      <td>82.883598</td>\n",
       "      <td>81.721794</td>\n",
       "      <td>60.809189</td>\n",
       "      <td>58.271427</td>\n",
       "      <td>...</td>\n",
       "      <td>18.805294</td>\n",
       "      <td>19.143200</td>\n",
       "      <td>19.196732</td>\n",
       "      <td>19.301188</td>\n",
       "      <td>48.810841</td>\n",
       "      <td>50.440975</td>\n",
       "      <td>50.276661</td>\n",
       "      <td>172.954086</td>\n",
       "      <td>0.022419</td>\n",
       "      <td>-0.035140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>87.836563</td>\n",
       "      <td>89.146248</td>\n",
       "      <td>87.385788</td>\n",
       "      <td>87.786224</td>\n",
       "      <td>868342784.000000</td>\n",
       "      <td>84.118309</td>\n",
       "      <td>82.904510</td>\n",
       "      <td>81.833176</td>\n",
       "      <td>60.517059</td>\n",
       "      <td>58.052025</td>\n",
       "      <td>...</td>\n",
       "      <td>18.736170</td>\n",
       "      <td>18.986288</td>\n",
       "      <td>19.074703</td>\n",
       "      <td>19.218828</td>\n",
       "      <td>48.922028</td>\n",
       "      <td>50.459312</td>\n",
       "      <td>50.297142</td>\n",
       "      <td>175.513550</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>-0.036271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>87.835968</td>\n",
       "      <td>89.030022</td>\n",
       "      <td>87.577362</td>\n",
       "      <td>87.896782</td>\n",
       "      <td>861266432.000000</td>\n",
       "      <td>84.181664</td>\n",
       "      <td>82.864395</td>\n",
       "      <td>81.950516</td>\n",
       "      <td>60.481701</td>\n",
       "      <td>57.895649</td>\n",
       "      <td>...</td>\n",
       "      <td>19.039679</td>\n",
       "      <td>18.989021</td>\n",
       "      <td>19.086403</td>\n",
       "      <td>19.242573</td>\n",
       "      <td>49.459152</td>\n",
       "      <td>50.818668</td>\n",
       "      <td>50.546402</td>\n",
       "      <td>172.886642</td>\n",
       "      <td>0.024495</td>\n",
       "      <td>-0.034737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>87.909164</td>\n",
       "      <td>89.142250</td>\n",
       "      <td>87.585571</td>\n",
       "      <td>87.950081</td>\n",
       "      <td>951838464.000000</td>\n",
       "      <td>84.236351</td>\n",
       "      <td>82.977425</td>\n",
       "      <td>81.846512</td>\n",
       "      <td>60.777172</td>\n",
       "      <td>58.123096</td>\n",
       "      <td>...</td>\n",
       "      <td>18.829750</td>\n",
       "      <td>18.981390</td>\n",
       "      <td>19.073338</td>\n",
       "      <td>19.194168</td>\n",
       "      <td>49.146332</td>\n",
       "      <td>50.643291</td>\n",
       "      <td>50.414036</td>\n",
       "      <td>173.357803</td>\n",
       "      <td>0.026003</td>\n",
       "      <td>-0.035348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open_x    High_x     Low_x   Close_x            Volume  EMA20_Close_x  \\\n",
       "0  87.211418 88.378296 86.803787 86.958466  946947520.000000      83.730179   \n",
       "1  87.042038 88.199036 86.686440 86.900658  890388480.000000      83.625900   \n",
       "2  86.802452 88.103111 86.402641 86.851761  801209728.000000      83.619713   \n",
       "3  86.487709 87.830811 86.118042 86.621788  767622784.000000      83.614433   \n",
       "4  86.368454 87.603996 85.996964 86.493233 1037429504.000000      83.578468   \n",
       "5  86.453842 87.843292 86.128029 86.489441  978492096.000000      83.613396   \n",
       "6  86.729782 88.108459 86.509819 87.024673  833890560.000000      83.614944   \n",
       "7  87.164406 88.562454 86.732292 87.172791  894394432.000000      83.500031   \n",
       "8  87.266518 88.646683 87.127541 87.403305  865003520.000000      83.557335   \n",
       "9  87.323799 88.719864 87.007256 87.513100  815534976.000000      83.618416   \n",
       "10 87.266541 88.708008 86.941895 87.386246  811039744.000000      83.663338   \n",
       "11 87.265610 88.612579 87.016838 87.539001  841540416.000000      83.717712   \n",
       "12 87.258102 88.510399 86.716179 87.256134  951623552.000000      83.835815   \n",
       "13 86.917717 88.286469 86.598381 86.989639  921405312.000000      83.814697   \n",
       "14 86.942673 88.178261 86.465263 86.774834  844027392.000000      83.826698   \n",
       "15 86.849892 88.324593 86.513161 87.078400  909426624.000000      83.823433   \n",
       "16 87.194130 88.545868 86.905228 87.255196  803806912.000000      83.799217   \n",
       "17 87.101685 88.336075 86.761375 87.107124  828306688.000000      83.898438   \n",
       "18 87.450096 88.701736 87.061577 87.274696  745115712.000000      83.841354   \n",
       "19 87.146973 88.504311 87.016350 87.416168  849390464.000000      83.792152   \n",
       "20 87.212822 88.529907 86.869125 87.199532  824095680.000000      83.908211   \n",
       "21 87.411743 88.531555 86.766914 87.415375  798740096.000000      83.843475   \n",
       "22 87.194656 88.676750 87.142532 87.640205  766999552.000000      83.863541   \n",
       "23 87.528488 88.985672 87.370888 87.703651  839732736.000000      83.918488   \n",
       "24 87.786293 89.150558 87.478333 87.820503  832502528.000000      83.991264   \n",
       "25 87.616005 89.107979 87.363625 87.755867  867173248.000000      84.018875   \n",
       "26 87.581940 89.064514 87.405334 87.844551  901885824.000000      84.088554   \n",
       "27 87.836563 89.146248 87.385788 87.786224  868342784.000000      84.118309   \n",
       "28 87.835968 89.030022 87.577362 87.896782  861266432.000000      84.181664   \n",
       "29 87.909164 89.142250 87.585571 87.950081  951838464.000000      84.236351   \n",
       "\n",
       "    EMA40_Close_x  EMA60_Close_x  RSI20_Close_x  RSI40_Close_x  ...   Close_y  \\\n",
       "0       82.526726      81.223221      60.528824      58.296509  ... 19.839705   \n",
       "1       82.463181      81.228577      60.446342      58.435841  ... 19.542809   \n",
       "2       82.461517      81.294991      60.624031      58.475307  ... 19.130501   \n",
       "3       82.491531      81.183426      59.727512      57.898697  ... 19.801800   \n",
       "4       82.368446      81.309959      59.756145      57.958862  ... 19.341043   \n",
       "5       82.286819      81.345818      59.682049      57.875252  ... 19.645220   \n",
       "6       82.379341      81.290665      60.150543      58.047993  ... 19.745634   \n",
       "7       82.469276      81.322487      59.954502      57.976070  ... 19.982641   \n",
       "8       82.519508      81.275597      60.299408      58.239246  ... 19.873808   \n",
       "9       82.475250      81.390152      60.415390      58.207253  ... 19.643829   \n",
       "10      82.485115      81.420868      60.067055      58.018085  ... 19.631489   \n",
       "11      82.548645      81.472855      60.112961      57.862000  ... 19.543217   \n",
       "12      82.630409      81.573944      59.783504      57.768341  ... 19.853672   \n",
       "13      82.496330      81.405624      59.396507      57.419224  ... 20.037889   \n",
       "14      82.720062      81.443489      59.008152      57.298809  ... 20.037483   \n",
       "15      82.570427      81.457291      59.709156      57.540867  ... 19.898146   \n",
       "16      82.587311      81.427094      59.742855      57.674969  ... 20.060862   \n",
       "17      82.686150      81.522598      60.064156      57.805172  ... 19.746765   \n",
       "18      82.570091      81.497818      59.871189      57.736755  ... 19.871058   \n",
       "19      82.651932      81.624222      59.792389      57.783043  ... 19.901266   \n",
       "20      82.681038      81.560844      60.152386      57.892338  ... 19.627054   \n",
       "21      82.629089      81.664291      59.943920      57.833721  ... 19.376436   \n",
       "22      82.717628      81.613579      60.282959      57.807774  ... 19.452875   \n",
       "23      82.667168      81.718193      60.505993      58.101524  ... 19.287966   \n",
       "24      82.814896      81.649460      60.592743      57.969723  ... 18.989660   \n",
       "25      82.843903      81.705788      60.415546      57.875359  ... 18.962221   \n",
       "26      82.883598      81.721794      60.809189      58.271427  ... 18.805294   \n",
       "27      82.904510      81.833176      60.517059      58.052025  ... 18.736170   \n",
       "28      82.864395      81.950516      60.481701      57.895649  ... 19.039679   \n",
       "29      82.977425      81.846512      60.777172      58.123096  ... 18.829750   \n",
       "\n",
       "    EMA20_Close_y  EMA40_Close_y  EMA60_Close_y  RSI20_Close_y  RSI40_Close_y  \\\n",
       "0       19.367441      19.308647      19.410965      51.167896      51.843208   \n",
       "1       19.322601      19.360321      19.463125      50.764824      51.567406   \n",
       "2       19.354540      19.276316      19.382448      50.036205      51.149681   \n",
       "3       19.322884      19.315168      19.450840      50.454590      51.432079   \n",
       "4       19.371004      19.342754      19.374475      49.940075      51.144272   \n",
       "5       19.339613      19.310829      19.374384      50.268562      51.315720   \n",
       "6       19.328209      19.348251      19.404387      50.572029      51.580471   \n",
       "7       19.411467      19.323660      19.398363      51.246265      51.948685   \n",
       "8       19.386909      19.383232      19.384977      51.052052      51.838978   \n",
       "9       19.348822      19.328451      19.388992      50.653000      51.458393   \n",
       "10      19.343689      19.307459      19.437685      50.752811      51.558784   \n",
       "11      19.374763      19.315952      19.387850      50.656067      51.490055   \n",
       "12      19.389791      19.397221      19.435562      51.124481      51.803581   \n",
       "13      19.366808      19.327314      19.444807      50.975349      51.747242   \n",
       "14      19.502844      19.382137      19.401579      51.181931      51.823334   \n",
       "15      19.488909      19.426672      19.404345      50.590912      51.585686   \n",
       "16      19.511982      19.341248      19.431828      50.652470      51.598526   \n",
       "17      19.439611      19.396122      19.384899      50.527599      51.494137   \n",
       "18      19.516706      19.412365      19.434956      50.869877      51.769711   \n",
       "19      19.440308      19.408546      19.430687      50.884521      51.731770   \n",
       "20      19.443377      19.378775      19.405169      50.031181      51.216278   \n",
       "21      19.430180      19.325537      19.365179      49.857735      51.164490   \n",
       "22      19.353798      19.314909      19.372129      49.697975      51.124619   \n",
       "23      19.227152      19.229275      19.324915      49.839596      51.026726   \n",
       "24      19.277592      19.241556      19.333048      49.712193      50.967922   \n",
       "25      19.221981      19.185495      19.302622      49.451191      50.848206   \n",
       "26      19.143200      19.196732      19.301188      48.810841      50.440975   \n",
       "27      18.986288      19.074703      19.218828      48.922028      50.459312   \n",
       "28      18.989021      19.086403      19.242573      49.459152      50.818668   \n",
       "29      18.981390      19.073338      19.194168      49.146332      50.643291   \n",
       "\n",
       "    RSI60_Close_y  shifted_day_of_year  cos_shifted_annual  sin_shifted_annual  \n",
       "0       51.288277           190.546097           -0.004181           -0.050945  \n",
       "1       51.071781           190.944687           -0.001361           -0.050009  \n",
       "2       50.752056           193.016357           -0.001870           -0.049512  \n",
       "3       50.973419           193.568359           -0.000645           -0.050532  \n",
       "4       50.755882           193.173004            0.000307           -0.049836  \n",
       "5       50.857185           193.804352            0.002148           -0.049477  \n",
       "6       51.090702           194.693420            0.005761           -0.048251  \n",
       "7       51.261475           191.875198            0.003781           -0.048134  \n",
       "8       51.237701           192.713562            0.003433           -0.048420  \n",
       "9       51.024021           189.656952            0.004507           -0.046360  \n",
       "10      50.992119           190.601349            0.007167           -0.047230  \n",
       "11      51.008400           189.864105            0.010479           -0.045660  \n",
       "12      51.228424           190.740189            0.007076           -0.045370  \n",
       "13      51.227005           190.852707            0.011517           -0.044179  \n",
       "14      51.297226           190.148956            0.010627           -0.043559  \n",
       "15      51.128433           187.934372            0.011695           -0.044894  \n",
       "16      51.186619           190.289825            0.013067           -0.042392  \n",
       "17      51.041462           190.260391            0.014444           -0.041696  \n",
       "18      51.211948           186.537964            0.013682           -0.041594  \n",
       "19      51.179256           188.464996            0.014583           -0.040094  \n",
       "20      50.876930           183.098801            0.017195           -0.040119  \n",
       "21      50.790318           182.550293            0.018988           -0.039584  \n",
       "22      50.739811           181.424072            0.019060           -0.039010  \n",
       "23      50.675674           178.173264            0.020286           -0.037636  \n",
       "24      50.690048           177.934418            0.021983           -0.037288  \n",
       "25      50.537003           173.871490            0.022582           -0.035634  \n",
       "26      50.276661           172.954086            0.022419           -0.035140  \n",
       "27      50.297142           175.513550            0.023670           -0.036271  \n",
       "28      50.546402           172.886642            0.024495           -0.034737  \n",
       "29      50.414036           173.357803            0.026003           -0.035348  \n",
       "\n",
       "[30 rows x 24 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverted_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
